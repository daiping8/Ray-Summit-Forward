| 分类                  | 演讲内容                                                     | 概要                                                         |
| --------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Ray RoadMap           | Ray: Last Year’s Progress and the Road Ahead \| Ray Summit 2025 | 视频描述：在2025年Ray峰会上，Anyscale公司的Ed Oakes和姚佳俊分享了过去一年为提升Ray在尖端大规模应用中的性能、韧性与可观测性所取得的重大进展。   他们详细介绍了Ray Core运行时的关键改进，包括：   Ray直接传输技术：新一代通信层，显著优化加速器间的数据传输   基于cgroups的原生资源隔离：实现更强的工作负载分离与更可预测的性能表现   增强的网络故障韧性：确保长期运行的分布式任务在真实环境中保持稳定   显著提升的可观测性：提供集群规模下系统行为的深度洞察   Ed和佳俊还首次展望了Ray Core 2026年路线图，规划了旨在支持日益复杂、性能关键的分布式工作负载的新一轮功能特性与架构升级。   与会者将清晰了解Ray如何持续演进以满足下一代AI系统需求，并掌握平台在未来一年的发展方向。 |
| Ray 生态              | Ray Summit 2025 Keynote Day 1\| Where AI Builders Shape What's Next | 该演讲开篇回顾计算技术从大型机到云计算的演进历程，指出 AI 时代已确凿到来且深刻影响多领域，强调 AI 时代需专属原生计算栈，现有技术栈迁移不可行，原因在于异构硬件、AI 处理流程演变、模型动态不确定性及对快速迭代的需求；接着提出新 AI 计算栈需满足跨集群编排、灵活兼容、容错、Pythonic 四大核心需求，点明 Ray 是为此构建的核心计算引擎，集成多框架且下载量激增，并展示 xAI、Cursor 等企业的应用案例；还分析企业构建 AI 栈的三种选择，指出创建 AI 原生基础设施栈是唯一长期可行选项，同时介绍 Ray 源于 2015 年 UC Berkeley，历经三阶段发展采用率爆发式增长，以及现代 AI 软件栈中 Ray 与分布式框架、容器编排器协同进化，Ray 核心与库有原生 RDMA 支持等最新进展；此外，提及 Anyscale 统一 AI 平台的层次、功能与合作，NVIDIA 物理 AI 的合成数据策略及基于 Ray 的模型训练，还有 Thinking Machines 的 Tinker 微调工具及对 Ray 的利用，全面呈现 Ray 及相关平台在 AI 基础设施中的价值。 |
| Ray Data              | Ray Data for Structured Workloads: Deep Dive \| Ray Summit 2025 | 原视频描述有误                                               |
|                       | How Ray Data Powers Scalable AI Workloads \| Ray Summit 2025 | 视频描述：在2025年Ray峰会上，Anyscale公司的Balaji Veeramani分享了Ray Data如何发展成为Ray生态系统中使用最广泛的库之一——专为新一代AI工作负载而构建。   与传统的数据处理引擎不同，Ray Data从设计之初就专注于多模态、加速器原生且以AI为核心的流水线。在本次演讲中，讲者概述了Ray Data的核心能力，并重点介绍了过去一年中新增的主要功能，以支持：   跨GPU和集群的大规模批量推理   面向海量模型的分布式训练数据准备与读取   涵盖图像、视频、文本等的高性能多模态数据处理   无论您正在构建LLM流水线、多模态训练工作流，还是高吞吐量推理系统，本次分享都将清晰展示Ray Data如何为规模化现代AI提供核心动力 |
| Ray Serve             | Ray Serve: Advancing scalability and flexibility \| Ray Summit 2025 | 视频介绍：本视频由Anyscale公司的Ray Serve团队工程师Abra Shik和Alex Yang分享，详细介绍了Ray Serve框架在过去一年中在可扩展性、性能和灵活性方面的重大进展。内容包括异步推理支持、自定义请求路由、自定义自动扩缩容、性能优化以及多云服务支持等核心功能。   结论：Ray Serve作为一个生产就绪的AI推理引擎，通过引入异步推理、自定义路由策略和跨部署自动扩缩容等高级功能，显著提升了框架的灵活性和表达能力。性能方面，Anyscale运行时相比开源版本实现了高达7倍的吞吐量提升，并保持了近线性的扩展能力。新推出的多云服务功能进一步解决了GPU资源稀缺和单点故障问题，为大规模AI工作负载提供了跨云厂商的高可用解决方案。这些改进使Ray Serve在简化AI模型部署复杂度的同时，兼顾了性能、可靠性和易用性，成为企业级AI推理的理想选择。 |
| Ray Train             | Ray Train: Distributed Solutions for Removing Training Bottlenecks \| Ray Summit 2025 | 视频介绍：本视频由Anyscale公司的RAID团队软件工程师Justin和Timothy分享，重点介绍了如何利用Ray Train和Ray Data消除分布式训练中的瓶颈问题，通过异构集群架构和异步操作显著提升GPU利用效率。   结论：通过系统性地解决数据加载、容错恢复、检查点和验证等关键瓶颈，Ray生态系统能够将分布式训练运行时间减少超过68%，同时保持成本效益。核心优化策略包括：使用Ray Data实现数据预处理的独立扩展、支持中段 epoch 恢复的快速容错机制、异步检查点上传以及并行验证执行。这些技术共同确保了GPU能够持续高效地进行模型训练，为大规模机器学习工作负载提供了可扩展且经济高效的解决方案。 |
|                       | How to Get Started with Distributed Training at Scale \| Ray Summit 2025 | 视频描述：在2025年Ray峰会上，Anyscale公司的Suman Debnath和Linda Haviv分享了如何掌握分布式训练策略，这些策略对于高效扩展当今深度学习模型至关重要。   他们首先解析了三大核心技术——数据并行、模型并行和流水线并行，并阐述了随着模型和数据集的增长，每种方法最适合的应用场景。本次讲座涵盖了分片训练和ZeRO等高级方法，以及实际大规模集群环境中出现的权衡取舍。   Suman和Linda还探讨了分布式训练中最棘手的挑战，包括通信开销、容错性、可复现性以及异构计算资源管理。随后他们演示了如何将PyTorch与Ray结合使用，以最少的代码改动实现这些策略，从而更轻松地从原型阶段扩展到生产环境。   您将学习到：   数据并行、模型并行和流水线并行的原理及适用场景   如何克服通信开销和系统故障等可扩展性瓶颈   如何运用Ray与PyTorch启动、编排和监控大规模分布式训练任务 |
| RLlib                 | RLlib: Lessons from the V2 Stack and Road Ahead \| Ray Summit 2025 | 视频描述：在Ray Summit 2025大会上，Anyscale公司的Artur Niederfahrenhorst和Simon Lars Zehnders分享了RLlib v2技术栈发布背后的重要里程碑——该版本现已正式可用，并针对新一代大规模强化学习工作负载进行了重新设计。   他们深入解析了塑造全新RLlib v2架构的关键经验，详细介绍了提升可扩展性、可靠性和可扩展性的工程改进。本次分享重点展示了RLlib实现万级环境运行器和百级学习器规模的核心增强能力，可支持高吞吐量的大规模分布式强化学习训练。   Artur和Simon还展望了RLlib的发展路线图，包括与各类模拟器的深度集成，以及针对复杂模拟密集型强化学习应用的扩展能力。   无论您正在构建大规模强化学习系统、加速研究流程，还是将强化学习投入生产环境，本场分享都将清晰展现RLlib的未来发展蓝图，以及它如何持续演进以满足现代人工智能的需求。 |
| Ray Observability     | Ray Observability Upgrades: Debug, Optimize, and Scale Faster \| Ray Summit 2025 | 随后，演讲者重点介绍了Anyscale平台在可观测性方面的重大升级，包括为Ray Core、Ray Train和Ray Data提供的可扩展持久化仪表盘视图。他们深入解析了这些仪表盘的底层架构，并阐释了如何在确保所有数据留存于客户云环境的前提下实现安全运维。   Nikita与Mengjin还正式发布了开源工具Ray Export API，该接口支持用户将Ray仪表盘中展示的各类事件持久化存储、分析并集成至自有监控或分析系统中。   通过现场演示，他们生动展现了如何运用这些工具诊断现实场景中的问题——从内存溢出异常到资源利用效率低下等各类状况。这些新功能使得Ray工作负载变得前所未有的透明可靠，优化工作也变得更加轻松高效。 |
| Ray Direct Transport  | RDMA P2P Deep Dive: KvCache Transfer, Weight Updates & MoE Routing at Perplexity \| Ray Summit 2025 | 视频介绍：本视频由Propensity AI的Lein分享，深入探讨了在大语言模型系统中使用RDMA点对点通信的技术，重点讲解了权重传输应用，并简要介绍了KV缓存传输和混合专家模型的通信优化。   结论：RDMA点对点通信技术为大语言模型系统中的动态通信模式提供了高效解决方案。相比传统的集合通信，RDMA点对点通信具有成员关系灵活、初始化简单、支持非均匀张量形状和无需强顺序保证等优势。通过权重传输、KV缓存传输和混合专家模型三个具体应用案例的实践，证明了该技术能显著提升通信效率，实现1.3秒内传输超大模型权重，并在AWS等云平台上实现万亿参数模型的运行。未来这一技术将继续推动大语言模型系统在多云环境下的可扩展性和性能优化。 |
|                       | Ray Direct Transport: RDMA Support in Ray Core \| Ray Summit 2025 | 视频介绍：本视频由Any Scale公司Ray核心团队的软件工程师Stephanie和Choling共同分享，详细介绍了Ray 2.51.1版本中新增的Ray Direct Transport（RDT）功能。这是一个处于alpha阶段的重要特性，旨在优化GPU间数据传输性能。   结论：Ray Direct Transport通过引入直接数据传输机制，成功解决了传统Ray对象存储在GPU间数据传输中的性能瓶颈。该功能支持多种高速传输技术（如NCCL、NIXL），能够实现高达1000倍的性能提升，特别适用于强化学习、大语言模型训练等需要大量GPU间数据传输的场景。RDT保持了Ray核心API的易用性，同时提供了灵活的数据传输选择，为分布式AI工作负载提供了更高效的数据传输解决方案。 |
| Anyscale              | MLOps with Ray on Anyscale \| Ray Summit 2025                | 视频描述：在集群上无缝扩展训练与推理   提升机器学习工作流的可观测性、可复现性与治理能力   在降低运维开销的同时加速迭代周期   参会者将学习如何基于Anyscale平台的Ray框架构建健壮的生产级机器学习系统——为任意规模的团队实现高效可扩展的MLOps能力。 |
|                       | Maximizing Compute Efficiency on Anyscale \| Ray Summit 2025 | 视频描述：在2025年Ray峰会上，Anyscale公司的Janet Li和Simran Mhatre分享了Ray可观测性的最新进展——这些工具旨在帮助开发者更清晰地调试、优化和理解分布式AI工作负载。   她们首先解释了为什么情境化可观测性至关重要。随着Ray成为分布式AI应用的标准框架，用户越来越多地遇到大型多节点系统固有的复杂性。专门构建的可观测性工具对于诊断资源瓶颈、任务失败和内存压力等问题至关重要。   Janet和Simran随后介绍了Anyscale平台在可观测性方面的重大改进，包括为Ray Core、Ray Train和Ray Data提供的可扩展持久化仪表盘视图。她们深入解析了支撑这些仪表盘的架构，并演示如何在保证所有数据留存于客户云环境的前提下安全运行。   演讲者还发布了开源的Ray Export API，该接口允许用户将Ray仪表盘中显示的相同事件持久化存储、分析并集成到自己的监控分析系统中。   通过现场演示，她们展现了如何使用这些工具调试现实场景中的问题——从内存溢出错误到低效资源利用率——使得Ray工作负载比以往任何时候都更加透明、可靠且易于优化。 |
|                       | Building Fault-Tolerant Massive Ray Clusters on Anyscale \| Ray Summit 2025 | 视频描述：在2025年Ray峰会上，Anyscale公司的Dhyey Shah与Ibrahim Rabbani分享了Ray如何通过工程化设计经受住大规模AI工作负载的严苛考验，以及如何在超过1万个节点的集群上稳定运行的实践之道。   演讲者首先阐述了极端规模下必然出现的挑战：网络波动、抢占式实例中断、硬件故障、资源争用以及不可预测的基础设施行为。他们解析了Ray分布式运行时如何优雅应对这些故障，在持续动荡中保持工作负载持续运行，并维持强大的可靠性。   随后，Dhyey和Ibrahim深入探讨了在超大规模环境下构建和运营Ray所获得的关键工程经验，包括容错策略、状态管理、恢复机制、弹性伸缩以及感知工作负载的调度方案。   最后他们展望了Ray的发展方向——重点介绍了为支撑下一代AI应用而即将推出的可扩展性、可靠性与性能优化方案。   无论您正在运行大规模模型训练、分布式推理、强化学习还是多模态流水线，本场分享都将带您深入探究：当其他系统纷纷崩溃时，Ray如何始终保持坚不可摧。 |
| 企业DL/ML实践         | Ray Summit 2025 Keynote: Building Cursor Composer with Sasha Rush | 这段视频由Cursor的AI研究员Sasha Rush主讲，深入介绍了其团队最新发布的基于智能体的代码大模型——Cursor Composer。视频阐述了构建该模型的动机、核心技术挑战、基础设施解决方案以及发布初期的成果与反思。Cursor Composer是一款成功结合了顶尖编码智能与极致效率的智能体模型，其性能接近前沿模型，同时在令牌生成效率上高出同类智能模型四倍，为用户带来了革命性的交互式编码体验。强化学习是构建此类高度专业化模型的关键范式，其效果随着计算资源的投入而稳定提升。基础设施的深度创新（如低精度训练、负载均衡、与生产环境一致的训练流程）是成功训练和部署复杂RL系统的决定性因素。模型与产品的协同设计使得训练出的智能体能更好地利用生产级工具（如语义搜索），从而在实际应用中表现更佳。用户反馈证实了速度与智能的结合能够显著改变开发者的工作流，提升效率与体验。 |
|                       | Scaling LinkedIn's Online Training Solution with Ray \| Ray Summit 2025 | 本视频由LinkedIn AI训练平台的工程师团队分享，详细介绍了他们如何利用Ray框架来扩展在线训练解决方案。深入探讨了在线训练系统的架构设计、技术挑战和实际应用效果。LinkedIn通过构建基于Ray的在线训练平台，成功解决了传统批量训练的局限性，实现了模型更新频率的大幅提升和业务指标的显著改善。该系统通过流式数据生成、高效数据注入、统一训练环境管理和完善的监控体系，为大规模AI应用提供了稳定可靠的在线训练能力。未来的发展方向包括更细粒度的参数更新、序列模型支持和自动化优化，展现了在线训练技术在工业级AI系统中的广阔应用前景。 |
|                       | How Zoox Built a Reliable, High-Velocity Model Serving Platform with Ray Serve \| Ray Summit 2025 | 本次演讲聚焦于自动驾驶公司Zuks在构建其内部模型服务平台Zserve时，如何利用Ray Serve解决可靠性与迭代速度方面的挑战。该平台最初支持数十个模型，现已扩展至生产环境中运行超过100个模型，服务于包括传统机器学习模型、大语言模型和多模态模型在内的多种用例。通过构建一个以“一键部署”为核心的模型部署服务，并辅以计算资源隔离、多层次环境以及全面的可观测性，Zuks成功地将模型部署时间从数天缩短至分钟级别，同时显著提升了平台的可靠性与用户的信任度。其核心成就包括实现了快速的开发迭代、高效的批量推理操作以及自动化的可靠性检查。 |
|                       | Inside Uber: Scaling Model Training with Ray \| Ray Summit 2025 | 本视频由Uber AI平台工程师Pong和Bharat共同分享，详细介绍了Uber如何利用Ray框架构建大规模机器学习训练平台，支持从核心ML模型到大型语言模型的训练，并分享了在模型规模不断扩大过程中的技术演进和优化策略。   结论: Uber通过构建基于Ray的分布式训练平台，成功应对了模型规模增长带来的挑战。该平台整合了硬件升级、框架优化和资源管理等多维度改进，实现了10倍训练吞吐量提升。关键经验表明，模型平台的优化需要硬件、基础设施和算法层的垂直整合。未来，Uber计划进一步探索容错训练、多云资源调度和自动化优化等方向，为工业级AI系统提供更加稳定高效的大规模训练能力。 |
|                       | Matrix: Reliable Framework for Data-Centric Experimentation at Scale \| Ray Summit 2025 | 视频介绍：本视频由Meta FAIR实验室的Rama Ragwendra和工程师Dong共同分享，详细介绍了他们基于Ray框架构建的数据中心化实验平台Matrix。该系统旨在解决AI模型训练中的数据瓶颈问题，通过高效的数据生成、处理和实验管理，推动下一代AI模型的发展。   结论：Matrix作为Meta FAIR实验室构建的数据中心化实验框架，成功解决了大规模AI模型训练中的数据挑战。该系统通过集成Ray集群管理、语言模型推理优化、容器化服务和作业调度等核心功能，为研究人员提供了高效的数据生成、处理和实验验证能力。实际应用表明，Matrix已成功支持多智能体协作、自然推理等前沿研究，生成数百万条高质量数据。该平台的开源化将进一步推动AI社区在数据中心化研究方面的发展，为突破当前数据瓶颈提供了切实可行的技术方案。 |
|                       | Inside Netflix’s Mako: The Next-Gen ML Training Platform \| Ray Summit 2025 | 视频介绍：本视频由Netflix训练平台团队的工程经理Aan和高级软件工程师Matan共同分享，详细介绍了Netflix新一代机器学习训练平台Macco的设计理念、架构特点和实际应用。这是Netflix首次公开分享Macco平台的技术细节。   结论：Netflix通过构建基于临时Ray集群的Macco平台，成功解决了第一代训练平台Manta在规模化过程中遇到的可靠性、隔离性和扩展性问题。Macco采用容器化作业、定制化调度器和优化数据加载等关键技术，为Netflix内部的大规模模型训练提供了更可靠、更灵活和更具扩展性的基础设施。平台不仅支持传统的分布式训练，还扩展到了异步推理和LLM后训练等复杂场景，展现了Netflix在ML基础设施领域的深厚技术积累和前瞻性设计理念 |
|                       | How Alibaba Cloud Accelerates AI Pipelines with AnalyticDB Ray \| Ray Summit 2025 | 视频介绍：本视频由阿里巴巴云数据团队的产品经理Fay和工程负责人Leang共同分享，详细介绍了阿里云AnalyticDB如何集成Ray框架构建数据与AI融合平台，以解决传统数据仓库在处理多模态数据和AI工作流时面临的挑战，并展示了该平台在多个实际应用场景中的成功实践。   结论：阿里巴巴云通过将全托管Ray服务深度集成到AnalyticDB数据仓库中，成功构建了一个统一的数据+AI平台，有效解决了多模态ETL处理、异构资源调度、AI工作流集成等核心难题。该平台凭借自动弹性伸缩、流式计算优化、企业级稳定性增强和全面可观测性四大核心增强功能，在批处理推理、机器人仿真、广告点击率预测和游戏助手等多个场景中实现了显著的性能提升和资源利用率优化。这一架构不仅消除了数据存储、分析与AI计算之间的隔阂，更为企业客户提供了一站式的数据智能解决方案，展现了云计算与AI技术融合的广阔前景。 |
|                       | Wisedocs’ Journey: Rebuilding & Accelerating ML with KubeRay \| Ray Summit 2025 | 视频介绍：本视频分享了医疗文档处理领域一个团队在过去一年中使用Ray框架重建机器学习管道的实践经验。演讲者详细介绍了从传统模块化架构迁移到基于Ray的分布式服务架构的全过程，包括技术选型考量、架构设计、部署策略以及迁移过程中遇到的实际挑战和解决方案。   结论：通过采用Ray Serve重构ML管道，团队成功实现了代码量减少50%、处理时间显著提升、支持更大文件规模的目标。新架构通过统一的服务层、模块化设计和Kubernetes原生部署，解决了原有系统在扩展性、部署效率和资源利用方面的瓶颈。迁移过程中采用的分阶段影子部署、功能标志和客户分段策略确保了零停机迁移。未来团队将继续优化硬件配置、完善自动扩缩容规则，并在保持开发效率的前提下进一步提升系统性能。 |
|                       | Pinterest’s Approach to Real-Time ML Experimentation Using Ray \| Ray Summit 2025 | 视频介绍：本视频由Pinterest的机器学习工程师团队分享，详细介绍了他们如何利用Ray框架显著提升机器学习开发效率，特别是在数据处理和模型训练方面实现的突破性改进。演讲者通过三个具体案例——采样优化、特征回填和下游奖励标签生成，展示了Ray如何帮助团队在竞争激烈的注意力经济中保持技术领先优势。   结论：Pinterest通过构建基于Ray的快速机器学习技术栈，成功解决了传统Spark工作流在数据处理上的瓶颈问题，将模型迭代周期从数周缩短到数天。这一技术转型不仅带来了每年超过35万美元的成本节约，更重要的是大幅提升了团队的技术创新能力，使工程师能够快速实验复杂的采样策略和下游奖励建模技术。Ray异构集群与智能分区的结合为大规模机器学习工作负载提供了灵活高效的解决方案，展现了现代ML基础设施在加速AI应用迭代中的关键作用。 |
|                       | How Workday Achieved 50x Cheaper Model Serving with Ray Serve \| Ray Summit 2025 | 视频介绍：本视频由Workday机器学习工程师Josh Carpel分享，详细介绍了他们如何利用Ray Serve框架将模型服务成本降低50倍，并解决了在单个Ray集群中部署数千个租户模型时遇到的技术挑战。   结论：Workday通过构建基于Ray Serve的新型模型服务平台，成功解决了传统租户分片系统存在的资源浪费、扩展性差和功能限制等问题。该系统通过动态创建模型应用、分离业务逻辑与模型服务、优化控制器性能等创新方法，实现了成本的大幅降低和系统能力的显著提升。虽然在某些优化功能完全合并到上游前仍存在一些限制，但整体架构已证明能够高效支持大规模多租户模型服务场景，为类似企业级AI平台提供了宝贵的技术参考。 |
|                       | Marin: Open Development of Open Foundation Models \| Ray Summit 2025 | 视频介绍: 本视频由前斯坦福大学基础模型研究中心研究工程负责人、现Open Athena非营利组织成员David Hall分享，详细介绍了Maren项目——一个致力于通过开源开发模式构建基础模型的开放实验室。演讲探讨了AI模型从开放到封闭的发展趋势，并展示了Maren如何通过实验驱动的方法、Ray分布式计算框架和社区协作来推动开放式AI模型开发。   结论: Maren项目成功证明了在开源环境下开发高性能基础模型的可行性，其发布的8B和32B参数模型在多项基准测试中表现出色，超越了部分主流开源模型。通过将软件工程的优秀实践（如GitHub工作流、数据流图表示）应用于机器学习研究，Maren建立了一个透明的、可复现的模型开发框架。该项目充分利用Ray处理TPU集群的调度挑战，特别是在抢占式计算环境下的稳定性问题，为社区驱动的AI开发提供了宝贵经验。随着多语言模型等社区项目的推进，Maren正在朝着"构建基础模型的Linux"这一愿景稳步前进。 |
|                       | How Roblox Trains 3D Foundation Models with Ray \| Ray Summit 2025 | 视频介绍：本视频由Roblox机器学习平台团队分享，详细介绍了他们如何利用Ray框架构建3D基础模型训练平台，并展示了其开源的Cube 3D模型实时生成能力。演讲通过现场演示、技术架构解析和性能优化案例，全面展现了在Roblox海量用户规模下实现高效分布式训练的工程实践。   结论：Roblox通过构建基于Ray的混合云训练平台，成功解决了3D基础模型训练中的数据预处理、资源调度和计算扩展等核心挑战。该平台实现了从开发环境快速迭代到大规模训练任务的无缝衔接，通过异构集群管理、智能资源调度和多云架构，为创作者提供了强大的AI辅助创作能力。未来将继续优化训练效率和支持更复杂的3D生成任务，展现了Ray在工业级AI系统中的广泛应用价值。 |
|                       | How Latitude AI Trains Perception Models at Massive Scale \| Ray Summit 2025 | 视频介绍: 本视频由福特汽车旗下自动驾驶技术公司Latitude AI的工程师团队在Ray Summit上分享，详细介绍了他们如何利用Ray框架优化感知机器学习训练数据流水线，实现了9倍于原始PyTorch数据加载器的性能提升。   结论: Latitude AI通过系统性地优化基于Ray的数据流水线，成功解决了自动驾驶感知模型训练中的数据瓶颈问题。团队从初始的PyTorch数据加载器迁移到Ray Data，通过采样策略重构、流水线阶段合并、序列化优化、预取机制改进等多维度优化，最终实现了训练速度的大幅提升和资源使用效率的显著改善。这些优化不仅解决了内存溢出、训练不稳定等实际问题，还为团队提供了更好的可观测性和调优灵活性，展现了Ray在复杂机器学习工作负载中的强大能力。 |
|                       | Scaling User-Focused Foundation Models at Grab with Ray \| Ray Summit 2025 | 视频介绍：本视频由Grab AI与实验平台团队的Chong Yu和Nick分享，详细介绍了他们在构建用户中心化基础模型方面的实践经验。作为东南亚领先的超级应用，Grab拥有涵盖出行、外卖、购物、金融等多元业务的丰富数据，这为构建能够深入理解用户行为的基础模型提供了独特机会。   结论：Grab通过创新的适配器中心化架构成功构建了用户中心化基础模型，该模型能够从表格数据和时序数据中联合学习用户表征。关键技术突破包括多模态令牌化、稀疏优化训练和异构集群部署，使得模型在多个下游任务中展现出优于传统方法的性能。未来发展方向包括改进预训练目标、扩展适配器能力和优化非均匀时序编码，为实现更精准的用户理解和个性化服务奠定基础。 |
|                       | How Autodesk Built a Next-Gen Deep Learning Platform with Ray \| Ray Summit 2025 | 视频介绍：本视频由Autodesk Research的机器学习工程经理Guo Del Castillo和首席机器学习工程师Kamal Rahim Malik Shang共同分享，详细介绍了他们如何基于Ray框架构建内部深度学习平台Ray Lab，以解决大规模模型训练、数据流水线和计算瓶颈等挑战。   结论：Autodesk通过构建基于Kubernetes的Ray Lab平台，成功将分散的GPU作业转变为统一的生产级研究平台，支持跨团队和跨区域的大规模深度学习工作负载。该平台通过简化的集群管理、统一的训练API、智能作业调度和全面的监控体系，在保持研究敏捷性的同时显著提升了资源利用效率和团队协作能力。未来的工作重点包括多租户公平性、自动扩缩容和更高效的资源管理策略，为工业级AI研究提供了可扩展的基础设施解决方案。 |
|                       | How Geotab Scales Video AI Efficiently with Anyscale \| Ray Summit 2025 | 视频介绍：本视频由Geotab公司的工程师Mike分享，详细介绍了该公司如何利用Anyscale Ray框架优化视频AI推理和运营的完整历程。内容涵盖了从车队管理摄像头系统的演进、技术挑战识别，到基于Ray的架构设计决策和最终实现的模型管理平台。   结论：Geotab通过构建基于Anyscale Ray的集中式机器学习平台，成功解决了视频AI处理中的多项技术挑战。该平台实现了从模型训练、测试到部署的全流程自动化，显著提升了团队协作效率和系统性能。具体成果包括视频处理QPS提升43倍、GPU利用率从10%提升至50%、GPU资源需求减少40%，并为未来扩展到非视频机器学习领域奠定了坚实基础。这一案例展示了Ray在企业级AI系统中的强大应用价值。 |
|                       | How xAI Scales Image & Video Processing with Ray \| Ray Summit 2025 | 视频介绍: 本视频由XAI技术团队成员Jay和Kashin Chan分享，详细介绍了他们如何利用Ray框架构建大规模图像视频数据处理基础设施，以支持Grok多模态模型的训练需求。   结论: XAI通过构建基于Ray和Redis Queue的视频数据管理流水线，成功解决了大规模视频处理中的扩展性和容错性挑战。该系统采用简洁的三层架构，通过幂等性Actor设计、可靠队列机制和KubeRay自动扩展功能，实现了高效的视频数据处理能力。关键创新包括将状态信息完全存储在Redis中、实现任务的至少一次处理语义，以及通过KubeRay降低运维成本。这些技术方案为大规模多模态AI训练提供了可靠的数据处理基础设施，展现了Ray在工业级AI系统中的强大应用潜力。 |
|                       | Motional’s Blueprint for High-Performance ML Systems in Autonomous Driving \| Ray Summit 2025 | 视频介绍：本视频由Motional公司的工程师Dhanj和Avi分享，重点介绍了他们在自动驾驶技术领域使用Ray框架构建机器学习数据管道的实践经验。演讲详细阐述了如何利用Ray处理PB级别的驾驶日志数据，以及通过三次迭代优化实现数据处理效率的显著提升。   结论：Motional通过采用Ray框架成功构建了高效可靠的机器学习数据管道，将数据处理规模从TB级扩展到PB级，实现了10倍的速度提升和100倍的成本降低。关键成功因素包括：基于Ray Core的任务编排、Ray Data API的高效数据处理、多层次容错机制、静态Actor池优化以及创新的节点单例初始化器模式。这些实践不仅大幅提升了模型训练效率，还赋能ML工程师自主管理数据管道，将数据发布周期从数周缩短至数天，为自动驾驶技术的快速发展提供了坚实的数据基础。 |
|                       | Ray Data & vLLM for Scalable Image Captioning at Zoox \| Ray Summit 2025 | 视频介绍：本视频由Zuks机器学习平台的分布式训练负责人分享，重点介绍了如何利用Ray框架构建大规模LLM和VLM模型的后训练评估系统。演讲者从业务需求出发，详细阐述了评估工作流的技术架构选择、实现方案优化过程以及实际应用中的经验总结。   结论：通过采用Ray框架构建异构计算评估系统，Zuks成功实现了训练与评估工作流的解耦，大幅降低了模型评估对昂贵训练资源的占用。系统通过数据分区、批处理优化、状态管理和监控集成等策略，显著提升了多模态模型评估的效率和可扩展性。该方案不仅支持灵活的评估触发模式，还能无缝集成现有的自定义工具库，为大规模AI模型的工业化评估提供了可靠的技术基础。 |
|                       | How BMW Scales Automotive AI Workloads with the Ray Framework \| Ray Summit 2025 | 视频介绍：本视频由BMW集团的Thomas分享，详细介绍了BMW如何利用Ray框架在汽车行业中扩展AI工作负载。重点阐述了BMW Connected AI平台的架构演进历程，从最初的Kubeflow解决方案到基于Ray的现代化平台转型，并深入分析了在模型训练和模型服务两个关键领域的具体实践案例。   结论：BMW通过采用Ray框架成功解决了原有Kubeflow平台在基础设施复杂性、生成式AI支持和大规模训练扩展方面的局限性。平台转型的核心在于实现集中化管理的基础设施、提供可扩展的构建模块，并通过工作流API平衡用户自主权与平台治理。在语音助手训练案例中实现了从过夜训练到分钟级训练的显著加速，在大语言模型自托管方面成功部署了6850亿参数的模型。未来平台将扩展到大数据处理、多模型部署和强化学习等方向，展现了Ray框架在工业级AI系统中的强大应用价值。 |
|                       | Inside Adobe Firefly: JIT-Embedding with Ray Serve for Faster GenAI Training \| Ray Summit 2025 | 视频介绍：本视频由Adobe Firefly AI平台团队的工程师Han和Bachelo分享，详细介绍了他们如何利用Ray Serve框架构建"即时嵌入服务"(Just Embedding Service)，以支持Adobe Firefly大规模多模态基础模型的训练工作。   结论：Adobe通过构建基于Ray Serve的即时嵌入服务，成功解决了大规模基础模型训练中嵌入计算的效率与灵活性平衡问题。该系统通过专门的嵌入服务集群、优化的媒体编码传输、客户端负载均衡和预取机制，实现了训练GPU资源的充分利用，显著提升了迭代速度并降低了成本。该方案已成功支撑Adobe Firefly产品线中图像、视频、音频等多种模态的基础模型训练，展现了在工业级AI系统中专业化服务架构的重要价值。 |
|                       | How AWS Scales Reinforcement Learning Across Thousands of GPUs \| Ray Summit 2025 | 视频介绍：本视频由AWS的AI专家Anup和Kunal共同分享，重点探讨了如何利用AWS SageMaker HyperPod平台来扩展后训练工作负载，特别是在数千甚至数万GPU规模上优化强化学习等后训练任务。演讲深入分析了当前后训练领域的技术挑战、HyperPod的核心架构设计，以及实际应用中的性能优化策略。   结论：AWS SageMaker HyperPod通过构建弹性的、异构的、高效的AI训练平台，成功解决了大规模后训练工作负载中的基础设施挑战。该平台通过深度健康检查、自动节点替换、分布式内存存储、快速检查点恢复等创新功能，将训练时间减少了40%以上，检查点保存时间提升了2-3倍，模型加载时间提升了4-5倍。结合Kubernetes控制平面和Ray计算引擎，HyperPod为企业在快速演进的AI领域提供了灵活、可靠的规模化训练解决方案，已被Amazon自身用于Nova模型的训练，展现了其在工业级AI系统中的重要价值。 |
|                       | Apple’s Approach to Scalable Machine Learning Infrastructure on Ray \| Ray Summit 2025 | 视频介绍：本视频由Apple的研究工程师Yiha和Ho Chong共同分享，详细介绍了他们如何基于Ray框架构建一个灵活且可扩展的机器学习框架ASA，旨在解决内部机器学习工作流中的基础设施碎片化问题，并支持从传统深度学习到大规模基础模型训练的全流程工作。   结论：Apple通过构建ASA统一机器学习框架，成功解决了内部数千名机器学习开发者面临的工具链碎片化、环境切换复杂和资源效率低下等核心痛点。该框架基于Ray构建，具备基础设施无感知、原生开发者体验保持、分布式训练简化等核心特性，支持从单GPU实验到千卡级基础模型预训练的全尺度应用。实际案例证明，在70亿参数模型预训练中实现了每GPU 1,400 tokens/秒的高效训练性能，展现了框架在大规模生产环境中的成熟度和可靠性。该设计既为普通开发者提供了开箱即用的简易接口，又为高级用户保留了充分的灵活性和可扩展性。 |
|                       | Scaling Machine Learning at Tripadvisor: Our Journey with Ray and Anyscale \| Ray Summit 2025 | 视频介绍：本视频由TripAdvisor的机器学习工程师Jada Stories和机器学习平台团队成员Sam Jenkins共同分享，详细介绍了该公司如何通过采用Ray和Anyscale平台来优化其机器学习基础设施，以应对日益复杂的AI工作负载和业务需求。   结论：TripAdvisor通过全面采用Anyscale平台，成功实现了机器学习工作负载的现代化转型。该迁移带来了显著的效率提升，包括代码量减少超过80%、计算成本降低超过80%以及处理时间大幅缩短。平台提供的统一开发环境、弹性伸缩能力和完善的监控体系，使得团队能够更快速地部署和维护复杂的AI应用。未来，公司计划进一步扩展Ray在实时AI服务、LLM训练与微调等领域的应用，巩固其作为MLOps核心计算平台的地位。 |
| 多模态数据/大数据组件 | NVIDIA NeMo Curator: Scaling Multi-Modal Data Curation Workflows \| Ray Summit 2025 | 视频介绍: 本视频由NVIDIA工程师团队分享，详细介绍了他们如何利用Ray框架构建可扩展的多模态数据处理管道Nemo Curator，解决了大规模AI训练中的数据准备挑战。   结论: NVIDIA通过开发基于Ray的Nemo Curator数据预处理管道，成功解决了多模态AI训练中的数据规模化处理难题。该系统通过流式处理、灵活资源分配、自动负载均衡等核心技术，实现了数据处理速度10-30倍的提升，同时显著降低了计算成本。特别是在去重、语义分析等复杂场景下，通过GPU加速和混合执行器策略，能够处理从文本到视频的百PB级数据。该开源项目为社区提供了完整的解决方案，展现了Ray在工业级AI数据管道中的强大能力。 |
|                       | NVIDIA’s Framework for Scalable Data Curation \| Ray Summit 2025 | 视频介绍: 本视频由NVIDIA的Jacob及其团队分享，详细介绍了Cosmos平台及其两大核心组件Cosmos Zenna和Cosmos Curate。Cosmos是NVIDIA用于物理AI的基础模型平台，专注于生成世界基础模型和加速数据处理流程，旨在推动自动驾驶、机器人等物理AI应用的发展。   结论: Cosmos平台通过构建专为大规模多模态数据管道优化的Zenna库，成功解决了在有限基础设施条件下运行PB级视频、图像数据处理管道的挑战。团队从Dask到Ray Data再到自主开发Zenna的演进过程中，积累了关于流式执行、管道平衡、资源管理和可观测性等方面的宝贵经验。开源的Cosmos Curate为开发者提供了构建视频处理管道的参考实现，展现了在复杂基础设施环境下实现高效数据处理的完整解决方案。这些经验对任何处理大规模多模态数据管道的团队都具有重要参考价值。 |
|                       | Scaling Multimodal Data Curation with Ray and LanceDB \| Ray Summit 2025 | 视频介绍: 本视频由Netflix机器学习工程师Pablo和Lance DB联合创始人Lei共同呈现，深入探讨了大规模多模态数据集构建中的数据管理、批量推理和存储解决方案。演讲详细介绍了从原始数据到高质量训练数据集的完整数据管理流程，以及如何利用Ray框架和Lance DB技术栈实现高效的数据处理。   结论: 构建大规模多模态数据集需要模型管理、高效批量推理和智能存储系统的紧密结合。通过采用Ray的批量推理机制和Lance DB的零拷贝数据演化技术，团队能够有效处理数亿级别的图像和视频数据，实现数据集的持续优化和版本管理。这种技术组合为现代AI训练提供了可扩展、高效的数据基础设施解决方案，特别适用于需要频繁更新和迭代的大规模多模态数据集场景。 |
|                       | Scaling Multi-Modal Datasets to Petabytes with Ray at Apple \| Ray Summit 2025 | 视频介绍：本视频由技术团队分享他们如何利用Ray框架构建大规模多模态数据处理平台的经验。演讲者详细介绍了平台架构设计、性能优化策略以及在35,000个CPU核心上处理PB级数据时遇到的实际挑战和解决方案。   结论：通过构建基于Ray的统一数据处理平台，团队成功解决了多模态数据处理的规模化难题，实现了8.7倍的性价比提升。该平台通过强类型数据框架API、端到端流式执行、双重执行路径等创新设计，在保持研究人员生产效率的同时实现了极致的扩展性。未来将重点投入数据血缘追踪、增量处理和自服务调试工具，以进一步提升平台的可靠性和易用性。 |
|                       | How Daft Boosts Batch Inference Throughput with Dynamic Partitioning \| Ray Summit 2025 | 视频介绍：本视频由Daft数据引擎团队的Kevin分享，重点介绍了Daft如何通过前缀缓存和连续批处理技术优化批量推理性能，将处理时间减少50.7%。   结论：Daft通过动态前缀分桶和连续批处理技术的结合，在128 GPU集群上对20万条提示词进行批量推理时实现了50.7%的性能提升。这一创新方案有效解决了传统批处理方法中的GPU空闲时间问题，同时通过智能路由和本地分桶机制在保持高缓存命中率的同时实现了流式处理，避免了全量数据排序的开销。该功能已在Daft 0.6.9版本中作为公开测试版发布，为大规模型批量推理提供了高效的解决方案。 |
|                       | Exabyte-scale Streaming Iceberg IO with Ray, Flink, and DeltaCAT \| Ray Summit 2025 | 视频介绍: 本视频由亚马逊首席工程师Patrick分享，详细介绍了亚马逊在构建EB级数据湖仓过程中的技术演进，特别是从Spark迁移到Ray框架的实践经验，以及为解决Iceberg表格式中的流式写入问题而开发的开源解决方案Deltacat项目。   结论: 亚马逊通过构建基于Ray的分布式计算平台，成功解决了传统Spark在处理EB级数据湖仓 compaction 操作时的高成本和性能瓶颈问题。Deltacat项目不仅实现了82%的成本节约，还创新性地解决了Iceberg表格式中流式框架与批处理框架在删除操作上的兼容性问题。该系统通过高效的delete转换机制、多表格式同步能力和轻量级Python API，为大规模数据湖仓管理提供了完整的解决方案。未来的发展方向包括更完善的Flink集成、Python原生Beam连接器以及全栈Ray化，展现了分布式计算在数据湖仓领域的广阔应用前景。 |
| 大模型推理            | Ray + vLLM Efficient Multi Node Orchestration for Sparse MoE Model Serving \| Ray Summit 2025 | 本视频由AnyScale公司的LM团队负责人Kurush与工程师Si共同呈现，深入探讨了如何使用Ray Serve和vLLM高效部署稀疏混合专家模型，重点分析了提升服务效率的三大关键技术：宽专家并行、预填充与解码分离架构以及智能请求路由。通过Ray Serve LLM框架，团队成功构建了可扩展的稀疏MOE模型服务方案，实现了在高吞吐量场景下的显著效率提升。宽专家并行通过数据并行策略大幅增加KV缓存容量，预填充与解码分离优化了计算资源利用率，而智能请求路由则有效利用了前缀缓存。实验证明该方案在EP16配置下性能与原生vLLM相当，展现了Ray在生产环境中部署复杂模型服务模式的强大能力。未来将继续优化弹性容错、LM感知自动扩展等特性，为大规模AI推理提供更完善的解决方案。 |
|                       | KubeRay + vLLM at DatalogyAI: Engineering Trillion-Scale Synthetic Data Systems \| Ray Summit 2025 | 视频介绍：本视频由Dtology公司的联合创始人Bogdan和技术团队成员Fan共同分享，详细介绍了他们在万亿级别token规模的预训练中扩展合成数据所获得的工程经验教训。Dtology是一家数据策展产品公司，专注于帮助客户识别最佳数据子集，通过过滤、去重、转换、增强和生成合成数据来优化模型训练效果。   结论：Dtology通过构建统一的合成数据生成流水线，成功解决了在万亿token规模下合成数据生成的核心工程挑战。他们通过优化元数据获取机制、实现可恢复的执行架构、构建跨云调度系统以及快速搜索最优推理参数，实现了合成数据生成效率的质的飞跃。该解决方案使客户能够获得训练速度提升10-20倍、模型性能提升8-12个百分点、参数量减少1-2个数量级但性能相当的显著收益。未来发展方向包括解决资源打包问题、提升数据局部性和实现智能调度，展现了在大规模AI训练中合成数据技术的巨大潜力。 |
|                       | Accelerating vLLM with LMCache \| Ray Summit 2025            | 视频介绍：本视频详细介绍了IronCache项目，这是一个专为大语言模型推理设计的KV缓存解决方案。演讲者深入探讨了构建该项目的动机、系统架构设计、性能优化技术以及未来的机器学习优化方向，旨在打造连接推理引擎与存储硬件的最快开源KV缓存系统。   结论：IronCache通过解决KV缓存规模快速增长和存储需求变化的挑战，成功构建了一个高性能的KV缓存层。该项目采用创新的系统架构设计，包括与推理引擎协同演化的API接口、PCIe带宽饱和技术、请求级异步处理和层级流水线等优化手段，实现了显著的性能提升。同时，团队正在探索KV缓存压缩、稀疏注意力优化和跨模型缓存重用等机器学习优化技术，为大规模AI推理应用提供了可靠的缓存基础设施。该项目已获得广泛社区支持和企业采用，展现了在AI推理基础设施领域的重要价值。 |
|                       | How Red Hat Scales Large-Scale Serving with vLLM \| Ray Summit 2025 | 视频介绍：本视频由Red Hat工程师Rob分享，深入探讨了在vLLM平台上优化混合专家模型推理性能的技术方案，重点分析了传统张量并行方法的局限性以及数据并行注意力与专家并行相结合的新范式。   结论：通过采用数据并行注意力与专家并行的混合架构，结合高效的dispatch-combine操作、双批次重叠技术和预填充-解码分离策略，vLLM平台成功解决了现代大模型在多节点部署中的性能瓶颈。这些优化使得在96个GPU的集群上能够实现每秒2000个token的高吞吐量，同时显著提升了KV缓存的利用效率。未来还将继续完善弹性专家并行和容错机制，为大模型推理提供更稳定可靠的分布式解决方案。 |
|                       | Embedded LLM’s Guide to vLLM Architecture & High-Performance Serving \| Ray Summit 2025 | 视频介绍：本视频由Embedded LM的工程师TJ分享，详细介绍了如何在vLLM推理框架中集成自定义内核（kernel）来加速大语言模型推理，特别是针对AMD GPU平台的ROCm后端集成实践。   结论：通过将AMD优化的ROCm内核集成到vLLM框架中，成功实现了对DeepSeek等模型推理性能的显著提升。集成过程涉及自定义算子实现、注意力后端开发、CUDA图支持等多个关键技术环节。vLLM通过分层架构设计、torch compile兼容性处理和统一的集成评估流程，为不同硬件平台的内核集成提供了标准化方案。未来随着更多优化内核的加入，vLLM在多硬件平台上的性能表现将进一步提升。 |
|                       | vLLM TPU: A new unified-backend supporting Pytorch and JAX natively on TPU \| Ray Summit 2025 | 视频介绍：本视频由Google工程师团队分享，重点介绍了他们在vLLM（开源推理引擎）中最新推出的统一TPU后端，该后端在一周半前正式发布，原生支持JAX和PyTorch框架，旨在将TPU的竞争优势带给更广泛的用户群体。   结论：通过构建基于硬件插件架构的统一TPU后端，Google成功实现了在vLLM生态中深度集成TPU加速能力。该系统通过完全重写的XLA降低路径、优化的核心内核和SPMD编程范式，在保持与上游vLLM相同用户体验的同时，实现了5倍的性能提升。未来将重点支持Ironwood新一代TPU芯片、强化学习集成和LMD生产部署，展现了TPU在大规模语言模型推理领域的巨大潜力。 |
|                       | Elastic Expert Parallelism for vLLM \| Ray Summit 2025       | 视频介绍：本视频由AnyScale公司的Ray和UC Berkeley的Yung G共同分享，详细介绍了针对视觉语言模型(vLLM)的弹性专家并行技术(Elastic Expert Parallelism)。该技术旨在解决大规模混合专家模型在在线服务中面临的扩展性和容错性挑战。   结论：弹性专家并行技术通过实现细粒度的专家组级别扩展，显著提升了大规模混合专家模型在动态负载下的成本效益和服务连续性。该架构结合了Ray集群编排、专家权重管理和渐进式CUDA图更新等优化技术，将扩展过程中的服务中断时间从数分钟降低到几乎无感知的水平。未来发展方向包括增强容错机制、自动化扩展调度以及跨硬件平台支持，为工业级大规模AI推理服务提供了可靠的技术基础。 |
|                       | Scaling LLMs at Apple: Ray Serve + vLLM Deep Dive \| Ray Summit 2025 | 视频介绍：本视频由Apple数据平台团队的Ankur、Rahan和Deepak共同分享，详细介绍了在Apple内部如何基于VLM和Ray构建企业级开源模型托管平台，以应对快速发展的生成式AI环境中的挑战。   结论：Apple通过构建基于VLM和Ray的开源模型托管平台，成功解决了企业级AI应用中的无缝访问、治理简化、性能可靠性和成本效益等核心挑战。该平台通过统一的模型发现、分层的生命周期管理、智能的部署策略和全面的可观测性体系，为大规模企业应用提供了稳定可靠的模型服务能力。未来的发展方向包括更智能的自动扩缩容、模型多路复用和集成的成本可观测性，展现了企业级AI基础设施的成熟演进路径。 |
|                       | State of vLLM 2025 \| Ray Summit 2025                        | 视频介绍：本视频是vLLM项目负责人Simon在Ray Summit上的主题演讲，全面回顾了vLLM开源推理和服务引擎在过去一年的发展成果与未来规划。作为目前最受欢迎的LLM推理引擎之一，vLLM致力于构建最快、最易用的开源推理解决方案，支持在各种数据中心硬件上运行开源和专有的大语言模型。   结论：vLLM项目在过去一年取得了显著进展，在API功能、模型支持、引擎核心、硬件生态和分布式能力五个关键领域都实现了重大突破。项目通过v1引擎重构大幅提升了默认性能，扩展了多模态模型支持，建立了完善的硬件插件生态系统，并在分布式推理方面取得了重要进展。随着社区贡献者接近2000人、月运行GPU小时超过40万，vLLM正成为企业LLM部署的核心基础设施。未来，vLLM将继续推进全智能体API、更强大的多模态支持、异步调度优化等前沿功能，为大规模AI推理提供更强大的基础设施支持。 |
|                       | High-Performance LLM Serving on Intel: vLLM for XPU, HPU & CPU \| Ray Summit 2025 | 视频介绍：本视频由英特尔工程师分享，详细介绍了在英特尔全平台（包括CPU、GPU和Gaudi）上运行和优化vLLM（大型语言模型推理服务框架）的最新进展和性能表现。内容涵盖平台支持现状、大规模推理解决方案以及量化技术应用。   结论：英特尔通过深度参与vLLM开源社区，为其三大计算平台（CPU、GPU、Gaudi）提供了全面的推理加速支持。在性能方面，Granite Rapids CPU相比竞品展现出色吞吐量，Arc Pro B60 GPU在性价比上优势明显，而Gaudi 3在吞吐量和成本效益上均超越H200。通过PD分拆架构和量化技术的创新，英特尔为大规模语言模型推理提供了高性能、低成本的解决方案。未来还将继续推进异构架构支持和智能路由策略，进一步提升系统效率。 |
|                       | Agentic Workload Inference at Scale: ByteDance’s AIBrix & DeerFlow \| Ray Summit 2025 | 视频介绍：本视频由Bance计算基础设施团队的工程师李光分享，重点介绍了两个开源项目AIBrix和DeerFlow如何协同工作，以优化大规模语言模型的推理部署和管理。AIBrix是一个云原生的解决方案，专门用于部署、管理和扩展大语言模型推理，而DeerFlow则是一个多智能体研究框架，结合了自动化和人工参与的任务规划。   结论：AIBrix通过其创新的分层架构、分布式KV缓存卸载、预填充与解码分离、前缀感知路由等核心技术，显著提升了大语言模型推理的性能和成本效益。与DeerFlow框架的结合展示了从复杂问题研究到高效推理的完整工作流。该项目自开源以来获得了社区的广泛认可，并与多家行业领导者展开合作，展现了在云原生AI推理领域的强大潜力。未来，AIBrix将继续专注于降低推理成本和提升系统吞吐量，推动大规模AI推理技术的发展。 |
|                       | SGLang: An Efficient Open-Source Framework for Large-Scale LLM Serving \| Ray Summit 2025 | 视频介绍: 本视频由SGLang项目的核心开发者分享，全面回顾了SGLang在2025年上半年的发展成果和战略重点，并详细介绍了在GB200和V72硬件上部署DeepSeek模型的案例研究。   结论: SGLang作为开源AI推理引擎，在2025年上半年取得了显著进展，特别是在大规模部署、训练框架集成、推测解码、长上下文优化和内核优化等五大战略重点领域。通过PD解耦和专家并行技术，在92到3000个GPU规模上实现了行业领先的性能表现，相比DeepSeek API定价实现了5倍成本降低。下半年将重点提升生产可靠性、用户体验和功能兼容性，同时继续推进硬件优化和生态系统建设。在GB200硬件上的部署案例展示了低精度内核优化带来的显著性能提升，为企业在下一代硬件上部署大模型提供了可靠解决方案。 |
|                       | Inside NVIDIA Dynamo: Faster, Scalable AI Deployment \| Ray Summit 2025 | 视频介绍：本视频由NVIDIA推理服务团队的Harry分享，重点介绍了他们最新开发的推理服务堆栈Dynamo。作为对传统Triton方案的革新，Dynamo专门针对大语言模型推理的独特挑战而设计，通过系统级优化和模块化架构，为生产级AI推理提供高性能解决方案。   结论：Dynamo通过三个核心基础创新——智能调度、内存管理和数据传输，为大语言模型推理带来了革命性的性能提升。其独特的分离式服务技术能够将预填充和解码阶段分配到不同GPU，在扩展时实现超线性性能增长。结合KV路由、KV块管理和Nixl数据传输等系统级优化，Dynamo在Llama 70B等大型模型上展示了30%到2倍以上的性能提升。模块化设计使得开发者可以灵活选用各个组件，而生产级服务功能如AI配置器、动态规划器和Grove调度器，则确保了从配置到生产的无缝过渡。Dynamo代表了下一代AI推理服务的发展方向，为大规模模型部署提供了完整的解决方案。 |
|                       | CoServe: Max Performance, Minimal Compute \| Ray Summit 2025 | 视频介绍：本视频由Cohere公司的Chenia分享，详细介绍了该公司如何优化其推理技术栈，以高效服务Command系列大语言模型。内容涵盖从模型架构设计、推理优化技术到硬件支持的全方位技术方案，展示了企业级语言模型服务在效率与成本效益方面的创新实践。   结论：Cohere通过创新的模型架构设计、多维度推理优化技术和全面的硬件支持，成功构建了高效且成本可控的企业级语言模型服务体系。其核心技术包括交错滑动窗口注意力机制、推测解码、多种量化方案以及针对不同硬件平台的深度优化，在保持模型质量的同时显著提升了推理效率。这些优化使得Command系列模型能够在少量GPU上支持超长上下文，为企业客户提供了可部署于多种环境的灵活解决方案，推动了大规模语言模型在真实业务场景中的实际应用。 |
| 大模型后训练          | Scaling LLM Post-Training at Character.AI \| Ray Summit 2025 | 视频介绍：本视频由Character AI的后训练与数据研究团队负责人Han分享，详细介绍了该公司如何基于Ray生态系统构建可扩展的大语言模型后训练技术栈，以及如何利用海量用户交互数据进行强化学习来持续提升模型质量和用户参与度。   结论：Character AI通过构建基于Ray的Raymon后训练框架和创新的强化学习系统，成功实现了对6000亿参数以上大语言模型的高效微调。该系统在分布式训练、数据处理、混合精度优化等方面都有显著创新，特别是Pipeline SFT训练框架和自定义混合精度优化器的开发解决了大规模模型训练中的稳定性问题。通过利用每日数亿条用户交互数据进行的SFT、DPO和RL训练，Character AI能够持续优化模型性能，在实际A/B测试中实现了用户参与度指标的显著提升。未来工作将继续探索多奖励模型组合、角色扮演和创意故事生成等方向，展现了利用真实用户数据驱动大模型优化的巨大潜力。 |
|                       | Meet verl: An RL Framework for LLM Reasoning & Tool Use \| Ray Summit 2025 | 视频介绍：本视频由字节的研究科学家Hongong分享，详细介绍了他们开发的强化学习训练框架verl。演讲重点探讨了大规模语言模型强化学习的重要性、系统挑战，以及verl框架如何通过混合控制器架构解决这些挑战，同时展示了最新的代理式RL训练和大模型训练能力。   结论：verl框架通过创新的混合控制器架构，成功平衡了强化学习训练中的灵活性与效率需求。该框架不仅支持多种经典和新兴RL算法，还针对代理式任务和大规模模型训练进行了专门优化。通过单控制器的算法灵活性和多控制器的高效执行，verl为复杂RL工作流提供了强大支持。未来的路线图包括更精细的组件重构、异步训练管道优化以及对多模态数据的支持，展现了其在推动RL技术发展中的重要价值。 |
|                       | Applied Intuition’s Blueprint for Scalable RL + Batch Inference \| Ray Summit 2025 | 视频介绍：本视频由Applied Intuition公司的工程师团队分享，详细介绍了他们如何利用Ray框架构建大规模GPU基础设施，支持批量推理和强化学习等关键应用场景。   结论：Applied Intuition通过构建基于Ray的统一计算平台，成功解决了在自动驾驶领域面临的大规模数据处理和模型训练挑战。该平台采用多区域多集群架构，集成Kubernetes和Kueue批处理调度器，实现了资源利用率的显著提升。在批量推理方面，通过Ray Data实现CPU和GPU操作的流水线处理，获得了10倍成本优化；在强化学习场景中，通过RLlib框架和placement groups优化，有效支持了端到端驾驶模型的训练。Ray的统一运行时环境使得公司能够在单一框架下处理多样化的工作负载，为自动驾驶技术的研发提供了坚实的技术基础 |
|                       | SkyRL: A Scalable and Flexible Post-Training Framework \| Ray Summit 2025 | 视频介绍：本视频由SkyRL团队核心成员分享，详细介绍了他们开发的开源强化学习框架SkyRL的设计理念、架构特点以及实际应用案例。演讲从强化学习在语言模型领域的发展历程切入，深入解析了SkyRL如何通过模块化设计解决传统RL框架在应对复杂任务时的局限性。   结论：SkyRL通过将训练器、生成器和控制器三大核心组件解耦，实现了前所未有的灵活性和可扩展性。该框架不仅支持多样化的训练后端和推理引擎，还能让研究人员专注于算法创新而无需担忧底层系统复杂性。从简单的数学推理到复杂的多轮工具调用任务，SkyRL都展现了出色的适应能力。未来的发展方向包括更好的异步训练支持和内存管理优化，为大规模智能体训练提供更强大的基础设施支持。 |
|                       | SkyRL tx: A unified training and inference engine \| Ray Summit 2025 | 视频介绍: 本视频由SkyRL团队分享，详细介绍了他们开发的SkyRL TX项目——一个开源的Tinker API实现。该项目旨在为研究人员和开发者提供可自定义的模型训练服务，支持多租户LoRA适配器训练，并探索统一训练与推理引擎的创新架构。   结论: SkyRL TX项目通过构建统一的训练与推理引擎，成功实现了Tinker API的开源实现，支持多租户LoRA训练、高性能优化和灵活的扩展架构。该项目在短短一个月内取得了显著进展，包括支持Quen3模型、完整训练循环、采样端点和检查点功能。未来发展方向包括更多模型支持、分布式训练优化和与现有生态系统的深度集成，展现了开源AI训练框架在标准化和可定制化方面的巨大潜力。 |
| KubeRay               | Unlocking Peak Workload Performance & Efficiency with Ray on Kubernetes \| Ray Summit 2025 | 视频介绍：本视频由Google Cloud的Ray on GKE产品经理Nisha Johnson和工程师Ryan Oolirri共同呈现，深入探讨了如何将Ray与Kubernetes相结合，构建高效、可扩展的AI工作负载平台。演讲重点解决了在构建AI平台时面临的五大核心挑战：资源访问、性能优化、平台易用性、安全性和多租户管理，并展示了Google Cloud在GKE环境中提供的完整解决方案。   结论：通过动态工作负载调度器、自定义计算类别、Kubernetes原生队列系统Q、KubeRay操作符以及TPU深度集成等创新功能，Ray与Kubernetes的组合为企业AI平台建设提供了完整的解决方案。该平台不仅实现了成本效益最大化、资源利用率优化，还支持从训练、推理到智能体工作负载的全场景覆盖。Google与Anyscale的深度工程合作确保了该平台的持续演进，使其成为构建生产级AI系统的最佳基础架构选择。 |
|                       | Ray @ Robinhood: Distributed ML Training with KubeRay \| Ray Summit 2025 | 视频介绍：本视频由Robinhood的AI基础设施团队成员Robert和Lanting分享，详细介绍了他们如何采用Ray和Kubernetes Operator（KubeRay）构建分布式机器学习训练平台，以解决单节点训练面临的资源限制和性能瓶颈问题。   结论：通过采用Ray和KubeRay，Robinhood成功构建了一个灵活、安全的分布式训练平台，将可训练数据集规模提升了7倍，显著减少了GPU资源等待时间，同时保持了与现有开发流程的无缝集成。该解决方案通过按需创建Ray集群、严格的权限控制和统一的监控体系，为金融场景下的机器学习训练提供了可靠的基础设施支持，展现了开源技术在复杂企业环境中的成功应用。 |
|                       | How KubeRay Is Evolving for Massive AI Workloads \| Ray Summit 2025 | 视频描述：在Ray Summit 2025大会上，来自谷歌的Aaron Liang和Anyscale的Jui-An Huang分享了KubeRay的最新进展，旨在显著提升团队在Kubernetes上运行Ray的用户体验。   他们介绍了一系列重大RayJob增强功能，旨在简化并强化端到端工作负载生命周期管理，包括：   可预测且可控的清理删除策略   支持周期性及自动化Ray工作负载的定时调度功能   便于与周边服务及工具集成的边车模式   提升可靠性并降低运维负担的后台状态检查机制   这些升级共同使得Kubernetes上的Ray任务编排对开发者和运维人员而言更加直观、易管理且稳定可靠。   参会者将通过实际案例深入了解这些新功能如何优化生产环境中的Ray工作负载流程，并洞察KubeRay如何持续演进，使大规模分布式计算变得比以往更加轻松便捷。 |
|                       | Scaling Ray on Kubernetes: Pragmatic Strategies for Every Team \| Ray Summit 2025 | 视频介绍：本视频由Google GKE产品经理工程师Roger和软件工程师Spencer共同分享，详细介绍了在Kubernetes上优化Ray集群规模与性能的最佳实践。内容涵盖集群生命周期管理、资源优化、性能调优和可观测性等关键主题。   结论：通过结合Kubernetes和Ray的优势，企业能够构建高性能、可扩展的AI/ML平台。视频展示了从集群配置、资源分配到故障排查的完整解决方案，特别强调了动态资源分配、自动扩缩容、节点启动优化和作业调度等关键技术。Google与Anyscale的合作进一步推动了这一生态系统的发展，为大规模AI工作负载提供了企业级的部署和管理方案。 |
|                       | Secure & Scalable AI on Ray + Kubernetes: Google’s Decoupled Agent Pattern \| Ray Summit 2025 | 视频介绍：本视频由Google Cloud团队的产品经理Brandon Royal和工程师Alex Blenco在Ray峰会上的演讲，深入探讨了如何结合Ray和Kubernetes构建生产级的智能体AI系统，重点解决了分布式系统中的弹性、安全隔离和智能调度等核心挑战。   结论：通过将Ray的应用层编排能力与Kubernetes的基础设施编排及Agent Sandbox的安全隔离能力相结合，可以构建出能够处理长时间运行任务、支持大规模并行工具执行且具备故障恢复能力的生产级智能体系统。这种解耦架构不仅提供了卓越的性能和扩展性，还通过沙箱隔离确保了代码执行的安全性，为复杂智能体工作负载的工业化部署提供了完整解决方案。 |
| Agentic AI            | How Prime Intellect Builds Scalable Infrastructure for Agentic RL \| Ray Summit 2025 | 视频介绍：本视频由Prime Intellect联合创始人兼CTO Johannes和研究负责人Will Brown共同分享，详细介绍了他们在构建开源分布式基础设施方面的最新进展，重点聚焦于测试时扩展和智能体强化学习环境的创新框架。   结论：Prime Intellect通过构建完整的开源强化学习技术栈，成功实现了从计算层到环境开发的端到端解决方案。其核心创新包括基于异步强化学习的Primer训练框架、支持多样化环境开发的Verifiers工具包，以及促进社区协作的Environments Hub平台。这些技术已在Intellect 2和正在开发的1000亿参数混合专家模型Intellect 3中得到验证，展现了开源强化学习基础设施在大规模智能体训练中的可行性和扩展性。通过开发者友好的设计理念和开放的社区协作模式，Prime Intellect正在推动强化学习技术从封闭实验室向更广泛的研究者和开发者社区普及。 |
|                       | Introducing Terminal-Bench: Evaluating LLM Agents in Realistic Terminal Settings \| Ray Summit 2025 | 视频介绍：本视频由斯坦福大学博士后Mike Merrill分享，重点介绍了他们开发的Terminal Bench项目——一个针对语言模型在真实终端环境中作为智能体性能的硬基准测试。演讲探讨了评估基准的发展历程、终端环境对AI智能体的重要性，以及该基准的设计理念和未来规划。   结论：Terminal Bench作为一个前沿的智能体评估基准，通过80个基于真实工作场景的Docker化任务，有效衡量了语言模型在终端环境中的实际工作能力。目前最先进模型的成功率仅约50-65%，表明智能体在长时程任务、工具使用和错误恢复等方面仍有显著提升空间。项目团队基于开发经验进一步构建了Harbor平台，支持任意容器化环境的智能体评估和训练，为AI智能体的发展提供了重要的评估基础设施。这些工作凸显了在AI能力快速进步的当下，开发复杂、真实的评估基准对理解和推进前沿模型能力至关重要。 |
| Robotics              | Hybrid RL + Imitation Learning for Robotics with Ray at RAI Institute | 视频介绍: 本视频由机器人与人工智能研究所（RA Institute）的Ahmed和Valerio共同呈现，详细介绍了他们基于Ray框架开发的混合强化学习与模仿学习平台。该平台旨在解决机器人机器学习中的基础设施挑战，通过模块化设计实现仿真、训练和服务组件的解耦，显著提升了训练效率和系统性能。   结论: 通过构建基于Ray的混合学习平台，研究团队成功解决了机器人机器学习中仿真与训练基础设施的集成难题。该平台通过异构资源调度、运行时依赖隔离和异步训练机制，实现了最高达2倍以上的训练加速。特别是通过教师-学生知识蒸馏框架，能够高效地将 specialized 的强化学习策略迁移到通用的模仿学习模型中，为构建大规模通用机器人策略奠定了坚实基础。未来研究方向包括更细粒度的异步训练优化和跨集群互联速度提升，展现了混合学习方法在机器人领域的巨大潜力。 |
|                       | Ray Summit 2025 Keynote: Physical AI Turing Test with Jim Fan from NVIDIA | 结论：这段演讲从克劳德·香农在麻省理工学院博物馆的“残局机器”讲起，探讨了人工智能在解决棋类游戏等抽象问题上的巨大成功，进而引出了当前AI面临的下一个，也可能是最后一个重大挑战：让机器人像人类一样在混乱、不可预测的物理世界中自如地完成日常任务，即通过“物理图灵测试”。演讲的核心在于揭示机器人技术面临的“数据困境”以及英伟达团队如何通过创新的数据策略和模型策略来攻克这一难题。演讲系统地阐述了机器人技术面临的核心挑战——数据稀缺，并详细介绍了英伟达通过构建多层次合成数据“核燃料”（从精确的物理仿真到基于生成式AI的神经模拟器）来应对这一挑战的创新方法。其“数据最大化，模型最小化”的理念，以及由此开发的Groot VLA模型，为实现通用的物理智能奠定了坚实基础。展望未来，物理API的出现将彻底改变我们与物理世界互动的方式，开启一个由机器人无缝协助日常生活、加速科学和工业发展的新时代，最终让高级机器人技术变得无处不在且毫不起眼。 |