# Ray Summit 2025演讲-有字幕-LLM总结提取关键点

## Ray Summit 2025 Keynote: Ray + Anyscale Announcements - RDMA, PyTorch Foundation, MLOps, Azure

[https://www.youtube.com/watch?v=ki5N_hpRNOk](https://www.youtube.com/watch?v=ki5N_hpRNOk)

结论：Ray Summit 2025 的主题演讲聚焦于人工智能（AI）时代的计算范式演进，以及 Ray 作为 AI 原生计算引擎在这一变革中的核心地位。演讲从计算时代的演进历史入手，强调了每个计算时代都由其独特的计算栈和引擎驱动，而 AI 时代正催生着全新的计算基础设施需求。Ray 已经确立为 AI 原生计算栈的核心计算引擎，其设计初衷（为强化学习等复杂工作负载而生）使其能够完美应对当前 AI 工作负载在规模、异构性和复杂性上的爆炸式增长。Anyscale 平台则在 Ray 开源项目之上，提供了一个统一、生产就绪的 AI 平台，集成了开发工具、运行时优化、集群管理和治理功能，旨在赋能每个团队和开发者成为 AI 团队和 AI 开发者。Ray 的社区采纳度在近一年内呈现爆发式增长（下载量增长5倍），并宣布了与 PyTorch 基金会、Google GKE 以及 Microsoft Azure 的重要合作与集成。

**关键要点**：

1. **[00:01:09](https://youtube.com/watch?v=ki5N_hpRNOk&t=69s)** **AI 时代的到来与计算栈的演进**：

   - 演讲者回顾了计算时代的演进：大型机 -> 客户端/服务器 -> 互联网/Web服务 -> 云计算 -> AI 时代。
   - 每个时代都由新的计算栈和计算引擎驱动（例如，云时代的容器和 Kubernetes）。
   - 历史经验表明，简单地“提升和转移”旧有技术栈到新时代是行不通的，成功的关键在于快速适应新范式并构建相应的基础设施。
   - AI 时代将同样驱动其专属计算栈的出现，这是不可避免的变革。
2. **[00:05:09](https://youtube.com/watch?v=ki5N_hpRNOk&t=309s)** **驱动 AI 原生计算栈出现的四大核心因素**：

   - **异构硬件**：AI 计算依赖于 CPU 和多种加速器（GPU, TPU）的协同工作，并通过高速网络（如 InfiniBand, Ultra Ethernet）和 RDMA 技术互联。
   - **复杂的工作负载流水线**：AI 处理流水线（数据预处理、训练、推理/服务）正变得日益复杂和异构。

     - 数据预处理因多模态数据而需要混合使用 CPU 和 GPU。
     - 训练从预训练扩展到后训练和强化学习。
     - 推理从单节点演变为复杂的分布式推理，涉及推理、智能体等。
   - **模型的快速演进与核心地位**：模型规模和能力日新月异，成为 AI 计算的核心。
   - **非确定性系统与迭代速度**：AI 系统本质上是非确定性的，需要持续的监控、评估和对齐。迭代速度是制胜关键。
3. **[00:08:44](https://youtube.com/watch?v=ki5N_hpRNOk&t=524s)** **AI 计算引擎的核心需求与 Ray 的使命**：

   - 新的 AI 计算栈需要满足以下核心需求：

     - 跨异构集群（CPU, GPU）动态编排大量并行任务。
     - 计算引擎的灵活性，以应对今天和未来的工作负载。
     - 高容错性，以支持运行数周的训练任务和数天的批量推理任务。
     - Pythonic 的设计，因为 Python 是 AI 领域的通用语言。
   - Ray 正是为了满足这些需求而从零开始构建的，旨在成为 AI 原生栈的计算引擎，并集成 PyTorch、VLLM 等众多 AI 框架。
4. **[00:09:48](https://youtube.com/watch?v=ki5N_hpRNOk&t=588s)** **Ray 的起源与发展历程**：

   - Ray 起源于 2015 年 UC Berkeley 的实验室，最初是为了支持复杂的强化学习（RL）研究而创建。
   - Ray 的演进与 AI 工作负载的三个阶段紧密相关：

     - **经典神经网络阶段**：工作负载相对简单，规模适中，Ray 采纳度稳步增长。
     - **生成式 AI 第一阶段（以 ChatGPT 发布为标志）** ：模型规模和训练复杂性增加，Ray 变得更为相关（例如 OpenAI 使用 Ray 训练 ChatGPT），采纳度加速。
     - **生成式 AI 第二阶段（后训练与多模态崛起）** ：工作负载复杂性急剧增加（多模态数据处理、混合专家模型、复杂并行技术和推理优化），Ray 的优势充分显现，下载量爆炸性增长。Ray 的时间就是现在。
5. **[00:26:22](https://youtube.com/watch?v=ki5N_hpRNOk&t=1582s)** **现代 AI 软件栈中的 Ray 及其技术进展**：

   - 复杂的 AI 工作负载和异构硬件催生了连接应用与硬件的软件栈。这个栈包括：

     - 顶层的分布式训练/推理框架（如 PyTorch）。
     - 中层的分布式计算引擎（Ray），负责解决扩展性、协调、数据移动、容错等分布式系统难题。
     - 底层的容器编排器（如 Kubernetes）。
   - **技术公告与进展**：

     - **加入 PyTorch 基金会**：Ray 将与该生态更紧密合作。
     - **Ray Core 的可靠性与性能**：重点投资于大规模下的可靠性（ hardened failure handling, 可观测性）和性能（宣布**原生 RDMA 支持**，实现 GPU 到 GPU 的直接张量传输，对于 RL、分布式推理和训练至关重要）。
     - **Ray 库的增强**：

       - **Ray Data**：针对多模态数据处理进行了大量优化，支持大规模批量推理和异构数据类型。
       - **Ray Serve**：为支持大模型推理，增加了大量特性（如前缀解码分离、专家并行、自定义路由等）。
     - **丰富的生态系统**：众多项目（RL 框架、智能体框架、推理框架）构建在 Ray 之上。
6. **[00:37:49](https://youtube.com/watch?v=ki5N_hpRNOk&t=2269s)** **Anyscale 平台：统一的生产级 AI 平台**：

   - 构建企业级 AI 平台不仅需要计算栈（Ray），还需要产品化能力，包括模型集成、依赖管理、开发者工具、性能提升、安全策略、治理和部署。
   - Anyscale 平台正是为此而建，其营收在过去八个季度增长了 10 倍。
   - **客户案例**：

     - **Tripadvisor**：统一批处理和在线推理，将批处理嵌入成本降低 70%，代码冗余减少 80%。
     - **Coactive**：将开发到生产的时间从数周缩短至一天，成本降低 4 倍，GPU 计算成本降低 75%。
     - **Nubank**：使用 Ray Train 和 Ray Data 训练内部 Transformer 模型，向 AI 优先公司转型。
7. **[00:41:59](https://youtube.com/watch?v=ki5N_hpRNOk&t=2519s)** **Anyscale 平台的三层架构与开发者体验**：

   - Anyscale 平台主要由三层构成：

     - **开发者中心**：提供 Workspaces（无限的笔记本电脑体验）、Jobs（作业调度）、Services（生产就绪的服务端点）、Observability（指标、日志、追踪、数据仪表板、关键事件和引导式修复）以及新发布的 **Lineage Tracking（血缘追踪）** ，自动跟踪模型和数据转换。
     - **Anyscale Runtime**：完全兼容 Ray 开源 API，是经过优化的计算基底，在数据、训练和服务工作负载上相比开源 Ray 有显著性能提升（数据工作负载快 2-10 倍，服务吞吐量高 7 倍），并具备动态内存管理、训练中期恢复、弹性训练等增强特性。
     - **集群控制器**：负责端到端的计算生命周期管理，具备亚分钟级集群启动、智能自动扩缩容、多云多 GPU 支持以及治理功能（成本控制、预算）。
8. **[01:03:27](https://youtube.com/watch?v=ki5N_hpRNOk&t=3807s)** **基础设施创新与合作伙伴关系**：

   - **多资源云**：允许一个 Anyscale Cloud 绑定多个云资源（跨区域、跨云提供商、跨计算栈），智能搜索可用容量以满足工作负载需求。
   - **全局资源调度器**：跨所有 Ray 集群有效分配和优先化受限资源（包括预留、按需和 Spot 实例），实现更高的利用率和成本效益，支持抢占和恢复。
   - **重要合作伙伴公告**：

     - **与 Google GKE 深度合作**：共同优化 Ray 与 Kubernetes 的集成，实现更快的任务启动、更好的资源装箱和可靠性。
     - **Anyscale on Azure (私有预览)** ：作为 Azure 上的原生第一方服务发布，深度集成 Azure 基础设施（Azure Portal、AKS、Entra ID、Azure 承诺），为企业客户提供开箱即用的安全、可扩展的 Ray 体验。微软的 Brendan Burns 登台强调了开源创新、混合 AI 以及 democratizing AI（普及AI）的重要性。

---

## Ray Summit 2025 Keynote: Physical AI Turing Test with Jim Fan from NVIDIA

[https://www.youtube.com/watch?v=7fDiui8cAVQ](https://www.youtube.com/watch?v=7fDiui8cAVQ)

结论：这段演讲从克劳德·香农在麻省理工学院博物馆的“残局机器”讲起，探讨了人工智能在解决棋类游戏等抽象问题上的巨大成功，进而引出了当前AI面临的下一个，也可能是最后一个重大挑战：让机器人像人类一样在混乱、不可预测的物理世界中自如地完成日常任务，即通过“物理图灵测试”。演讲的核心在于揭示机器人技术面临的“数据困境”以及英伟达团队如何通过创新的数据策略和模型策略来攻克这一难题。演讲系统地阐述了机器人技术面临的核心挑战——数据稀缺，并详细介绍了英伟达通过构建多层次合成数据“核燃料”（从精确的物理仿真到基于生成式AI的神经模拟器）来应对这一挑战的创新方法。其“数据最大化，模型最小化”的理念，以及由此开发的Groot VLA模型，为实现通用的物理智能奠定了坚实基础。展望未来，物理API的出现将彻底改变我们与物理世界互动的方式，开启一个由机器人无缝协助日常生活、加速科学和工业发展的新时代，最终让高级机器人技术变得无处不在且毫不起眼。

**关键要点**：

1. [00:00:05](https://youtu.be/7fDiui8cAVQ?t=5) 从抽象智能到物理智能的挑战：

   - 演讲者以香农的“残局机器”为例，指出早期AI研究者曾认为解决国际象棋就是AI的终极目标。
   - 过去70年，AI在棋类、扑克、电子游戏乃至数学奥林匹克和蛋白质折叠等抽象领域取得了远超预期的成就。
   - 然而，一个看似简单却极其困难的挑战依然存在：让机器人能够清理派对后的混乱房间、准备烛光晚餐，并让人无法分辨是人类还是机器人所为。演讲者将此称为“物理图灵测试”，并认为这是AI的下一个重大挑战。
2. [00:01:52](https://youtu.be/7fDiui8cAVQ?t=112) 当前机器人技术的笨拙现状与核心困境：

   - 视频展示了最先进的人形机器人在尝试简单任务（如抓取、烹饪）时的频繁失败，显得笨拙且不可靠。
   - 其根本原因在于数据稀缺。与大型语言模型（LLM）可以利用海量互联网文本数据（被称为“化石燃料”）不同，机器人技术需要的是高维、连续的关节控制数据，这些数据无法从互联网上直接获取，必须通过物理方式收集，过程缓慢且昂贵。
3. [00:03:35](https://youtu.be/7fDiui8cAVQ?t=215) 机器人数据的“金字塔”与“核燃料”愿景：

   - 数据策略是解决机器人问题的两大支柱之一。
   - 当前主要的数据收集方式是“遥操作”：操作员佩戴VR设备，实时控制机器人完成任务。这种方式数据质量高（“人力燃料”），但极其耗时，每天每台机器人最多只能收集约4小时数据，无法规模化。
   - 演讲者提出了一个数据金字塔：顶端是稀缺的“真实数据”，底部是LLM使用的“化石燃料”数据。而最令人兴奋的是中间的“合成数据”，它被比喻为“核燃料”，具有近乎无限的潜力，是推动机器人革命的关键。
4. [00:05:28](https://youtu.be/7fDiui8cAVQ?t=328) 仿真1.0：构建“数字孪生”与仿真原理：

   - 这是生成“核燃料”的第一种方法。通过在GPU上进行大规模并行物理仿真（如Isaac Lab），可以极大地加速训练过程（比实时快10,000倍）。
   - 关键技术是“领域随机化”：在成千上万个仿真环境中随机改变重力、摩擦力等物理参数。遵循“仿真原则”，一个能在百万种不同物理参数的仿真世界中掌握技能的AI模型，有很大概率能直接（零样本）应用于现实世界。
   - 应用案例包括训练机械手转笔、机器狗平衡和行走，甚至处理瑜伽球这种难以精确仿真的软体物体，并成功转移到现实世界。
5. [00:08:45](https://youtu.be/7fDiui8cAVQ?t=525) 迈向仿真2.0：生成式AI与“神经物理引擎”：

   - 仿真1.0需要人工创建精确的“数字孪生”，成本高且速度慢。
   - 仿真2.0利用生成式AI（如文本生成3D模型、扩散模型改变纹理）自动、程序化地生成无限多样的仿真环境和资产。英伟达开源的Robocassa引擎就是此类“混合仿真引擎”的代表。
   - 更进一步，通过“Groot Dreams”技术，利用视频基础模型作为“神经模拟器”。它能够根据语言指令，从相同的初始画面模拟出不同的未来（例如，拿起苹果还是拿起罐头）。这种方法不关心场景复杂度，计算效率恒定，并能自动学习光影、反射和物体力学，是“由互联网视频和数据编程”的“神经物理引擎”。
   - 演讲者甚至展示了在视频世界模型中实时遥操作机器人的前瞻性技术。
6. [00:14:10](https://youtu.be/7fDiui8cAVQ?t=850) 数据最大化与模型最小化的核心哲学：

   - 演讲过半才提及模型，强调了其核心理念：**数据最大化，模型最小化**。
   - 机器人专家数据匮乏，因此不能挑剔，必须整合所有类型的数据（真实数据、仿真1.x数据、仿真2.x数据），通过一个简单的加权采样配方进行协同训练。事实证明，加入合成数据能显著提升模型性能。
7. [00:15:02](https://youtu.be/7fDiui8cAVQ?t=902) Groot VLA模型：从光子到动作的具身AI大脑：

   - 模型架构受“快与慢思考”启发，包含两个系统：

     - 系统二（慢思考）：一个VLM，负责理解语言指令并进行深思熟虑的推理。
     - 系统一（快思考）：一个扩散模型，用于生成连续、平滑的机器人动作，反应快速，运行频率超过100赫兹。
   - 这就是英伟达开发的Groot N1视觉-语言-动作基础模型。它仅含20亿参数，结构紧凑，但能通过不同的输出头适配多种机器人硬件。
   - 演示展示了Groot能够理解“拿最健康的零食”这样的复杂指令，并通过推理选择苹果。另一个演示是机器人能够连续数小时无故障地组装GPU，展现了极高的鲁棒性。
8. [00:17:54](https://youtu.be/7fDiui8cAVQ?t=1074) 未来展望：物理API与技能经济：

   - 随着物理AI的成熟，世界将出现“物理API”——一个用于编程和操纵原子世界的接口。
   - 这将催生一系列新事物：物理提示方法、具身MCP、智能体舰队协调、可编程工厂（快速定制生产）、自动驾驶湿实验室（自动化科学发现）。
   - 最终，将形成一个“物理技能经济”，一个终极应用商店，让任何人都能访问和享受人类所有灵巧性的总和。届时，机器人将融入背景，解决“物理图灵测试”将变得像星期二一样平常。

---

## Ray Summit 2025: Jimmy Ba on Efficient Teams, AI Research, and What’s Next 

[https://www.youtube.com/watch?v=dK3Y3r0G3_Y](https://www.youtube.com/watch?v=dK3Y3r0G3_Y)

结论：本次访谈揭示了xAI以其独特的“小团队、高对齐”运营模式和对“极致求真”AI的不懈追求，在竞争激烈的人工智能领域开辟了一条不同的道路。他们清醒地认识到，当前AI发展的前沿阵地已从算法创新转向了解决前所未有的大规模工程挑战。xAI的愿景远不止于改进聊天助手，而是致力于利用不断进化的AI智能，协同多智能体和机器人技术，去攻克人类面临的最根本、最艰巨的实体世界难题，从而深刻塑造未来的经济形态和人类社会。

**核心要点：**

1. [00:00:20](https://youtu.be/dK3Y3r0G3_Y?t=20s) **高效运营模式：小团队、高人才密度与矢量对齐**

   - xAI的成功基石在于其独特的运营模式：坚持全员线下办公，将顶尖人才聚集一处，通过高强度协作“压榨”出卓越成果，形象地比喻为“制造钻石”。
   - 团队规模极小但效率极高：Grok 1由约12人开发，Grok 2由约30人完成。
   - 公司结构极其扁平，旨在实现快速的迭代循环。其核心理念是“矢量对齐”——确保公司内每个成员（每个矢量）的目标和方向高度一致，即使团队规模小，也能因方向统一而爆发出巨大的前进动能。
   - 沟通极度透明：全公司使用一个群聊（如Slack频道或X群组），形成了“单一的集体意识”，确保信息快速流通和决策高效。
2. [00:04:00](https://youtu.be/dK3Y3r0G3_Y?t=240s) **核心使命：构建极致求真（Maximum Truth Seeking）的AI**

   - xAI成立的初衷是打造一个以“极致求真”为最高准则的AGI（通用人工智能）。这与许多对AI行为进行严格控制和观点塑造的模型形成鲜明对比。
   - 公司将控制权交给开发者，自身不对AI应如何表现持过多主观意见。
   - 提出了“伽利略测试”这一核心概念：批评当前AI模型通过最小化互联网数据的平均损失进行训练，这实质上是学习“多数共识”，而历史上多数共识可能是错误的（如地心说）。xAI的目标是让AI能像伽利略一样，穿透噪音，通过第一性原理和实验工具发现真理。
   - Grok百科是迈向此目标的一步，旨在让AI审查所有原始资料，质疑二手信息，从第一性原理出发生成新的知识总结。
3. [00:08:47](https://youtu.be/dK3Y3r0G3_Y?t=527s) **企业愿景与模型智能的演进**

   - 模型智能飞速发展：从2023年Grok 1仅具备“高中生”智能水平，到如今Grok 4能解决部分“研究生”级别的复杂推理任务。
   - 然而，大多数用户提出的问题（如解数学题、写论文）并未发生质变。这引出一个根本问题：为何要持续构建更智能的模型？
   - xAI的答案是：旨在利用这些高度智能的模型去解决人类面临的最艰巨挑战，例如：

     - 在火星上建立殖民地。
     - 部署数百万机器人。
     - 实现清洁能源突破。
     - 加速材料科学等领域的科学发现。
   - 企业级应用的焦点在于利用AI实现业务流程的自动化与优化，从而极大加速科学进步。
4. [00:10:30](https://youtu.be/dK3Y3r0G3_Y?t=630s) **对AI研究现状的反思：工程挑战远大于基础研究**

   - 回顾过去十年AI发展，认为核心思想（如Transformer、Adam优化器）大多在约八年前已被发现。
   - 近年的进展更多是“工程上的极致推进”，而非基础研究的突破。xAI内部甚至“禁止使用‘研究’一词”，将当前工作比作“阿波罗登月计划”——目标明确，核心是解决前所未有的工程难题。
   - 列举了大规模训练中面临的严峻工程挑战：

     - 可靠性：在数万GPU集群上， cosmic rays（宇宙射线）等微小事件都可能导致比特翻转，引发训练崩溃，需要极其复杂的健康检查机制。
     - 垂直整合：xAI自建数据中心和计算集群，需要从硬件、冷却、网络拓扑到训练框架、工作负载编排的全栈优化。
     - 异构计算：未来的训练将不再是单一的同步梯度计算，而是推理、反向传播等不同任务在异构节点上异步进行，调度和权重同步极为复杂。
     - 长时任务推理：随着模型能处理长达数小时的任务，强化学习中的数据生成和梯度更新面临新的“扼流”问题。
5. [00:18:10](https://youtu.be/dK3Y3r0G3_Y?t=1090s) **计算规模与数据供给的未来**

   - 对于“如果有百倍、千倍算力会做什么”的回答是：毫不犹豫地用于创造更智能的模型。因为未来大部分算力消耗将发生在推理和强化学习的数据生成阶段，而非仅仅预训练。
   - 指出人类标注数据的“认知供应链”即将达到瓶颈。培养一个人类专家需要数十年，而AI模型迭代仅需数月。
   - 解决方案在于“自我对弈”和“自我改进”循环。通过AI模型自身生成高质量数据，并在此过程中超越其“教师模型”。
   - 类比人类社会：人类大脑硬件（神经网络）百万年来变化不大，但通过多智能体社会（村庄、城邦间的竞争与合作）形成了“信息永动机”，实现了知识的持续积累和创新。AI的未来发展也需要引入这种多智能体竞争与协作的动态。
6. [00:24:39](https://youtu.be/dK3Y3r0G3_Y?t=1479s) **未来展望：机器人与实体世界的交互**

   - 对未来几年最期待的领域是机器人技术。预测在未来两三年内，世界将出现“数百万台机器人”。
   - 这些机器人将致力于解决实体世界中的核心问题，例如：

     - 供应链与制造业。
     - 建立晶圆厂等极其复杂的工程。
     - 最终实现全球经济的“10倍规模”扩张，这需要大规模地“移动原子”，而不仅仅是“移动比特”。
   - 当被问及人类在此世界中的角色时，幽默地提及xAI发布的“伴侣机器人”，暗示AI和机器人将承担更多工作，人类可能需要重新定义自身的价值与活动。

---

## Ray Summit 2025 Keynote: AI OSS Stack Panel with vLLM + PyTorch + Kubernetes

[https://www.youtube.com/watch?v=cMAzh2GeiNM](https://www.youtube.com/watch?v=cMAzh2GeiNM)

**结论：**   
本视频汇集了多位AI与机器人领域顶尖专家在Ray峰会上的分享，核心脉络围绕**智能技术在物理世界和复杂软件工程中的规模化应用与挑战**。从Applied Intuition将车辆智能拓展至全球汽车、矿业和国防行业，到Physical Intelligence致力于开发通用机器人模型以应对多样化物理任务，再到xAI通过极致工程效率和小团队模式追求构建最大限度寻求真理的AI，以及Cursor利用强化学习和协同设计打造高速、智能的编码助手，这些演讲共同揭示了几个关键趋势：**基础设施和工程能力已成为驱动AI前沿突破的核心要素**；**针对特定领域进行深度优化和规模化（无论是通过行业合作、数据收集还是RL）是提升AI实用性的有效路径**；以及**AI技术本身正开始赋能其自身的研发进程，形成加速循环**。尽管在机器人泛化、可靠性、记忆和速度等方面仍存在开放挑战，但演讲者们对未来几年AI，特别是机器人和物理智能的规模化应用表达了强烈的信心和期待。

1. [00:01:21](https://youtube.com/watch?v=cMAzh2GeiNM?t=81s) 演讲者介绍：Peter Lewig，Applied Intuition联合创始人兼CTO，公司致力于为全球车辆带来智能。
2. [00:02:24](https://youtube.com/watch?v=cMAzh2GeiNM?t=144s) 汽车行业现状分析：纠正公众认知误区，指出丰田、大众、现代、斯特兰蒂斯和通用汽车是全球真正的市场领导者，而特斯拉仅占全球市场份额的2%（基于2023年数据）。这些传统车企拥有深厚的技术积累、强大的品牌和庞大的生产资产，市场份额变化缓慢。
3. [00:03:50](https://youtube.com/watch?v=cMAzh2GeiNM?t=230s) 汽车行业核心优势与待加强领域：

   1. 核心优势：

      1. 硬件工程与系统集成：现代车辆包含数千个组件，行业在此方面能力卓越。
      2. 功能安全：车辆机械安全性高，经过大量测试和验证。
      3. 卓越制造：现代化工厂生产效率极高。
   2. 待加强领域：

      1. AI技术应用：将AI技术整合到车辆本身的能力相对较弱。
      2. 软件定义安全：随着更先进的AI技术上车，确保其安全可靠运行成为关键挑战。
4. [00:05:34](https://youtube.com/watch?v=cMAzh2GeiNM?t=334s) Applied Intuition的三大产品支柱：

   1. 工具与基础设施：支持自动驾驶系统及整个车辆软件开发生命周期的工具链，许多现有的高级驾驶辅助系统都使用了他们的工具。
   2. 车辆操作系统：一个现代化的软件平台，让汽车制造商开发车辆软件变得像开发手机应用一样容易，是AI技术在全球车辆中量产的关键。
   3. 自动驾驶产品：为整个行业（包括汽车、卡车、飞机、无人机等所有领域）提供自动驾驶解决方案，这些产品基于其工具和操作系统构建。
5. [00:07:29](https://youtube.com/watch?v=cMAzh2GeiNM?t=449s) 数据引擎与Ray的应用：大规模数据收集、处理、标注、训练和评估是其技术和战略的核心。他们使用Ray来扩展这项工作，利用大量GPU进行高效计算。
6. [00:08:00](https://youtube.com/watch?v=cMAzh2GeiNM?t=480s) 自动驾驶系统示例：展示了其端到端架构的自驾系统SDS for cars，该系统集成了视觉语言模型，能够描述驾驶时的思考过程。借助Ray作为HPC基础设施的核心部分，系统每周都在显著改进。
7. [00:09:15](https://youtube.com/watch?v=cMAzh2GeiNM?t=555s) 行业合作与规模化应用案例：

   1. [00:09:24](https://youtube.com/watch?v=cMAzh2GeiNM?t=564s) 斯特兰蒂斯：全球第四大汽车制造商，将采用Applied Intuition的车辆操作系统用于下一代车型，以带来顶尖的车内体验。这涉及到复杂的电气架构和长达15年的车辆生命周期承诺，最终将扩展到数百万辆汽车。
   2. [00:10:34](https://youtube.com/watch?v=cMAzh2GeiNM?t=634s) 小松：全球第二大矿业设备制造商，与Applied Intuition达成为期六年的战略合作伙伴关系，这是小松104年历史上最重要的技术投资。目标是将其自动驾驶技术从仅占全球服务矿卡3%的市场份额扩展到剩余的97%。这需要车辆具备更强的车载智能，以应对各种场景，而无需持续连接。
   3. [00:14:19](https://youtube.com/watch?v=cMAzh2GeiNM?t=859s) 五十铃：日本最大的卡车制造商，合作应对日本因人口老龄化和劳动法修订导致的卡车司机短缺问题（被称为"2024问题"）。在日本政府组织的自动驾驶卡车竞赛中，五十铃使用Applied技术完成了3000公里、为期7天的复杂路况自动驾驶测试。目标是到2028年实现该自动驾驶卡车的商业化。
8. [00:17:31](https://youtube.com/watch?v=cMAzh2GeiNM?t=1051s) 国防领域的车辆智能：乌克兰战争展示了低成本无人机作战系统的威力。Applied Intuition已将其技术集成到超过十几种不同类型的国防车辆中。强调了协同自主的重要性，而不仅仅是单车自主。与Ax Aerospace的合作在一次测试中成功执行了19次任务，成功率100%。
9. [00:21:43](https://youtube.com/watch?v=cMAzh2GeiNM?t=1303s) Chelsea Finn演讲开场：Physical Intelligence联合创始人，斯坦福大学教授，机器人学习领域的先驱。演讲主题是将AI带入物理世界。
10. [00:22:56](https://youtube.com/watch?v=cMAzh2GeiNM?t=1376s) 物理AI的挑战与目标：目前为特定机器人应用（如仓库物流、实验室自动化、厨房烹饪）构建解决方案非常困难，通常需要围绕下游应用组建整个公司，从头开始定制硬件和软件。Physical Intelligence的目标是开发一个通用模型，使任何机器人都能执行任何任务，其核心论点是通用模型可能比专用模型更好、更容易开发，类似于语言模型的发展趋势。
11. [00:23:54](https://youtube.com/watch?v=cMAzh2GeiNM?t=1434s) 数据来源的考量与策略：虽然规模很重要，但工业自动化数据缺乏多样性，YouTube数据存在具身鸿沟，仿真数据缺乏真实感。Physical Intelligence押注于扩展在真实机器人上收集的真实数据，通过遥操作（Teleoperation）来启动。
12. [00:26:19](https://youtube.com/watch?v=cMAzh2GeiNM?t=1579s) 模型进展与泛化挑战：展示了一年前的PI Zero模型能够执行折叠衣物等长周期任务，并能通过微调适应清理桌子、控制不同机器人等新用例。但早期模型在泛化到新环境方面存在局限，容易受环境变化影响。
13. [00:28:05](https://youtube.com/watch?v=cMAzh2GeiNM?t=1685s) 实现机器人泛化到新环境：为了解决机器人在陌生家庭中执行任务（如整理厨房）的挑战，他们在旧金山多个不同的家庭以及模拟厨房/卧室中收集了大规模、多样化的真实世界数据，覆盖超过100个独特房间。关键发现是，即使在预训练数据混合中，移动操作数据仅占2.4%，也能通过基础通用模型显著提升在新环境中的性能，体现了通用模型的力量。
14. [00:30:53](https://youtube.com/watch?v=cMAzh2GeiNM?t=1853s) 改进语言指令跟随：通过修改模型架构，在预训练阶段预测离散化动作并在后训练阶段结合扩散模型，同时停止梯度回传，成功保留了预训练视觉语言模型的语言理解能力，将语言跟随率从20%提升到80%。
15. [00:32:52](https://youtube.com/watch?v=cMAzh2GeiNM?t=1972s) 泛化能力演示：结合多样化数据和新的训练方法，机器人能够被放置在从未见过的家庭中，成功执行关闭橱柜、将碗碟放入水槽、清理溢出物等指令，展现了在语言、视觉和行动模态上的泛化能力。
16. [00:36:47](https://youtube.com/watch?v=cMAzh2GeiNM?t=2207s) 处理开放式提示和插话：通过分层模型（高层视觉语言模型解释开放式提示并生成具体指令，低层视觉语言动作模型执行原子命令）并结合语言模型合成生成大量训练提示，使机器人能够响应复杂的开放式指令（如制作特定类型的三明治）和在任务执行过程中接受插话与修正。
17. [00:40:33](https://youtube.com/watch?v=cMAzh2GeiNM?t=2433s) 模型开源与应用扩展：PI Zero和PI 0.5模型已开源，并被社区用于微调至手术机器人、四轴飞行器等远超预期的领域，证明了模型的通用性。
18. [00:41:19](https://youtube.com/watch?v=cMAzh2GeiNM?t=2479s) 物理AI面临的开放挑战：

      1. 记忆问题：当前最先进的机器人模型通常没有记忆，无法记住已完成的操作。
      2. 任务执行速度：需要提高机器人完成任务的速度，达到或超过人类水平。
      3. 可靠性：需要将成功率从80%提升到更高，以实现实际部署。
19. [00:43:48](https://youtube.com/watch?v=cMAzh2GeiNM?t=2628s) 总结与展望：通用模型通常比专用模型更成功，这在物理AI和机器人领域将继续如此。大规模真实世界数据是解决物理智能问题的必要但不充分条件，仍需更多研究才能在实际场景中部署这些机器人。
20. [00:44:17](https://youtube.com/watch?v=cMAzh2GeiNM?t=2657s) Jimmy Ba访谈开场：xAI联合创始人。
21. [00:44:34](https://youtube.com/watch?v=cMAzh2GeiNM?t=2674s) xAI的运营模式：团队规模小，人才密度高，全员线下办公，强调快速迭代和矢量对齐。公司结构扁平，通过单一的群聊（X group chat）实现快速决策和信息同步，形成一种"集体意识"。
22. [00:48:13](https://youtube.com/watch?v=cMAzh2GeiNM?t=2893s) xAI的产品理念与"伽利略测试"：公司的根本信念是构建最大限度寻求真理的AI，将控制权交给开发者。提出了"伽利略测试"概念，指出当前模型训练最小化平均损失是在学习互联网上的多数共识观点，但这可能不是真理（如伽利略时代地心说曾是共识）。xAI致力于让AI能够去伪存真，从第一性原理进行推理。
23. [00:50:28](https://youtube.com/watch?v=cMAzh2GeiNM?t=3028s) 实现真理寻求的实践：Grokpedia是迈向此目标的一步，旨在通过查阅所有主要来源、从第一性原理思考来重写整个数据集，为AI训练创造最大价值的数据集。社区笔记（Community Notes）和AI作为事实核查者也是获取反馈和促进真理的途径。
24. [00:52:26](https://youtube.com/watch?v=cMAzh2GeiNM?t=3146s) 企业级应用的思考：AI模型智能已从"高中生"水平爆炸性增长到"研究生"水平，但大多数聊天助手用户的问题仍然相对基础。xAI认为构建更智能模型的终极目标是用它们来解决人类面临的最棘手问题，如将人类送上火星、清洁能源、药物研发、加速材料科学发现等，通过自动化业务流程和优化来加速科学进步。
25. [00:54:41](https://youtube.com/watch?v=cMAzh2GeiNM?t=3281s) AI研究过去十年的变化：认为过去八年在核心研究思想上进展不大，许多核心思想（如Transformer、Adam优化器、归一化、扩散模型）在多年前已被发现。当前的竞争更多是工程挑战，在于组合这些已有思想、快速运行并在技术树中选择正确的分支。
26. [00:57:26](https://youtube.com/watch?v=cMAzh2GeiNM?t=3446s) xAI内部对"研究"的看法与工程挑战：在xAI内部，"研究"一词实际上被禁止使用，因为他们将构建AI视为类似阿波罗登月计划的工程挑战。面临的极端工程挑战包括在数十万GPU上协调训练、处理硬件错误（如宇宙射线导致的比特翻转）、构建自己的数据中心以实现更快更便宜的部署、以及处理异构工作负载（推理、反向传播异步进行）等。
27. [01:02:24](https://youtube.com/watch?v=cMAzh2GeiNM?t=3744s) 计算规模与数据生成：如果有千倍更多的GPU，大部分计算将用于推理和强化学习中的数据生成。随着模型能处理的任务时长从2小时扩展到8小时，推理时间将呈指数级增长。即使算法效率提升10倍，对更智能模型的需求仍会驱动对更多计算的需求。
28. [01:04:35](https://youtube.com/watch?v=cMAzh2GeiNM?t=3875s) 人类标注数据的局限与自我改进：人类标注数据（"肉计算机"）的增长速度（18年）远跟不上模型迭代速度（29个月从Grok 1到Grok 4），导致"认知供应链"出现瓶颈。自我对弈和自我改进循环变得至关重要，这类似于人类社会的进步轨迹，通过多智能体系统形成村庄、国家间的合作与竞争动态，推动知识和创新的持续增长。
29. [01:09:03](https://youtube.com/watch?v=cMAzh2GeiNM?t=4143s) 对未来几年的展望：特别看好机器人领域。认为机器人技术的发展可能会比自动驾驶更快，借鉴了自动驾驶领域的经验教训（如真实世界反馈循环和大规模数据收集的重要性）。预测未来两三年将出现数百万机器人，它们将负责物流、供应链、制造业，甚至建立晶圆厂等复杂任务，通过移动原子来真正实现全球经济的指数级增长。
30. [01:11:34](https://youtube.com/watch?v=cMAzh2GeiNM?t=4294s) Sasha Rush演讲开场：Cursor的研究科学家，介绍Cursor Composer。
31. [01:12:14](https://youtube.com/watch?v=cMAzh2GeiNM?t=4334s) Cursor Composer简介：新发布的基于智能体的LLM，结合了一流的编码智能和高效的生成速度。在内部基准测试中，性能接近最好的前沿模型，优于去年夏季发布的模型和开源模型，同时令牌生成效率比同等智能模型高4倍，速度也显著快于专为快速编码设计的模型。
32. [01:13:02](https://youtube.com/watch?v=cMAzh2GeiNM?t=4382s) 开发动机：受Cursor应用中快速、智能的"Cursor Tab"功能启发，希望为智能体编码提供类似的快速交互体验。早期原型"Cheetah"因用户体验好而获得积极反馈，促使他们构建更智能且保持高效能的Composer。
33. [01:14:58](https://youtube.com/watch?v=cMAzh2GeiNM?t=4498s) Composer的体验演示：用户提交查询后，智能体立即并行调用多种工具（终端命令、代码库搜索、编辑等），在一两秒内完成完整编辑并总结，提供了不同于传统编辑器内智能体的体验。
34. [01:16:13](https://youtube.com/watch?v=cMAzh2GeiNM?t=4573s) Cursor智能体工作原理与RL流程：智能体通过生成令牌（形成XML模式）来调用工具（如读文件、编辑文件、代码库搜索、运行终端命令等）。RL过程中，从同一起点进行多次 rollout，对不同的工具调用序列进行评分，并基于优势更新模型参数。
35. [01:19:27](https://youtube.com/watch?v=cMAzh2GeiNM?t=4767s) 智能体RL的核心挑战与基础设施解决方案：

      1. 训练与推理匹配：训练大型混合专家模型并在数千GPU上分布式进行，需保持训练和采样版本的同步。

         - 解决方案：开发用于低精度训练的自定义内核，使用MXFP8微缩放格式，在Blackwell芯片上为MoE层带来3.5倍加速，且无需后训练量化。
      2. 长且异构的Rollout：真实编码任务的rollout可能使用10万到100万个令牌，进行数百次工具调用，且不同rollout的调用次数和时间差异很大。

         - 解决方案：使用Ray和单一控制器接口进行负载均衡，高效管理rollout过程。
      3. 一致性：训练环境需与生产环境（Cursor产品）保持一致。

         - 解决方案：利用Cursor Cloud Agents的相同基础设施（微VM）来创建用于训练的状态化环境，确保工具格式和响应与生产环境一致。这使得模型能够成为语义搜索等强大工具的"高级用户"。
36. [01:27:24](https://youtube.com/watch?v=cMAzh2GeiNM?t=5244s) Composer发布初期成果：

      1. RL有效性：随着RL步骤（计算量）的增加，模型在内部基准上的性能持续提升，达到发布水平，证明了RL在困难专业任务上的可扩展性。
      2. 行为改进：模型学会了调用更多并行工具以加快响应速度，并通过阅读更多文件和进行更多搜索来做出更明智的编辑决策。
      3. 用户反馈：用户欣赏其速度与智能的结合，能够快速获得结果并继续下一个问题，改变了编码体验。
37. [01:29:58](https://youtube.com/watch?v=cMAzh2GeiNM?t=5398s) 经验总结：

      1. RL非常适合构建专业化模型，实现了从通用大模型到定制化领域专家的范式转变。
      2. AI系统本身改变了研发流程，团队使用自己构建的智能体来开发仪表板、后端等，使小团队也能快速推进。
      3. 强化学习的进展很大程度上由基础设施发展驱动，需要将产品、规模和机器学习训练紧密结合，凸显了基础设施在现代软件系统中的重要性。

‍

---

## Ray Summit 2025: Bringing AI to the Physical World with Chelsea Finn from Physical Intelligence

[https://www.youtube.com/watch?v=mej049HSFg4](https://www.youtube.com/watch?v=mej049HSFg4)

结论:由Physical Intelligence公司的联合创始人介绍将人工智能引入物理世界所面临的挑战与机遇。核心论点是，开发一个通用模型，让任何机器人执行任何任务，可能比针对特定应用开发专用模型更优、更容易，这借鉴了大语言模型（LLMs）的成功经验。通用模型在物理AI和机器人应用中，其表现往往优于专用模型，并且这一趋势预计将持续。大规模真实世界数据对于解决物理智能问题是必要的，但并非充分条件。尽管取得了显著进展，但在机器人真正部署到现实世界之前，仍需进行大量的研究来克服现有挑战。通过多样化的数据、有效的预训练和微调方法，机器人已经能够在全新环境中执行任务并响应开放式指令，展现了物理AI的巨大潜力。

关键要点：

1. [00:00:17](https://youtube.com/watch?v=mej049HSFg4&t=17s) **物理AI的挑战与通用模型愿景**：

   1. 当前将机器人技术产品化极其困难，因为每个下游应用（如仓库物流、实验室自动化、厨房烹饪）通常都需要成立专门的公司，从头开始定制硬件、软件和动作模式。
   2. Physical Intelligence公司的目标是开发一个通用模型，使任何机器人能够执行任何任务。
   3. 核心论点是，借鉴大语言模型的成功经验，一个在更广泛数据上训练的通用模型可能比专用模型表现更好、开发更容易。
2. [00:02:04](https://youtube.com/watch?v=mej049HSFg4&t=124s) **规模的重要性与数据来源的权衡**：

   1. 规模是实现通用物理AI模型的关键要素。
   2. 然而，不同的数据来源各有优劣：

      1. 工业自动化数据：规模巨大，但行为多样性不足。
      2. YouTube视频数据：规模巨大，但存在“具身鸿沟”，即人类行为方式与机器人执行方式之间存在显著差异。
      3. 模拟数据：规模巨大，但缺乏物理和图形上的真实感。
   3. 最终结论是，规模虽重要，但必须服务于解决实际问题。Physical Intelligence公司押注于扩展在真实机器人上收集的真实数据，最初通过遥操作（Teleoperation）方式启动数据收集，如视频中展示的通过操作臂控制机器人点燃蜡烛。
3. [00:04:29](https://youtube.com/watch?v=mej049HSFg4&t=269s) **早期成果：PI Zero模型与任务泛化**：

   1. 展示了一年前开发的PI Zero模型，能够执行折叠衣物等长周期任务，并能从错误中自主恢复。
   2. 该模型的“配方”并非任务特定，同一个基础模型（PI Zero）可以通过微调应用于不同的用例，如清理桌子、为不同机器人控制舀取咖啡豆、组装纸箱、点燃蜡烛等，展示了初步的通用性。
4. [00:05:47](https://youtube.com/watch?v=mej049HSFg4&t=347s) **核心挑战与环境泛化**：

   1. PI Zero模型的主要局限在于其数据收集于特定环境，导致模型无法有效泛化到新环境，甚至对原有环境的微小变化也显得脆弱。
   2. 目标是在机器人从未见过的环境中（如一个新家的厨房）也能成功执行任务（如整理厨房），这要求模型能同时在语言理解、视觉感知和动作执行三个模态上实现泛化，例如适应不同的柜台高度、橱柜把手形状等。
5. [00:07:24](https://youtube.com/watch?v=mej049HSFg4&t=444s) **解决方案：大规模多样化数据集**：

   1. 通过在旧金山多样化的真实家庭、模拟厨房和卧室中收集数据，构建了覆盖超过100个独特房间的数据集。
   2. 最终用于预训练的数据混合体包含：

      1. 移动操作数据（仅占2.4%）
      2. 静态机械臂数据
      3. 实验室环境数据
      4. 网络数据和高层指令数据
   3. 关键发现：仅需少量（2.4%）针对新任务（移动操作）的数据，结合大量已有的通用数据，就能有效启动新应用，这体现了通用模型的力量。
6. [00:09:01](https://youtube.com/watch?v=mej049HSFg4&t=541s) **架构改进与语言遵循能力提升**：

   1. 发现PI Zero模型有时会忽略语言指令。
   2. 通过修改模型架构，在预训练阶段预测离散化动作并对扩散头（Diffusion Head）应用停止梯度（Stop Gradient）操作，更好地保留了预训练视觉语言模型中的语言先验知识。
   3. 这一改进使得训练速度提升了4-5倍，并将语言指令遵循率从20%大幅提升至80%。
7. [00:11:02](https://youtube.com/watch?v=mej049HSFg4&t=662s) **环境泛化成果展示**：

   1. 结合多样化数据和改进的模型配方，机器人能够在从未见过的家庭环境中成功执行指令，如关闭橱柜、将碗碟放入水槽、清理泼洒物等。
   2. 定量评估表明：

      1. 包含网络数据和其他机器人数据对于在未知环境中取得良好性能至关重要。
      2. 随着训练数据中环境数量的增加（达到约100个），机器人的性能显著提升，并能接近在目标环境中收集数据所能达到的性能水平，基本弥合了泛化差距。
   3. 同时展示了模型的一些失败案例，如任务完成度判断错误、难以抓取薄物体、物体识别混淆（如将烤箱误认为抽屉）等，指出了在速度、长周期规划和部分可观测性方面仍存在挑战。
8. [00:14:36](https://youtube.com/watch?v=mej049HSFg4&t=876s) **处理开放式提示与中途指令**：

   1. 为了突破有限指令集的限制，探索了分层模型架构：高层视觉语言模型负责解析开放式提示并转化为具体指令，低层视觉语言动作模型负责执行原子指令。
   2. 挑战在于难以规模化收集多样化的人机交互数据。
   3. 解决方案：利用大语言模型根据机器人数据合成生成大量不同的提示和场景，作为高层模型的训练数据。
   4. 成果展示：机器人能够处理复杂的开放式提示（如“给我做一个不含泡菜的纯素三明治”），并能响应中途插入的指令（如“多加些泡菜”），实现了对机器人行为更高程度的可控性。
   5. 与现有通用基础模型（如GPT-4o, Gemini 2.5）相比，专门为机器人开发的模型在视觉理解和情境推理方面表现更优。
9. [00:19:00](https://youtube.com/watch?v=mej049HSFg4&t=1140s) **模型的开源与应用扩展**：

   1. PI Zero和后续模型是开源的，已被社区广泛用于微调，应用范围远超预期，包括手术机器人和四轴飞行器等，进一步证明了模型的通用性。
10. [00:19:27](https://youtube.com/watch?v=mej049HSFg4&t=1167s) **物理AI领域的开放性问题**：

      1. **记忆问题**：当前最先进的机器人模型通常不具备记忆能力，导致在需要记忆的任务（如擦拭架子后放回物品）中会重复操作。
      2. **执行速度**：机器人完成任务的速度远低于人类，视频通常需要加速播放。需要通过更直观的接口或强化学习微调来提高速度。
      3. **可靠性**：需要将任务成功率从80%左右提升到更高水平，以满足实际部署要求。研究表明强化学习算法有助于实现极高的可靠性，例如在精密任务（如GPU插入）中。
      4. 总之，物理AI领域仍有许多严峻挑战有待解决。

---

---

## Ray Summit 2025 Keynote: The Shift to LLM Fine-Tuning with Thinking Machines

[https://www.youtube.com/watch?v=Xb34YmbEiOc](https://www.youtube.com/watch?v=Xb34YmbEiOc)

结论：本视频由Thinking Machines Lab的Dendra介绍其团队推出的首个产品——Tinker。这是一个旨在简化大型语言模型（LLM）微调过程的训练API。Dendra阐述了当前LLM微调面临的两大痛点，并展示了Tinker如何通过一个简单易用的Python接口，在保持灵活性和交互性的同时，抽象掉分布式训练的复杂性。Tinker定位为当前LLM微调两种极端方案（完全自建基础设施 vs. 黑盒API）之间的“最佳平衡点”。它让研究人员和开发者能够像在实验室里一样，通过编写熟悉的Python训练循环（使用`forward`、`backward`、`optimizer step`等原子操作）来灵活定义自己的算法、数据和环境，而无需关心底层的GPU管理、分布式并行策略或基础设施可靠性。这极大地提升了研究迭代速度，并显著降低了使用成本。视频通过现场演示证实了其易用性和高效性，用户可在数秒内启动对大型模型（如670B参数）的训练。

关键要点：

1. [00:00:37](https://youtu.be/Xb34YmbEiOc?t=37s) **当前LLM微调的困境与Tinker的定位**：

   - **问题分析**：视频指出当前微调LLM存在两个极端选择。

     - 一端是“自带GPU”方案：用户需要自行租赁GPU、配置CUDA、PyTorch等深度学习库及其版本兼容性，并解决大规模训练时的并行策略（数据并行、模型并行等）、GPU内存管理、梯度检查点等复杂问题，整个过程耗时且结果可能并非最优。
     - 另一端是“黑盒API”方案：用户上传数据集并点击按钮即可训练，但缺乏交互性（无法实时查看损失、梯度），难以调试，灵活性差（无法自定义损失函数、RL环境，部分服务不支持下载模型权重）。
   - **Tinker的解决方案**：Tinker旨在找到中间地带，既保留了在自有GPU上训练的交互性和灵活性，又通过API抽象掉了分布式训练的复杂性。
2. [00:03:39](https://youtu.be/Xb34YmbEiOc?t=219s) **Tinker是什么及其核心设计理念**：

   - **产品定义**：Tinker是一个训练API。
   - **核心接口**：提供`forward`、`backward`、`optimizer step`、`sample`、`save`、`load`等原子函数，其设计灵感来源于模型训练实验室中研究人员常用的接口。
   - **使用模式**：用户专注于训练逻辑，通过编写简单的Python脚本来指定自己的训练数据、环境、算法、损失函数和超参数。这个脚本在用户本地的CPU上运行，无需配置GPU。当调用Tinker的API时，所有复杂的分布式训练、模型支持、可靠性保障均由Tinker后端自动处理。
3. [00:04:53](https://youtu.be/Xb34YmbEiOc?t=293s) **Tinker的核心能力与优势**：

   - **支持的模型**：目前支持约20个模型，包括Llama系列、Qwen系列、DeepSeek等。
   - **大规模训练抽象**：用户可以轻松训练参数量巨大的模型（如DeepSeek 671B），而无需关心需要多少GPU或使用何种并行策略。
   - **可靠性保障**：透明处理硬件故障等问题，用户无需担忧。
   - **极简安装与部署**：仅需安装一个Python包，无需设置NVIDIA驱动、CUDA、PyTorch或其他采样库。
4. [00:05:42](https://youtu.be/Xb34YmbEiOc?t=342s) **Tinker的基本工作流程与现场演示**：

   - **基本流程**：

     - 导入Tinker包，创建训练客户端。
     - 准备数据集（无需特殊格式，可使用简单Python代码处理）。
     - 在训练循环中调用`forward`、`backward`、`optimizer step`等API，实时观察损失下降。
     - 训练后，快速创建采样客户端，输入提示词即可获得模型响应。
   - **演示亮点**：现场演示了在笔记本电脑上，使用仅7个数据点的Pig Latin（一种文字游戏）示例，在6步训练内成功让Qwen 30B模型学会了该任务，展示了极快的任务启动（数秒内）和采样客户端创建速度。
5. [00:10:24](https://youtu.be/Xb34YmbEiOc?t=624s) **使用Tinker的主要价值主张**：

   - **提升研究速度**：

     - 极快的任务启动时间（秒级）。
     - 通过简单更改基础模型字符串，即可在1B到671B参数的模型间无缝切换。
     - 不仅支持监督微调（SFT），也能轻松进行强化学习（RL），用户可以自定义环境和奖励函数。
   - **降低成本**：

     - 按使用量（训练token和采样token）付费。
     - Tinker通过作业间分时复用和多租户技术，实现了资源的高效利用，从而能够提供有竞争力的价格（例如，DeepSeek训练成本为每百万token 3.38美元）。
   - **内置的容错与恢复**：自动处理故障，从最后一个检查点恢复，重放数据，对用户透明。
   - **模型权重可下载**：用户可在任何检查点下载训练好的模型权重，用于本地推理或部署。
6. [00:14:24](https://youtu.be/Xb34YmbEiOc?t=864s) **构建Tinker的技术挑战与架构**：

   - **三大专业领域**：构建此类产品需要融合核心基础设施（分布式系统、路由调度）、机器学习基础设施（多节点训练与采样、并行策略）和机器学习科学（提供文档、教程、配方）。
   - **后端技术**：使用Ray来动态管理训练器和采样器GPU集群，处理不同模型的GPU资源池。
   - **优化目标**：注重易用性和性能，管理多模型副本、用户队列路由调度、实现多租户以提升效率、确保可靠性及各任务间的隔离。
7. [00:16:09](https://youtu.be/Xb34YmbEiOc?t=969s) **生态系统、资源与未来规划**：

   - **开源Cookbook**：提供完整的Tinker Cookbook，包含从基础SFT/RL到数学推理、偏好学习、工具使用、多智能体RL等复杂场景的现实示例。
   - **社区集成**：已与Prime Intellect Environments Hub、Gem库等环境集成。出现了开源项目（如Skyold）重新实现Tinker API，允许用户自建后端。
   - **研究与教学资助**：为学术用途提供资助计划。
   - **未来路线图**：产品刚发布一个月，计划增加全参数微调、图像输入支持（用于训练VLM）等功能。
   - **获取方式**：可通过提供的网站链接注册使用。

---

## **Scaling LinkedIn's Online Training Solution with Ray | Ray Summit 2025**

[https://www.youtube.com/watch?v=dMvZu4W0SG4](https://www.youtube.com/watch?v=dMvZu4W0SG4)

视频介绍：本视频由LinkedIn AI训练平台的工程师团队分享，详细介绍了他们如何利用Ray框架来扩展在线训练解决方案。深入探讨了在线训练系统的架构设计、技术挑战和实际应用效果。

结论：LinkedIn通过构建基于Ray的在线训练平台，成功解决了传统批量训练的局限性，实现了模型更新频率的大幅提升和业务指标的显著改善。该系统通过流式数据生成、高效数据注入、统一训练环境管理和完善的监控体系，为大规模AI应用提供了稳定可靠的在线训练能力。未来的发展方向包括更细粒度的参数更新、序列模型支持和自动化优化，展现了在线训练技术在工业级AI系统中的广阔应用前景。

关键点：

1. [00:00:19](https://youtu.be/dMvZu4W0SG4?t=19s) 在线训练的核心概念与价值

    - 与传统批处理离线训练不同，在线训练系统建立了用户交互数据与模型之间的持续反馈循环
    - 每个点击、新职位发布和用户互动都成为近线训练过程的一部分
    - 业务价值：模型更新频率从季度/天级别缩短至15分钟，职位推荐模型的应用率提升超过2%
    - 计算成本优化：避免重复训练历史数据，基于检查点扩大训练时间窗口
2. [00:01:47](https://youtu.be/dMvZu4W0SG4?t=107s) 系统架构设计

    - 基于内部Kubernetes计算环境构建，每个训练作业作为Ray集群启动
    - 应用层解耦数据工作器（处理专家管理的离线数据或Kafka流数据）和训练工作器（基于TensorFlow或PyTorch）
    - 使用HDFS存储模型检查点和训练状态
    - 训练完成后导出模型并进行后处理发布
3. [00:02:43](https://youtu.be/dMvZu4W0SG4?t=163s) 面临的主要技术挑战

    - 训练与推理特征一致性：确保训练和推理时使用的特征完全一致
    - Kafka客户端性能：基于REST代理的自有客户端性能不如原生Java实现
    - 脆弱的运行时环境：依赖管理困难，问题排查复杂
    - 容错性要求：需要支持快照和故障状态恢复
    - 监控告警：需要及时捕获异常并通知用户
4. [00:04:23](https://youtu.be/dMvZu4W0SG4?t=263s) 流式训练数据生成方案

    - 构建完整的流处理系统，从Kafka主题输入到多级流处理器
    - 特征归因服务：记录在线模型推理时使用的确切特征值
    - 一小时窗口连接：平衡正标签获取与存储开销
    - 优化策略：将特征向量卸载到外部缓存，减轻流处理器内存压力
    - 数据转换服务：将嵌套格式特征扁平化为适合训练的宽列格式
5. [00:09:30](https://youtu.be/dMvZu4W0SG4?t=570s) 数据注入服务的优化

    - 跨语言障碍：在Java栈的Kafka基础设施与Python模型训练之间建立桥梁
    - Ray核心优势：利用Python原生开发体验构建可扩展的数据注入服务
    - 性能优化：

      - 每个Ray工作器处理一个请求，实现线性扩展
      - 利用零拷贝对象存储减少数据传输成本
      - 实现预取机制，IO阶段与数据处理阶段重叠
    - 效果：吞吐量提升5倍（零拷贝优化）+ 2倍（预取优化）
6. [00:13:00](https://youtu.be/dMvZu4W0SG4?t=780s) 训练环境管理创新

    - 用户通过导出API编译和序列化训练计算图（前向传播、损失计算、反向传播、优化器更新）
    - 使用安全模型和torch export确保静态图的可重现性和安全性
    - 统一管理训练状态快照：模型参数、优化器状态、数据分区（离线）、Kafka偏移量（在线）
    - 支持分布式训练（Horovod for TensorFlow和torch distribute）和并行评估服务
7. [00:15:22](https://youtu.be/dMvZu4W0SG4?t=922s) 监控告警体系

    - 输入流量监控：训练数据QPS和偏移量延迟
    - 标签分布监控：确保采样率正确
    - 实时渐进式评估指标：在线训练的实时性能监控
8. [00:15:50](https://youtu.be/dMvZu4W0SG4?t=950s) 未来发展方向

    - 流式部分参数更新：直接将优化器参数更新发送到服务容器，模型更新延迟降低15倍
    - 序列模型训练：处理用户历史行为序列，避免重复训练过去序列
    - 自动硬件规划：基于用户提供的批大小自动决定最佳硬件需求
    - 图优化：进一步编译、内核融合或替换
    - 扩展系统支持：模型蒸馏和强化学习等后训练步骤
9. [00:18:28](https://youtu.be/dMvZu4W0SG4?t=1108s) 模型验证与发布流程

    - 在线A/B测试系统：对比新模型与基线模型性能
    - 验证频率：根据垂直领域需求，从每15分钟到每天不等
    - 验证周期：通常需要2周时间进行统计显著性验证
    - 版本管理：每个新版本覆盖前一个版本，仅保留最新版本用于生产测试

## **How Zoox Built a Reliable, High-Velocity Model Serving Platform with Ray Serve | Ray Summit 2025**

[https://www.youtube.com/watch?v=HcnMgJ_zLIg](https://www.youtube.com/watch?v=HcnMgJ_zLIg)

视频介绍：本次演讲聚焦于自动驾驶公司Zuks在构建其内部模型服务平台Zserve时，如何利用Ray Serve解决可靠性与迭代速度方面的挑战。该平台最初支持数十个模型，现已扩展至生产环境中运行超过100个模型，服务于包括传统机器学习模型、大语言模型和多模态模型在内的多种用例。

结论：通过构建一个以“一键部署”为核心的模型部署服务，并辅以计算资源隔离、多层次环境以及全面的可观测性，Zuks成功地将模型部署时间从数天缩短至分钟级别，同时显著提升了平台的可靠性与用户的信任度。其核心成就包括实现了快速的开发迭代、高效的批量推理操作以及自动化的可靠性检查。

关键点：

1. [00:01:49](https://youtu.be/HcnMgJ_zLIg&t=109s) ​**Zserve平台的目标与挑战**：演讲者阐述了构建模型服务平台的两个北极星目标：速度与可靠性。

    - **速度**：机器学习工作流中90%是实验，快速迭代能使用户创新更快，保持团队领先。
    - **可靠性**：作为平台的核心支柱，可靠性能够建立用户信任，确保用户持续使用平台。生产环境中的模型故障可能导致高昂的代价和错误的决策。
2. [00:07:11](https://youtu.be/HcnMgJ_zLIg&t=431s) ​**初始痛点分析**：详细描述了平台发展初期遇到的具体瓶颈。

    - **速度痛点**：最初采用Kubernetes Helm模板和Serve配置部署模型，新模型上线或旧模型下线都需要代码审查和手动部署，对于一个仅有3人的小团队来说，这严重拖慢了迭代速度。
    - **可靠性痛点**：平台支持的用例非常多样化，包括延迟敏感型小模型、高吞吐量大模型以及夜间突发流量模型。将这些不同特性的模型部署在同一个Ray集群上，难以设定明确的性能预期，一个用例的流量高峰可能会影响集群上的其他服务。
3. [00:10:55](https://youtu.be/HcnMgJ_zLIg&t=655s) ​**解决方案一：计算资源隔离**：为了解决可靠性问题并模拟生产行为，团队引入了基于用例的Ray集群隔离策略。

    - 根据用户用例的资源需求（如CPU/GPU比例），为其创建独立的Ray集群。这确保了不同用户和用例之间的工作负载互不影响。
    - 团队同时开发了工具来抽象化集群创建的复杂性，用户只需关注向特定模型发送请求，内部系统会自动将请求路由到正确的集群和端点。
4. [00:12:14](https://youtu.be/HcnMgJ_zLIg&t=734s) ​**解决方案二：模型部署服务**：为了提升部署速度，团队构建了一个核心的“一键部署”服务。

    - 该服务部署在每个Ray集群上，与模型注册表集成。用户只需在注册表中选择模型并指定目标环境（开发、预生产、生产），即可触发部署流程。
    - 部署流程自动化：服务会拉取模型，运行可靠性检查，并利用Ray Serve API动态地将模型作为服务启动在Ray集群上，无需再使用繁琐的Helm模板。
    - 该系统强制要求模型在晋升至生产环境前，必须在预生产环境通过测试，确保了生产部署的质量。
5. [00:15:00](https://youtu.be/HcnMgJ_zLIg&t=900s) ​**模型配置的核心作用**：模型配置是驱动部署服务的关键，它包含了模型部署所需的所有元数据。

    - 配置内容广泛，包括：可靠性检查选项、不同环境下的自动扩缩容配置、模型类型、所属用例、输入输出张量定义以及所需的CPU/GPU资源等。
    - 这套配置体系使得部署服务能够智能地、自动化地处理各种模型的部署需求。
6. [00:16:52](https://youtu.be/HcnMgJ_zLIg&t=1012s) ​**全面的可观测性**：平台提供了详尽的指标和状态监控，同时覆盖服务端和客户端。

    - 暴露给用户的关键指标包括：应用/模型状态（运行中、部署中、不健康、部署失败、等待资源）、可靠性检查状态、资源利用率、工作节点Pod数量等。
    - 特别强调了客户端指标的重要性，它能提供从用户角度看到的完整性能画像，避免仅依赖服务端指标可能产生的误导。
7. [00:18:19](https://youtu.be/HcnMgJ_zLIg&t=1099s) ​**多层次部署环境**：平台设计了开发、预生产和生产三层环境，以平衡速度与安全。

    - **开发环境**：资源有限，允许用户快速部署和测试模型，失败后可以快速迭代，摩擦极小。
    - **预生产环境**：用于运行规模测试和可靠性检查，是模型晋升至生产前的必经关卡。
    - **生产环境**：仅部署那些在预生产环境中经过验证的模型，确保高可靠性。
8. [00:18:50](https://youtu.be/HcnMgJ_zLIg&t=1130s) ​**对新技术的探索与实践**：团队积极整合Ray生态系统的新功能，并探索其他解决方案。

    - **Ray Serve LLM APIs**：已集成到部署服务中，使用户能够快速部署和试验开源或内部微调的LLM及多模态模型，尽管将其生产化并引入相应的可靠性检查仍是进行中的工作。
    - **Ray Data LLM**：用于内部评估任务，通过简化代码和利用其并行处理能力，将作业运行时间减少了60%。
    - **Triton推理服务器**：正在探索将其用于某些特定场景，例如在运行繁重模拟工作负载时，将模型服务器靠近客户端以减少网络延迟和成本。

## **Inside Uber: Scaling Model Training with Ray | Ray Summit 2025**

[https://www.youtube.com/watch?v=xhn6NVJZgp4](https://www.youtube.com/watch?v=xhn6NVJZgp4)

视频介绍: 本视频由Uber AI平台工程师Pong和Bharat共同分享，详细介绍了Uber如何利用Ray框架构建大规模机器学习训练平台，支持从核心ML模型到大型语言模型的训练，并分享了在模型规模不断扩大过程中的技术演进和优化策略。

结论: Uber通过构建基于Ray的分布式训练平台，成功应对了模型规模增长带来的挑战。该平台整合了硬件升级、框架优化和资源管理等多维度改进，实现了10倍训练吞吐量提升。关键经验表明，模型平台的优化需要硬件、基础设施和算法层的垂直整合。未来，Uber计划进一步探索容错训练、多云资源调度和自动化优化等方向，为工业级AI系统提供更加稳定高效的大规模训练能力。

关键点:

1. [00:01:22](https://youtu.be/xhn6NVJZgp4&t=82s) **Uber AI平台概述与使命**

    - Uber AI平台（Micron Zero）是Uber的集中式机器学习平台，支撑着公司众多核心业务的模型训练和服务，包括外卖配送、目的地搜索、ETA预估、动态定价、欺诈检测、个性化推荐等关键应用场景。
    - 平台的核心使命是帮助生产团队可靠、高效地扩展模型规模和数据量，确保模型训练的质量和效率。作为集中化平台，它需要满足不同业务团队的多样化需求，从延迟敏感的小模型到高吞吐量的大模型。
    - 技术栈采用分层架构：底层是数据层（HDFS和云存储CFS）和计算层（Kubernetes），中间是分布式训练层（Ray作为核心模块），上层支持PyTorch、TensorFlow等深度学习框架，以及DeepSpeed、Horovod等分布式训练加速器。
2. [00:02:26](https://youtu.be/xhn6NVJZgp4&t=146s) **核心ML模型规模演进历程**

    - 模型规模持续增长：近年来Uber的核心ML模型规模不断扩大，从最初的数据并行逐步演进到模型并行和DeepSpeed Zero优化，以突破单GPU内存限制。
    - 技术栈演进：从TensorFlow迁移到PyTorch，并引入Ray框架来统一管理不同集群和云提供商的计算资源。随着模型超过GPU内存限制，开始采用模型并行技术。
    - 训练模式创新：由于训练时间延长，平台支持增量训练模式，同时进行了大量优化来提高训练效率。这种演进反映了工业界对更大模型规模和更高训练效率的持续追求。
3. [00:03:54](https://youtu.be/xhn6NVJZgp4&t=234s) **DeepSpeed优化与内存管理**

    - DeepSpeed Zero冗余优化器原理：通过在数据并行基础上对模型参数、梯度和优化器状态进行分片，每个GPU只保存部分模型分片，显著减少跨GPU的内存冗余。
    - 内存突破：这种技术使得整个集群能够训练远超单GPU内存容量的大型模型，与PyTorch FSDP采用相似的底层实现原理。
    - 硬件协同优化：随着模型规模扩大，发现了对优化技术的迫切需求，推动了从计算层到框架层的全面优化。
4. [00:04:55](https://youtu.be/xhn6NVJZgp4&t=295s) **计算层优化策略**

    - 硬件升级：从基于RTX 5000的旧硬件升级到A100和H100 GPU，解锁了低精度训练等高级功能，显著提升整体效率。
    - 作业调度优化：采用主机内GPU打包策略，优先在同一主机内调度实例，利用NVLink的高带宽通信，避免跨主机的网络通信瓶颈。
    - 先进技术集成：今年引入了B200 GPU并启用RDMA，实现跨节点GPU直接通信，进一步优化了跨节点通信性能。
5. [00:06:31](https://youtu.be/xhn6NVJZgp4&t=391s) **框架层优化技术**

    - 嵌入层优化：使用多哈希嵌入技术，在不影响模型准确性的前提下减少嵌入查找时间。
    - 混合精度训练：将大部分计算转换为低精度张量，加速训练速度，同时保持数值稳定性。
    - Flash Attention优化：通过内核融合技术，将操作融合到单个内核中，避免在芯片SRAM和HBM之间的频繁数据拷贝，显著减少内存拷贝时间。
    - 优化效果：结合计算层和框架层的改进，在升级的硬件上实现了10倍吞吐量提升。
6. [00:09:14](https://youtu.be/xhn6NVJZgp4&t=554s) **平台规模与多云挑战**

    - 作业规模：平台每天运行约2000个训练流水线，其中60%涉及GPU工作，平均作业时长3小时，P90超过5小时，P99超过一天。
    - 多云架构：从两年前开始从本地数据中心向云迁移，需要全球路由策略来确保数据在本地和云供应商之间的同步。
    - 数据流水线：支持Spark和Ray进行数据处理，通过统一的文件系统在不同云供应商间分发数据，利用Ray的跨云兼容性简化技术栈。
7. [00:12:24](https://youtu.be/xhn6NVJZgp4&t=744s) **Ray作业启动架构**

    - 分层架构：顶层是用户客户端（CLI、Notebook、内部框架），中间层是基于Kubernetes的平台服务（API服务器和控制器管理器）。
    - CRD建模：将流水线处理相关工件建模为Kubernetes自定义资源定义（CRD），用户可以使用原生Kubernetes API与控制平面交互。
    - 资源联盟：支持本地数据中心和云端的混合部署，通过集群CRD管理底层集群，提供资源联盟能力，用户无需关心作业运行的具体集群。
8. [00:15:04](https://youtu.be/xhn6NVJZgp4&t=904s) **作业生命周期管理**

    - 调度机制：作业控制器将作业加入调度队列，调度器根据资源亲和性分配集群，控制器负责启动和监控作业生命周期。
    - 集群健康监控：集群控制器监控底层集群健康状况，为调度器提供近实时的资源快照。
    - 组织感知调度：支持Uber的组织层级结构，根据团队预算和资源所有权进行作业路由，实现组织级策略管理。
9. [00:17:51](https://youtu.be/xhn6NVJZgp4&t=1071s) **作业历史与调试支持**

    - 历史追踪：作业完成后不删除CRD，而是标记为不可变并转移到外部存储，支持审计、调试和状态追踪。
    - 终止协议：用户可主动终止长时间运行的作业，控制器负责将终止信号传播到运行中的作业并清理资源。
    - 错误监控：作业控制器监控生命周期，能够检测常见错误（如OOM、容器启动失败等），并将错误信息直接反馈给用户，加速迭代调试。
10. [00:22:54](https://youtu.be/xhn6NVJZgp4&t=1374s) **弹性资源管理**

     - 层次化资源池：资源按组织层级组织成资源池树状结构，每个池可由团队或子团队拥有。
     - 弹性资源共享：当某些团队未充分利用资源时，系统可将资源分配给超预算的团队，实现资源的高效利用。
     - 动态调度：基于需求和当前使用情况计算资源权益，通过自定义准入控制器实现弹性容量分配，该功能已开源并在迁移到Kubernetes时进行了移植。

## **How Daft Boosts Batch Inference Throughput with Dynamic Partitioning | Ray Summit 2025**

[https://www.youtube.com/watch?v=FwGEM3ZdZLI](https://www.youtube.com/watch?v=FwGEM3ZdZLI)

视频介绍：本视频由Daft数据引擎团队的Kevin分享，重点介绍了Daft如何通过前缀缓存和连续批处理技术优化批量推理性能，将处理时间减少50.7%。

结论：Daft通过动态前缀分桶和连续批处理技术的结合，在128 GPU集群上对20万条提示词进行批量推理时实现了50.7%的性能提升。这一创新方案有效解决了传统批处理方法中的GPU空闲时间问题，同时通过智能路由和本地分桶机制在保持高缓存命中率的同时实现了流式处理，避免了全量数据排序的开销。该功能已在Daft 0.6.9版本中作为公开测试版发布，为大规模型批量推理提供了高效的解决方案。

关键点：

1. [00:01:16](https://youtu.be/FwGEM3ZdZLI&t=76s) **批量推理性能突破**

    - Daft引入新的推理后端，通过动态前缀分桶和基于流的连续批处理技术，将批量推理时间减少一半
    - 在代表性工作负载测试中（128 GPU集群、80亿参数模型、20万条提示词、1.28亿总token），实现了50.7%的性能提升
    - 该功能已正式发布，用户可通过daft.functions中的prompt函数直接使用
2. [00:02:32](https://youtu.be/FwGEM3ZdZLI&t=152s) **Daft平台核心能力**

    - 三大核心功能：在数据上运行模型、处理多模态数据、通过Ray扩展AI数据工作负载至数千节点和PB级数据
    - 提供Python原生的DataFrame API，支持图像、张量、嵌入、文件等多模态数据类型
    - 内置强大的AI函数，包括prompt和embed_text等，支持从文档提取内容到向量数据库写入的完整流程
    - 主要应用场景：嵌入生成和摄取、结构化数据提取、内容去重
3. [00:05:18](https://youtu.be/FwGEM3ZdZLI&t=318s) **批量推理与在线推理对比**

    - 在线推理特点：实时请求（如ChatGPT对话、代码建议）、关注首token时间和单个请求的token吞吐量
    - 批量推理特点：离线预处理整个数据集（如计算嵌入、数据标注、生成训练数据）、关注token成本和聚合吞吐量
    - 批量推理的独特挑战与机遇：

      - 性能优化空间更大
      - 数据集可能无法完全装入GPU/CPU内存
      - 成本与GPU利用率密切相关
      - 可基于已知数据分布进行进一步优化
4. [00:08:20](https://youtu.be/FwGEM3ZdZLI&t=500s) **基础批处理方法及其局限性**

    - 朴素批处理：将数据集分割成适合内存的小批次，发送到LLM服务引擎副本
    - 简单可扩展：通过增加LLM节点数量来扩展
    - 存在的问题：

      - 批次间GPU空闲：预处理和后处理步骤（分词、数据传输、批处理）导致GPU闲置
      - 批次内序列完成时间不均：长序列形成拖尾效应，进一步降低GPU利用率
5. [00:10:09](https://youtu.be/FwGEM3ZdZLI&t=609s) **连续批处理技术**

    - vLLM服务引擎支持的连续批处理技术：在token级别而非序列级别进行批处理
    - 核心优势：允许在前一批次完成期间开始后续批次的推理，实现良好的流水线处理
    - Daft实现：构建流式同步操作器，维护输入批次缓冲区，确保vLLM随时有可用序列
    - 性能提升：相比朴素批处理获得11%的速度提升
6. [00:12:55](https://youtu.be/FwGEM3ZdZLI&t=775s) **前缀缓存优化**

    - 原理：模型提示通常包含重复内容（系统提示、通用指令），通过缓存公共前缀避免重复计算
    - vLLM自动前缀缓存：自动在GPU内存中缓存序列的计算值
    - 分布式批处理的挑战：

      - 缓存驱逐：GPU显存有限，相似序列间隔较远时缓存可能被驱逐
      - 缓存局部性：在分布式集群中，前缀缓存是单机本地的，相似前缀的请求可能分布在不同机器上
    - 全局排序解决方案：推理前全局排序数据，将相同前缀的提示分组，缓存命中率从26%提升到54%，速度提升35%
7. [00:16:01](https://youtu.be/FwGEM3ZdZLI&t=961s) **动态前缀分桶技术**

    - 全局排序的局限性：排序是阻塞操作，期间GPU闲置；大数据集可能无法完全装入内存
    - 动态前缀分桶组成：

      - 本地前缀分桶：在每个机器上维护按前缀分桶的输入缓冲区，按最大桶优先弹出
      - 前缀感知路由：本地执行器查询全局路由器，基于缓存局部性和负载均衡确定最佳服务引擎副本
    - 技术优势：在保持高缓存命中率（54.0%）的同时实现流式执行，无需全量数据物化
    - 性能表现：比全局排序技术快12.7%，总比朴素批处理快50.7%
8. [00:21:17](https://youtu.be/FwGEM3ZdZLI&t=1277s) **基准测试结果**

    - 测试配置：20万条提示词，每条512 token，1.02亿输入token，512个唯一前缀，使用Qwen-8B模型，生成128个输出token
    - 硬件环境：128 GPU集群（NVIDIA L4，24GB显存）
    - 方法对比：朴素批处理、连续批处理、排序+连续批处理、动态前缀分桶、Ray Data基准
    - 扩展性测试：从32到128 GPU实现接近线性扩展（97%效率从32到64 GPU，90%从64到128 GPU）
    - 前缀数量影响测试：前缀越少性能越好，但从64到4096唯一前缀（64倍增长）性能差异在10%以内
9. [00:25:30](https://youtu.be/FwGEM3ZdZLI&t=1530s) **技术可用性与未来展望**

    - 功能发布：动态前缀缓存和连续批处理功能已在Daft 0.6.9版本作为公开测试版发布
    - 使用方式：通过pip install daft安装，使用prompt AI函数即可体验
    - 未来优化方向：模型权重加载优化（当前固定30-60秒）、Ray扩展性提升、技术瓶颈突破
    - 多模态支持：近期已增加图像到prompt函数的能力，支持图像与文本结合生成结构化输出

## **Ray + vLLM Efficient Multi Node Orchestration for Sparse MoE Model Serving | Ray Summit 2025**

[https://www.youtube.com/watch?v=J3zJ1MZtLN8](https://www.youtube.com/watch?v=J3zJ1MZtLN8)

视频介绍：本视频由AnyScale公司的LM团队负责人Kurush与工程师Si共同呈现，深入探讨了如何使用Ray Serve和vLLM高效部署稀疏混合专家模型，重点分析了提升服务效率的三大关键技术：宽专家并行、预填充与解码分离架构以及智能请求路由。

结论：通过Ray Serve LLM框架，团队成功构建了可扩展的稀疏MOE模型服务方案，实现了在高吞吐量场景下的显著效率提升。宽专家并行通过数据并行策略大幅增加KV缓存容量，预填充与解码分离优化了计算资源利用率，而智能请求路由则有效利用了前缀缓存。实验证明该方案在EP16配置下性能与原生vLLM相当，展现了Ray在生产环境中部署复杂模型服务模式的强大能力。未来将继续优化弹性容错、LM感知自动扩展等特性，为大规模AI推理提供更完善的解决方案。

关键点：

1. [00:01:21](https://youtu.be/J3zJ1MZtLN8&t=81s) ​**稀疏MOE模型的成本效率优势**：

    - 与传统稠密模型对比：在稠密模型中，每个令牌的前向传播都需要激活所有参数，计算量随批次大小线性增长；而在MOE模型中，通过路由机制仅激活专家子集，计算增长与批次大小呈亚线性关系
    - 实现效率的关键：需要最大化批次大小才能充分利用稀疏性优势，这使得MOE模型在高吞吐量场景下表现尤为突出
    - 历史背景：MOE理论基础源于1990年代，近年来在Transformer架构中重新获得关注，特别是Mistral AI开源其混合专家模型后，DeepSeek V3等模型突破了原有关于大模型开发和服务成本的假设
2. [00:03:31](https://youtu.be/J3zJ1MZtLN8&t=211s) ​**服务系统效率评估框架**：

    - 交互性指标：包括每个输出令牌的时间、首令牌时间等，不同应用对SLA有不同要求
    - 效率指标：在满足SLA前提下最大化效率，如每美元令牌数或每GPU吞吐量
    - 系统性能曲线：典型系统在增加批次大小时会牺牲部分SLA以换取吞吐量，但存在饱和点；理想系统应推动曲线向右上方移动
3. [00:05:14](https://youtu.be/J3zJ1MZtLN8&t=314s) ​**批次大小限制与KV缓存优化**：

    - 主要瓶颈：KV缓存大小是限制批次大小的关键因素，GPU内存被模型权重、CUDA图和激活占用，剩余空间决定KV缓存容量
    - 并行化策略：通过模型并行（如张量并行）减少单个GPU上的模型分片内存占用，从而为KV缓存释放更多空间
    - 核心问题：增加并行化是否能带来整体吞吐量的实际提升，需要权衡资源投入与性能收益
4. [00:06:43](https://youtu.be/J3zJ1MZtLN8&t=403s) ​**多潜在注意力机制的关键突破**：

    - 与传统多头注意力对比：经典Transformer中需要存储独立的KV单元，而MLA在训练期间学习跨所有头的压缩表示
    - KV缓存效率：推理时只需存储跨头的潜在表示，大幅减少存储需求。DeepSeek V3每令牌仅需70KB，而Llama 405V等模型需要近7倍存储空间
    - 与张量并行的兼容性问题：MLA的潜在表示无法在头间分割，在张量并行中需要复制，这违背了增加批次大小和聚合GPU内存的初衷
5. [00:09:54](https://youtu.be/J3zJ1MZtLN8&t=594s) ​**数据并行注意力的性能优势**：

    - 实验对比：在TP8、DP8和DP16配置下分析DeepSeek V3模型的服务性能
    - 内存分配分析：TP8需要35GB每rank，DP8为10GB，DP16为44GB，但聚合后的KV缓存容量DP策略显著更优
    - 并发请求能力：在5000输入令牌场景下，TP8仅支持92个并发请求，DP8提升2.5倍，DP16达到10倍提升，展现了DP与MLA结合的巨大潜力
6. [00:11:46](https://youtu.be/J3zJ1MZtLN8&t=706s) ​**Ray Serve中的宽专家并行实现**：

    - 代码示例：展示了在Ray Serve LLM中部署DP的紧凑代码表示，通过LM配置、引擎配置和数据并行设置快速构建部署
    - 基础设施优势：将基础设施与应用程序分离，所有应用逻辑保持在Python中，无需处理YAML和CRD，提供内置可观测性和更快的迭代速度
    - 架构细节：包含请求路由、TP协调、KV缓存分片、注意力矩阵复制、专家分片以及阶段间的all-to-all通信
7. [00:15:01](https://youtu.be/J3zJ1MZtLN8&t=901s) ​**预填充与解码分离架构的必要性**：

    - 同步开销问题：在专家并行中，各rank需要同步执行，混合预填充和解码会导致夸张的同步开销
    - 内核级优化机会：专用内核分别针对预填充（高吞吐量）和解码（低延迟）进行优化，统一部署会损失性能
    - CUDA图性能提升：从纯torch编译图的800毫秒TTFT切换到分离架构的CUDA图后，性能提升至200毫秒，显示显著优势
8. [00:17:42](https://youtu.be/J3zJ1MZtLN8&t=1062s) ​**分离式服务架构实现**：

    - 架构设计：预填充引擎计算KV缓存后通过NCCL发送到解码引擎，各自针对不同阶段优化
    - 构建器模式：提供预填充配置和解码配置的独立设置，支持副本比例、引擎参数等灵活配置
    - 性能分析：在5000输入令牌、250输出令牌工作负载下，分离式服务在高并发场景提供更好的吞吐量和交互性，但在低并发时可能不如2xTP4副本
9. [00:20:26](https://youtu.be/J3zJ1MZtLN8&t=1226s) ​**传输后端对性能的关键影响**：

    - 网络要求：KV缓存传输必须快速才能有效工作，节点内通过NVLink、节点间通过EFA、以太网通过TCP的性能对比显示显著差异
    - 硬件依赖：部署分离式服务需要硬件能够支持高速数据传输，否则吞吐量会受到严重影响
10. [00:21:11](https://youtu.be/J3zJ1MZtLN8&t=1271s) ​**智能请求路由与前缀缓存利用**：

     - 前缀树机制：记录请求内容，后续请求通过前缀树匹配近似KV缓存内容，充分利用vLLM中的前缀缓存
     - 实现方式：通过LM配置中的内置请求路由器，支持前缀缓存亲和路由，也可自定义路由策略
     - 性能收益：相比随机路由，前缀缓存感知路由在扩展副本时保持高命中率，带来显著的吞吐量提升
11. [00:24:27](https://youtu.be/J3zJ1MZtLN8&t=1467s) ​**Ray Serve生产环境性能验证**：

     - 性能对比：在EP16配置下，Ray Serve LLM与原生vLLM性能相当，达到约2100令牌/秒/GPU
     - 监控工具：Ray Dashboard提供LM特定指标，便于部署和故障排查
     - 扩展目标：持续优化以支持DP32及更高规模的部署，确保性能竞争力
12. [00:27:00](https://youtu.be/J3zJ1MZtLN8&t=1620s) ​**技术选型与未来方向**：

     - 流水线并行局限性：相比DPP，流水线并行存在气泡问题，在混合预填充和解码场景下效果不佳
     - 量化兼容性：量化与宽专家并行正交，即使在FP4等量化模式下仍能通过减少每GPU专家数提升计算效率
     - 未来发展：弹性容错YDP、LM感知自动扩展、用户体验改进等方向将持续推进

## **Matrix: Reliable Framework for Data-Centric Experimentation at Scale | Ray Summit 2025** 

[https://www.youtube.com/watch?v=MXEG_r04bY8](https://www.youtube.com/watch?v=MXEG_r04bY8)

视频介绍：本视频由Meta FAIR实验室的Rama Ragwendra和工程师Dong共同分享，详细介绍了他们基于Ray框架构建的数据中心化实验平台Matrix。该系统旨在解决AI模型训练中的数据瓶颈问题，通过高效的数据生成、处理和实验管理，推动下一代AI模型的发展。

结论：Matrix作为Meta FAIR实验室构建的数据中心化实验框架，成功解决了大规模AI模型训练中的数据挑战。该系统通过集成Ray集群管理、语言模型推理优化、容器化服务和作业调度等核心功能，为研究人员提供了高效的数据生成、处理和实验验证能力。实际应用表明，Matrix已成功支持多智能体协作、自然推理等前沿研究，生成数百万条高质量数据。该平台的开源化将进一步推动AI社区在数据中心化研究方面的发展，为突破当前数据瓶颈提供了切实可行的技术方案。

关键点：

1. [00:01:42](https://youtu.be/MXEG_r04bY8&t=102s) ​**AI训练数据的演进与挑战**：演讲者通过数据趋势图展示了AI训练数据的重大转变。

    - 数据规模爆炸式增长：过去五年间，AI训练数据从百万级扩展到万亿级，数据规模呈指数级增长，这是推动AI能力突破的关键因素。
    - 人类生成数据面临瓶颈：当前已接近人类生成数据的利用极限，需要寻找新的数据来源来支持AI能力的持续扩展。
    - 合成数据的必要性：为了突破数据瓶颈，必须探索使用AI模型生成合成数据来训练其他AI模型的新范式。
2. [00:04:49](https://youtu.be/MXEG_r04bY8&t=289s) ​**未来数据的四大来源**：详细分析了突破数据瓶颈的潜在解决方案。

    - 公开网络数据：继续利用Common Crawl等公开数据集，但清理和利用效率需要进一步提升。
    - 付费数据资源：探索付费墙后的高质量数据资源，虽然仍是人类生成数据，但能提供新的数据维度。
    - 产品内部数据：对于Meta这样的公司，可以利用其30亿用户产生的产品数据，在合规前提下挖掘数据价值。
    - 合成数据生成：通过AI模型自动生成训练数据，这是突破数据瓶颈最有前景的方向，仅ChatGPT一年的交互数据就相当于整个Common Crawl的规模。
3. [00:06:01](https://youtu.be/MXEG_r04bY8&t=361s) ​**数据质量的重要性**：强调在追求数据数量的同时必须保证数据质量。

    - 质量与数量并重：需要确保输入模型的数据具有高质量，避免低质量数据损害模型性能。
    - 数据价值最大化：通过更好的数据选择和优化，实现"用更少数据获得更多价值"的目标，打破指数级扩展的依赖。
    - 质量定义标准化：需要根据具体应用场景建立明确的数据质量评估标准。
4. [00:06:52](https://youtu.be/MXEG_r04bY8&t=412s) ​**数据中心化实验框架的核心流程**：构建完整的数据实验流程体系。

    - 高质量数据发现：利用LLM、扩散模型、游戏引擎等多种方式生成高质量数据。
    - 数据增强处理：为原始数据添加缺失的信号，如为视频添加字幕、语言翻译等增强操作。
    - 数据精炼筛选：通过专业化筛选提升数据质量，选择与模型训练最相关的数据子集。
    - 灵活的训练集成：支持静态数据加载和实时数据生成，兼容各种训练框架。
    - 系统评估验证：运行消融实验、评估测试和基准测试，确保数据集能真正提升模型性能。
5. [00:09:44](https://youtu.be/MXEG_r04bY8&t=584s) ​**Matrix系统架构与核心服务**：详细介绍Matrix平台的技术架构。

    - 基于Slurm的研究基础设施：在Slurm集群上构建，研究人员可以独立启动多个Ray集群实例。
    - 多层次服务架构：

      - 底层资源管理：通过Slurm管理计算资源，支持检查点恢复和动态扩容。
      - 在线服务层：提供语言模型推理、容器运行、视频编码、外部API代理等服务。
      - 易用API层：为研究人员提供简单易用的接口，支持各种使用场景。
6. [00:11:24](https://youtu.be/MXEG_r04bY8&t=684s) ​**Matrix核心API功能**：展示平台提供的四大类API服务。

    - 集群管理API：通过单命令或简单Python API启动Ray集群，支持HTTP服务进行集群管理。
    - 语言模型推理API：支持一键部署大多数开源模型，提供优化后的推理服务。
    - 容器服务API：支持部署数千个容器，运行任意Docker镜像，适用于代码生成等场景。
    - 作业调度API：高层次的任务管理，支持检查点评估等复杂工作流的自动化管理。
7. [00:15:24](https://youtu.be/MXEG_r04bY8&t=924s) ​**推理服务优化技术**：重点介绍在语言模型推理方面的技术创新。

    - 架构优势：基于Slurm的熟悉环境，支持HTTP和gRPC协议，具备自动扩缩容能力。
    - 性能优化：

      - 负载均衡：通过本地缓存实现客户端直接与工作节点通信，避免头部节点瓶颈。
      - gRPC协议优化：减少HTTP协议开销，提升通信效率。
    - 监控调试：集成Grafana仪表板，实时监控系统性能指标。
8. [00:18:44](https://youtu.be/MXEG_r04bY8&t=1124s) ​**容器服务的创新实现**：解决有状态容器管理的技术挑战。

    - 状态保持机制：通过注册表和Ray Actor实现容器ID到实例的映射，确保请求路由的一致性。
    - 大规模部署能力：支持并发部署数千个容器，满足代码生成、工具使用等研究需求。
    - 应用场景广泛：适用于代码验证、性能测试、AI数据生成等多种研究场景。
9. [00:20:38](https://youtu.be/MXEG_r04bY8&t=1238s) ​**实际研究应用案例**：展示Matrix在具体研究项目中的成功应用。

    - 多智能体协作研究：生成450万条对话轨迹，研究语言模型的 persuasive、assertive 能力和错误纠正机制。
    - 自然推理数据集：生成280万条推理数据，直接基于网络文档通过提示工程创建，显著提升下游模型的推理能力。
    - 语义任务聚类：利用Llama 8B的最后一层嵌入，通过DBSCAN聚类实现文本多样化。
10. [00:21:57](https://youtu.be/MXEG_r04bY8&t=1317s) ​**未来发展方向与开源计划**：展望平台的技术演进路线。

     - 技术升级：持续升级Ray框架，支持多模态数据处理。
     - 研究扩展：开展多智能体数据合成，支持Three Bench和Tall Agents等新用例。
     - 开源贡献：平台已开源，邀请社区共同参与开发和改进。

## **Inside Netflix’s Mako: The Next-Gen ML Training Platform | Ray Summit 2025**

[https://www.youtube.com/watch?v=nPHYclgj81c](https://www.youtube.com/watch?v=nPHYclgj81c)

视频介绍：本视频由Netflix训练平台团队的工程经理Aan和高级软件工程师Matan共同分享，详细介绍了Netflix新一代机器学习训练平台Macco的设计理念、架构特点和实际应用。这是Netflix首次公开分享Macco平台的技术细节。

结论：Netflix通过构建基于临时Ray集群的Macco平台，成功解决了第一代训练平台Manta在规模化过程中遇到的可靠性、隔离性和扩展性问题。Macco采用容器化作业、定制化调度器和优化数据加载等关键技术，为Netflix内部的大规模模型训练提供了更可靠、更灵活和更具扩展性的基础设施。平台不仅支持传统的分布式训练，还扩展到了异步推理和LLM后训练等复杂场景，展现了Netflix在ML基础设施领域的深厚技术积累和前瞻性设计理念。

关键点：

1. [00:02:23](https://youtu.be/nPHYclgj81c&t=143s) ​**Manta V1平台的演进与挑战**：

    - 平台起源：2022年作为实验性项目启动，旨在简化分布式训练，构建了首个持久化Ray集群
    - 快速增长：从支持小规模研究团队快速扩展到成为Netflix个性化和推荐模型的核心基础设施
    - 架构特点：采用持久化Ray集群设计，通过调度器将作业分发到多个集群
    - 规模化瓶颈：

      - 模型规模爆炸式增长：从1亿参数模型扩展到1000亿参数模型成为常态
      - 集群数量4倍增长，GPU资源年增长3倍
      - 头节点过载问题：由于"嘈杂邻居"效应，头节点性能逐渐下降
      - 运营负担加重：团队被大量运营问题淹没
2. [00:04:39](https://youtu.be/nPHYclgj81c&t=279s) ​**Macco V2平台的设计目标**：

    - 更高可靠性：确保用户训练作业成功完成，最小化平台错误
    - 更少约束性：提供灵活的使用路径，同时允许用户基于自身知识和能力进行调整
    - 更强扩展性：为未来模型规模持续增长做好准备
    - 平台状态：几周前刚刚发布beta版本
3. [00:06:04](https://youtu.be/nPHYclgj81c&t=364s) ​**Macco核心架构设计**：

    - 作业提交流程：

      - 用户通过Macco客户端定义和提交作业
      - 指定资源需求（节点数、GPU类型）
      - 定义运行命令和运行时环境（Docker、requirements.txt）
    - 计算编排：基于Netflix的Titus计算编排器，为高端GPU类型设置私有GPU池
    - 集群管理：

      - 作业一：使用高端GPU（来自私有池）和CPU节点（来自共享池）
      - 作业二：支持非Ray框架（如PyTorch DDP），使用低端GPU（来自共享池）
      - 所有节点容器化，支持对等发现和调度
4. [00:07:59](https://youtu.be/nPHYclgj81c&t=479s) ​**Macco关键技术特性**：

    - 临时集群：Ray集群仅在作业期间存在，仅包含作业内节点，消除嘈杂邻居问题
    - 容器化作业：使用Docker和uv进行运行时管理，提供更好的隔离性
    - 组调度：通过对等发现、公平共享和协调提交实现多节点作业
    - 定制调度器：支持公平共享、超额订阅等标准特性，平衡多租户访问
5. [00:08:58](https://youtu.be/nPHYclgj81c&t=538s) ​**定制调度器设计**：

    - 功能特性：

      - 支持FIFO和基于优先级的调度
      - 多节点作业的组调度
      - 公平共享和资源借用机制
      - 为ML特定调度需求提供定制点
    - 用户界面：显示调度信息、作业状态，并集成ML特定实体（检查点、实验跟踪链接、性能分析）
6. [00:09:51](https://youtu.be/nPHYclgj81c&t=591s) ​**训练工具包与自动追踪**：

    - SDK功能：提供Macco客户端和标准工具，简化分布式训练任务
    - 自动追踪特性：

      - 自动检测TensorFlow和PyTorch代码收集性能追踪
      - 存储在追踪存储中，通过调度器UI可视化
      - 优势：研究人员无需手动检测模型，平台工程师可主动识别优化机会
7. [00:11:28](https://youtu.be/nPHYclgj81c&t=688s) ​**资源调度与碎片化挑战**：

    - 问题：作业规模差异大（从单GPU到多节点），导致集群碎片化
    - 解决方案：

      - 抢占小作业进行碎片整理：可能对工作流造成干扰
      - 分割GPU池为小作业池和大作业池：降低集群利用率
      - 混合方案：大部分集群保留给大作业，小部分同时服务大小作业
8. [00:13:01](https://youtu.be/nPHYclgj81c&t=781s) ​**数据加载优化策略**：

    - FSX for Luster高性能网络文件系统：

      - 性能接近本地SSD，在初始作业中实现24倍加速
      - 作为S3缓存，支持自动数据预加载
      - 作业完成后通过缓存驱逐守护进程释放空间
    - 快速写入：提供本地SSD和S3 fuse加速上传
9. [00:14:15](https://youtu.be/nPHYclgj81c&t=855s) ​**网络性能优化**：

    - 问题：节点间带宽低，影响多节点训练性能
    - AWS EFA解决方案：

      - 提供RDMA功能，类似InfiniBand行为
      - 内部基准测试显示节点间带宽提升80倍
      - 接近节点内带宽的2-3倍
      - 使模型并行和张量并行更加可行
10. [00:15:40](https://youtu.be/nPHYclgj81c&t=940s) ​**平台扩展应用场景**：

     - 异步推理：

       - 针对5-10分钟推理任务，无需常驻服务
       - 优化启动时间：缓存Docker镜像和模型权重
       - 将开销从7分钟降低到2分钟
     - LLM后训练框架：

       - 支持监督微调、离线和在线强化学习
       - 得益于Macco的少约束性API，支持复杂分布式通信模式
       - 无共享Ray集群减少包版本约束
11. [00:17:56](https://youtu.be/nPHYclgj81c&t=1076s) ​**未来规划与经验总结**：

     - 发展路线：

       - 持续采用最新开源工具，基于业务需求进行增量开发
       - 计划明年Q1实现GA版本
       - 继续扩展大规模作业和集群支持
       - 简化高级工作流，降低分布式训练使用门槛
     - 经验教训：

       - 与开源生态对齐：利用成熟的OSS基础设施
       - 构建通用性：不过度优化特定方法，让最终用户驱动体验
       - 设计适应性：组件可替换，适应快速变化的ML领域

## **NVIDIA NeMo Curator: Scaling Multi-Modal Data Curation Workflows | Ray Summit 2025** 

[https://www.youtube.com/watch?v=FarU2GM4yRQ](https://www.youtube.com/watch?v=FarU2GM4yRQ)

视频介绍: 本视频由NVIDIA工程师团队分享，详细介绍了他们如何利用Ray框架构建可扩展的多模态数据处理管道Nemo Curator，解决了大规模AI训练中的数据准备挑战。

结论: NVIDIA通过开发基于Ray的Nemo Curator数据预处理管道，成功解决了多模态AI训练中的数据规模化处理难题。该系统通过流式处理、灵活资源分配、自动负载均衡等核心技术，实现了数据处理速度10-30倍的提升，同时显著降低了计算成本。特别是在去重、语义分析等复杂场景下，通过GPU加速和混合执行器策略，能够处理从文本到视频的百PB级数据。该开源项目为社区提供了完整的解决方案，展现了Ray在工业级AI数据管道中的强大能力。

关键点:

1. [00:00:19](https://youtu.be/FarU2GM4yRQ?t=19s) **多模态数据处理的挑战与机遇**

    - 数据重复问题严重：在训练数据中，30-40%的数据存在重复，90-95%的数据价值有限
    - 规模化处理的重要性：提高模型下游任务准确率，节省训练成本和时间，加速模型上市
    - 多模态数据规模差异：从文本的PB级到图像、视频、音频的数百PB级数据量增长
2. [00:03:50](https://youtu.be/FarU2GM4yRQ?t=230s) **Nemo Curator架构设计理念**

    - 模块化设计：以任务(task)为最小数据单元，通过可组合管道连接不同处理阶段
    - 多执行器支持：基于不同场景使用Cosmos Zena、Ray Data、Reactor Pool等执行器
    - 开源生态：所有代码、数据集、模型和研究论文完全开源，支持社区使用
3. [00:06:58](https://youtu.be/FarU2GM4yRQ?t=418s) **流式处理的核心优势**

    - 解决批处理瓶颈：避免阶段间磁盘IO，特别适合视频、图像等大规模数据
    - 计算重叠：在异构CPU/GPU管道中实现计算阶段重叠，提升整体吞吐量
    - 性能提升：在简单的分词+推理两阶段管道中，流式处理带来20%的性能提升
4. [00:08:56](https://youtu.be/FarU2GM4yRQ?t=536s) **灵活的资源分配机制**

    - 资源指定简化：通过ray.remote装饰器轻松指定CPU/GPU资源需求
    - 运行时资源覆盖：支持在实例化时动态覆盖默认资源设置
    - 实际收益：通过精确资源分配，在特定场景下获得5倍速度提升
5. [00:11:20](https://youtu.be/FarU2GM4yRQ?t=680s) **自动负载均衡技术**

    - 吞吐量监控：在应用层监控各阶段吞吐量，动态调整资源分配
    - 瓶颈优化：为慢速阶段分配更多资源，避免对象存储溢出和管道阻塞
    - 资源利用率：实现99.5%的GPU利用率，显著减少空闲时间
6. [00:13:29](https://youtu.be/FarU2GM4yRQ?t=809s) **视频处理管道实践**

    - 多阶段处理：下载→解码分割→多模型推理→过滤→vLLM标注
    - 线性处理特性：大多数视频片段可独立处理，适合并行化
    - 规模化能力：支持从文本到视频的多模态数据处理需求
7. [00:14:16](https://youtu.be/FarU2GM4yRQ?t=856s) **去重管道的特殊挑战与解决方案**

    - 非独立处理：需要全局数据集视图，与大多数可并行管道不同
    - 模糊去重：基于局部敏感哈希识别相似但不完全相同的文档
    - 语义去重：在嵌入空间中使用聚类技术识别语义相似的重复项
8. [00:16:03](https://youtu.be/FarU2GM4yRQ?t=963s) **GPU加速的去重实现**

    - 全GPU流水线：文档哈希、数据混洗、图计算全部在GPU完成
    - 高速传输：利用UCX、NVLINK、InfiniBand实现GPU间快速数据传输
    - 图算法应用：使用连通分量或并查集算法识别全局重复文档
9. [00:18:07](https://youtu.be/FarU2GM4yRQ?t=1087s) **规模化能力验证**

    - 互联网规模处理：支持整个Common Crawl数据集的处理
    - 大规模运行：在256个GPU上处理80-100TB的模糊去重任务
    - 单节点性能：单个节点可处理8TB数据，相当于两个Common Crawl快照
10. [00:20:39](https://youtu.be/FarU2GM4yRQ?t=1239s) **重复数据移除优化**

     - 连接操作挑战：在大规模数据上执行左反连接成本高昂
     - 文档ID优化：动态生成文档ID，通过文件映射避免大规模数据移动
     - Parquet过滤：利用列式存储特性实现高效的数据过滤和批量处理
11. [00:22:14](https://youtu.be/FarU2GM4yRQ?t=1334s) **未来发展方向**

     - 可恢复性：实现框架级别的自动检查点和状态恢复
     - 部署优化：提供更多部署示例和环境指南
     - 阶段融合：探索IO与GPU阶段的直接融合，减少数据传输
     - GPU对象存储：利用RDMA技术避免CPU-GPU间的数据来回传输
12. [00:23:39](https://youtu.be/FarU2GM4yRQ?t=1419s) **客户案例与成效**

     - 处理速度提升：帮助客户实现10-30倍的数据处理加速
     - 模型效果改善：采用优化管道后模型效果显著提升
     - 成本降低：通过管道优化显著降低计算成本

## **How Alibaba Cloud Accelerates AI Pipelines with AnalyticDB Ray | Ray Summit 2025** 

[https://www.youtube.com/watch?v=lnDCyiWFDL4](https://www.youtube.com/watch?v=lnDCyiWFDL4)

视频介绍：本视频由阿里巴巴云数据团队的产品经理Fay和工程负责人Leang共同分享，详细介绍了阿里云AnalyticDB如何集成Ray框架构建数据与AI融合平台，以解决传统数据仓库在处理多模态数据和AI工作流时面临的挑战，并展示了该平台在多个实际应用场景中的成功实践。

结论：阿里巴巴云通过将全托管Ray服务深度集成到AnalyticDB数据仓库中，成功构建了一个统一的数据+AI平台，有效解决了多模态ETL处理、异构资源调度、AI工作流集成等核心难题。该平台凭借自动弹性伸缩、流式计算优化、企业级稳定性增强和全面可观测性四大核心增强功能，在批处理推理、机器人仿真、广告点击率预测和游戏助手等多个场景中实现了显著的性能提升和资源利用率优化。这一架构不仅消除了数据存储、分析与AI计算之间的隔阂，更为企业客户提供了一站式的数据智能解决方案，展现了云计算与AI技术融合的广阔前景。

关键点：

1. [00:00:42](https://youtu.be/lnDCyiWFDL4&t=42s) ​**数据与AI融合的背景与挑战**：演讲者从数据世界的"封闭世界"和AI世界的"开放世界"对比入手，深入分析了传统数据系统与AI模型各自的局限性及融合必要性。

    - 封闭世界特征：传统数据仓库系统如AnalyticDB擅长处理企业内部结构化数据，支持精确查询和实时更新，但存在处理海量多模态数据能力不足、难以吸收外部知识的局限。
    - 开放世界优势：以大语言模型为代表的AI系统具备强大的知识压缩和概括能力，能够处理文本、图像、音频等低密度价值数据，但存在幻觉问题、只能进行模糊查询和周期性更新。
    - 融合价值：数据重力持续存在，客户需要在原地使用数据的同时，结合开放世界AI能力来创造更大价值。这种融合使得企业能够从主要处理结构化数据扩展到处理多模态数据的全新范式。
    - 转型挑战：从数据仓库范式转向多模态范式面临三大核心挑战——处理引擎不适配（如Spark处理多模态数据不够直观）、资源利用单一（传统仅用CPU，现需同时利用GPU和CPU）、AI工作流门槛高（需要专门系统且学习曲线陡峭）。
2. [00:05:23](https://youtu.be/lnDCyiWFDL4&t=323s) ​**AnalyticDB Ray解决方案的核心价值**：阿里云通过推出全托管Ray服务，构建了统一的数据+AI平台，旨在解决客户在数据智能转型中的核心痛点。

    - 平台定位：类似于从数据仓库到数据湖仓的演进，AnalyticDB在原有MPP和Spark服务基础上，新增全托管Ray服务，形成完整的AI管道构建和优化平台。
    - 核心目标：实现多模态ETL的无缝集成，将传统的SQL导向结构化ETL扩展到多模态ETL，并与AI工作流结合，保持数据重力的同时降低从数据工作负载到数据+AI的转型门槛。
    - 差异化优势：相比客户自管理Ray on Kubernetes或ECS，托管服务专注于数据团队需求，提供细粒度资源调度、稳定的集群运维和高可用性保障，特别针对长时运行作业进行了性能优化。
3. [00:08:31](https://youtu.be/lnDCyiWFDL4&t=511s) ​**四大企业级增强功能**：相比开源版本，阿里云对Ray内核进行了深度优化，提供了四项关键增强功能。

    - 自动弹性伸缩：实现服务器less行为，当大型ETL作业到来时自动扩展，显著提升GPU利用率，客户案例显示从不足5%提升到40%。
    - 流式计算节点：针对多模态ETL中的IO瓶颈问题，通过实现对象存储优化来消除瓶颈，提升管道延迟和吞吐量性能。
    - 企业级稳定性：通过一系列改进提升故障转移和正常运行时间，特别针对大型数据ETL作业场景进行了专门优化。
    - 全面可观测性：提供开源版本不具备的监控能力，方便客户进行集群管理和性能分析。
4. [00:11:25](https://youtu.be/lnDCyiWFDL4&t=685s) ​**批处理推理与数据蒸馏应用**：展示了在多模态大模型训练中的数据准备场景中的成功实践。

    - 客户背景：专注于视频和音频多模态大模型开发的客户，需要强大的文本能力作为基础。
    - 解决方案：使用AnalyticDB Ray作为中央数据仓库，从QN模型蒸馏生成文本样本，结合多模态训练数据准备最终模型训练数据。
    - 技术实现：利用缓存加速能力提升输入输出性能，通过弹性调度为推理和数据准备任务分配资源，显著提升GPU利用率，最终成功发布市场受欢迎的多模态语言模型。
5. [00:13:43](https://youtu.be/lnDCyiWFDL4&t=823s) ​**机器人公司的物理AI工作流**：与国内机器人公司合作，构建端到端的物理AI工作流和管道。

    - 行业现状：机器人公司在数据工程方面处于类似2015-2016年机器学习工程的状态，缺乏协作、未充分利用云资源、数据与实验不共享。
    - 解决方案：提供一键式ISAC gym和ISAC lab版本，结合人类捕获数据与仿真数据，通过ETL处理为强化学习和实际应用准备数据集。
    - 技术整合：在托管Ray版本上运行机器人技术栈，结合企业级增强功能，为客户提供完整的AI仿真用例支持。
6. [00:17:14](https://youtu.be/lnDCyiWFDL4&t=1034s) ​**技术架构深度解析**：详细阐述了支撑数据+AI平台的核心技术架构设计。

    - 统一资源池：管理所有CPU和GPU资源，同时服务SQL引擎、Spark传统数据处理工作流以及Spark Ray AI计算。
    - 存储加速：通过FPGA等先进技术实现存储加速集群，使数据访问更快速高效。
    - 内存池集成：通过ERDMA远程内存池和对象存储，实现所有组件间的内存级数据共享和处理，确保数据在系统间高效流动。
    - 四大支柱增强：在可用性方面提供一键部署和可观测性控制台；在稳定性方面实现无缝迁移、自愈和高可用；在成本性能方面通过虚拟集群支持多租户、自动伸缩和细粒度调度；在生态系统方面深度集成数据仓库服务并支持多种数据湖格式。
7. [00:21:27](https://youtu.be/lnDCyiWFDL4&t=1287s) ​**多模态数据处理流水线优化**：针对汽车和机器人公司的视频处理需求，构建了高效的多模态数据处理流水线。

    - 流水线复杂度：通常包含20-30个任务节点，处理从文件注入、解码、转码到嵌入的完整流程。
    - 自适应流式管道：通过消除CPU和GPU间的资源空闲时间，实现两者完全饱和运行，CPU处理完部分数据后GPU立即开始工作，同时CPU转向下一批数据处理。
    - 负载感知自动伸缩：确保有足够CPU资源向GPU持续输送工作负载，保持GPU完全利用。
    - 规模成效：客户案例显示每天处理400万片段，峰值时每秒处理8万片段，相比原有流水线无法应对高变体负载，新方案成功支持了高吞吐量处理。
8. [00:24:37](https://youtu.be/lnDCyiWFDL4&t=1477s) ​**成本效率与高级调度优化**：通过精细化调度和资源管理实现极致的成本效益。

    - 任务画像分析：通过性能剖析理解每个Ray任务的内存足迹，提高每个节点上的任务部署密度。
    - 实时迁移能力：支持Ray节点在不同机器间实时迁移，不影响客户实时处理。
    - 协同调度优化：在同一个节点上协同调度不同任务，包括GPU任务和CPU任务，最大化任务吞吐量。
    - 集群规模：AI工作负载集群规模已超过100万CPU，日弹性容量达50万核心，确保底层GPU计算有足够数据流支撑。
9. [00:26:40](https://youtu.be/lnDCyiWFDL4&t=1600s) ​**启动加速与缓存优化**：确保弹性伸缩能够真正发挥效用，避免因启动延迟影响整体性能。

    - 两级缓存架构：池缓存使节点启动时间小于1秒，Python环境缓存使Python任务启动时间达到5-10毫秒级别。
    - 即时响应能力：当自动伸缩器调用任务时，任务几乎可以瞬时准备就绪，消除了处理过程中的等待时间。
    - 技术价值：这种启动加速技术使得所有流式管道处理和自动伸缩成为可能，确保了系统的高效连续运行。
10. [00:27:50](https://youtu.be/lnDCyiWFDL4&t=1670s) ​**电商广告点击率预测优化**：展示了在电商场景中通过优化数据处理流水线显著提升GPU利用率的成功案例。

     - 业务挑战：离线批量推理需要处理数百万甚至数十亿数据点，进行ETL处理和文本理解，往往导致昂贵GPU资源大量闲置等待。
     - 性能瓶颈：迁移前GPU利用率仅5%，大部分时间花费在等待CPU完成数据处理和准备上。
     - 解决方案效果：通过负载感知自动伸缩确保GPU持续获得CPU输送的数据，GPU利用率提升至80%，结合Ray对象存储和ERDMA远程内存池避免繁重数据移动，整体性能提升2-3倍。
     - 业务价值：更快的管道运行速度使客户能够进行更多迭代，通过更精准的用户行为分析创造更大商业价值。
11. [00:30:09](https://youtu.be/lnDCyiWFDL4&t=1809s) ​**游戏助手场景的分布式优化**：为游戏公司构建基于多模态处理的智能游戏助手平台。

     - 业务需求：构建能够理解游戏角色、对象和环境上下文的模型，通过游戏截图提供可操作建议给玩家。
     - 技术挑战：需要持续进行模型微调以保持与游戏更新的相关性，原有单机方案不可扩展，分布式方案稳定性不足。
     - 解决方案：提供基于Ray的微调和服务统一平台，支持大规模数据微调保持模型时效性。
     - 成效评估：性能提升3-5倍，更重要的是通过分布式处理获得更快速度，通过托管服务获得更高稳定性，能够同时支持微调和在线服务流量。

## **Unlocking Peak Workload Performance &amp; Efficiency with Ray on Kubernetes | Ray Summit 2025** 

[https://www.youtube.com/watch?v=gHmHnrIJPPI](https://www.youtube.com/watch?v=gHmHnrIJPPI)

视频介绍：本视频由Google Cloud的Ray on GKE产品经理Nisha Johnson和工程师Ryan Oolirri共同呈现，深入探讨了如何将Ray与Kubernetes相结合，构建高效、可扩展的AI工作负载平台。演讲重点解决了在构建AI平台时面临的五大核心挑战：资源访问、性能优化、平台易用性、安全性和多租户管理，并展示了Google Cloud在GKE环境中提供的完整解决方案。

结论：通过动态工作负载调度器、自定义计算类别、Kubernetes原生队列系统Q、KubeRay操作符以及TPU深度集成等创新功能，Ray与Kubernetes的组合为企业AI平台建设提供了完整的解决方案。该平台不仅实现了成本效益最大化、资源利用率优化，还支持从训练、推理到智能体工作负载的全场景覆盖。Google与Anyscale的深度工程合作确保了该平台的持续演进，使其成为构建生产级AI系统的最佳基础架构选择。

关键点：

1. [00:01:00](https://youtu.be/gHmHnrIJPPI&t=60s) ​**构建AI平台的五大核心挑战**：演讲者系统性地阐述了在构建企业级AI平台时面临的关键难题。

    - 资源访问挑战：选择合适的加速器（GPU/TPU）并在需要时及时获取是首要问题。Google Cloud提供了丰富的TPU和GPU选项，从低成本推理的L4到前沿模型训练的V6E/V7E TPU。
    - 性能优化挑战：获得加速器后需要平衡新模型、算法优化和不同硬件类型的性能调优，同时确保大规模投资的使用效率最大化。
    - 平台易用性挑战：管理各种开源框架、硬件类型和编排工具的复杂性，避免平台管理成为全职工作。
    - 多租户管理挑战：多个团队共享资源时的资源分配、优先级和隔离问题。
    - 安全性挑战：适当的访问控制和安全保障措施。
2. [00:03:01](https://youtu.be/gHmHnrIJPPI&t=181s) ​**动态工作负载调度器解决方案**：Google Cloud提供的灵活计算资源访问机制。

    - Flex Start模式：适用于运行时间不超过7天的作业，采用按需付费模式，无需固定预留，适合快速实验、微调或批量推理等短期任务。
    - Calendar模式：提供短期预留，类似于酒店预订，最长可达90天，需要预付全款但提供启动时间保证，适合需要更强时间保证的长期运行作业。
    - 优势对比：Flex Start提供灵活性，Calendar模式提供确定性，两者都避免了传统的年度或多年度合约绑定。
3. [00:04:50](https://youtu.be/gHmHnrIJPPI&t=290s) ​**自定义计算类别与多租户资源管理**：解决团队间资源共享和优先级问题的核心机制。

    - 回退逻辑设计：允许开发者在YAML文件中定义资源获取的优先级策略，例如首先尝试H200预留，失败后回退到H100 DWS flex starts，最后尝试H100抢占式实例。
    - 开发者友好集成：支持直接从Ray代码中配置回退逻辑，避免YAML配置的复杂性，使Python开发者能够轻松使用。
    - 资源弹性保障：通过多级回退策略确保作业能够在资源紧张时依然找到可用的计算资源，提高作业成功率。
4. [00:06:31](https://youtu.be/gHmHnrIJPPI&t=391s) ​**Kubernetes原生队列系统Q**：实现精细化的资源调度和优先级管理。

    - 配额管理：为不同团队设置资源配额，实现公平的资源分配。
    - 智能调度：不仅支持简单的先进先出，还内置智能调度逻辑，允许资源需求较少的低优先级作业优先启动。
    - 抢占机制：支持抢占低优先级作业，确保高优先级任务能够及时获得所需资源。
    - 资源隔离：通过命名空间实现资源和安全隔离，每个团队使用自己的本地队列提交作业。
5. [00:07:34](https://youtu.be/gHmHnrIJPPI&t=454s) ​**KubeRay操作符与GKE集成**：简化Ray在Kubernetes上的部署和管理。

    - KubeRay功能：开源操作符，简化Ray应用在Kubernetes上的部署，管理Ray集群、Ray作业和Ray服务的创建、删除和协调。
    - GKE托管体验：通过Ray插件提供托管的KubeRay操作符，自动处理日志收集、可观测性集成到Google Cloud Monitoring，并安装有用的Web钩子。
    - TPU支持：专门配置TPU Pod调度和运行时环境，确保TPU工作负载能够成功初始化，简化开发者的配置工作。
6. [00:09:56](https://youtu.be/gHmHnrIJPPI&t=596s) ​**平台集成架构与实践示例**：展示如何将各个组件整合成统一调度体验。

    - 端到端工作流：平台管理员配置集群队列和本地队列，定义自定义计算类别，连接Q与动态工作负载调度器。
    - 开发者体验：开发者只需在代码中指定标签选择器或回退策略，系统自动处理复杂的资源调度和回退逻辑。
    - 代码示例：展示如何在Ray训练工作器中使用装饰器指定首选TPU类型和回退选项，以及在Ray作业自定义资源中配置队列优先级。
7. [00:13:10](https://youtu.be/gHmHnrIJPPI&t=790s) ​**TPU深度集成与性能优化**：专门针对TPU工作负载的优化功能。

    - Ray util TPU库：提供TPU调度工具，使用新的bundle标签选择器API确保工作负载原子性地调度在TPU切片上，避免工作节点碎片化。
    - Jax Trainer集成：为TPU工作负载优化的分布式训练组件，遵循SPMD范式，自动处理TPU多主机切片的原子性预留。
    - 服务部署简化：支持相同的Ray和vLLM技术栈在TPU上运行，只需调整部署配置以适应不同的设备形态，实现价格和性能的实时优化。
8. [00:16:09](https://youtu.be/gHmHnrIJPPI&t=969s) ​**启动延迟优化与可观测性**：提升平台整体性能的关键特性。

    - GKE镜像流式传输：结合辅助启动磁盘，大幅减少大型容器镜像的拉取时间，使16GB镜像的挂载速度提升高达29倍。
    - 自动扩展加速：更快的Pod启动时间意味着Ray自动扩展响应更快，Pod被抢占或驱逐后的重启也更迅速。
    - 全面监控集成：将TPU指标完全集成到Ray仪表板，支持自定义Grafana或Prometheus仪表板，在云控制台中提供Ray的GPU和TPU指标可视化。
9. [00:18:05](https://youtu.be/gHmHnrIJPPI&t=1085s) ​**智能体AI工作负载支持**：面向未来AI工作负载的平台能力。

    - 解耦智能体平台：Ray提供理想的智能体架构，使用有状态的Actor作为LLM大脑，无状态的Ray任务处理工具调用。
    - 安全隔离：与gVisor沙箱深度集成，提供内核级隔离，安全执行不受信任的LLM代码。
    - 容错与资源管理：结合Kubernetes的集群生命周期管理和Q的队列调度，确保智能体任务获得所需资源而不相互竞争。
    - 端到端优化：从原型到生产的完整平台支持，解决智能体AI的生产化挑战。

## **NVIDIA’s Framework for Scalable Data Curation | Ray Summit 2025** 

[https://www.youtube.com/watch?v=j0OLbJEL_yE](https://www.youtube.com/watch?v=j0OLbJEL_yE)

视频介绍: 本视频由NVIDIA的Jacob及其团队分享，详细介绍了Cosmos平台及其两大核心组件Cosmos Zenna和Cosmos Curate。Cosmos是NVIDIA用于物理AI的基础模型平台，专注于生成世界基础模型和加速数据处理流程，旨在推动自动驾驶、机器人等物理AI应用的发展。

结论: Cosmos平台通过构建专为大规模多模态数据管道优化的Zenna库，成功解决了在有限基础设施条件下运行PB级视频、图像数据处理管道的挑战。团队从Dask到Ray Data再到自主开发Zenna的演进过程中，积累了关于流式执行、管道平衡、资源管理和可观测性等方面的宝贵经验。开源的Cosmos Curate为开发者提供了构建视频处理管道的参考实现，展现了在复杂基础设施环境下实现高效数据处理的完整解决方案。这些经验对任何处理大规模多模态数据管道的团队都具有重要参考价值。

关键点:

1. [00:00:24](https://youtu.be/j0OLbJEL_yE&t=24s) ​**Cosmos平台概述与定位**：

    - Cosmos是NVIDIA专注于物理AI的基础设施平台，核心目标是加速自动驾驶、机器人等物理AI应用的开发进程。
    - 平台包含两大核心能力：生成世界基础模型和针对视频等物理AI数据的加速处理与标注管道。
    - 团队处理的数据规模达到PB级别，涵盖视频、图像和3D数据等多种模态，主要通过深度学习模型进行自动标注。
    - 管道拓扑结构相对简单，主要是级联的map操作，不涉及复杂的reduce或shuffle阶段，这种设计专门针对GPU推理流水线优化。
2. [00:00:54](https://youtu.be/j0OLbJEL_yE&t=54s) ​**Cosmos Zenna的核心特性与设计理念**：

    - Zenna是一个开源的数据管道库，采用库而非框架的设计理念，具有相对自包含和可移植的特点。
    - 管道采用多阶段设计，典型流程包括数据下载、CPU预处理、GPU推理和结果上传等阶段，复杂管道可能包含多达20个阶段。
    - 每个阶段可以配置独立的批次大小和资源需求，支持基于模型TP大小的参数化资源配置。
    - 独特支持SPMD（单程序多数据）模式，允许actor跨多个GPU和节点运行，这是针对PyTorch运行方式的专门优化。
    - 与Ray Data的关键区别包括：直接传递Python对象而非数据块、更明确的多节点多GPU支持、专门为级联推理管道优化而不处理shuffle等复杂操作。
3. [00:04:04](https://youtu.be/j0OLbJEL_yE&t=244s) ​**技术演进历程与架构决策**：

    - 团队经历了从Dask到Ray Data再到自主开发Zenna的技术演进过程，每次转变都是为了解决特定的性能瓶颈。
    - 最初使用Dask时遇到效率低下和诊断困难的问题，转向Ray Data后虽然有所改善，但仍面临内存背压和效率问题。
    - 基于对Ray Data的分支修改和持续的技术债务积累，团队决定构建完全自主的Zenna库，过去两年已通过该库运行了数百GPU年的计算任务。
    - 开源决策主要是为了支持其他开源产品的需求，虽然目前主要内部使用，但团队愿意基于社区兴趣增加外部支持。
4. [00:06:01](https://youtu.be/j0OLbJEL_yE&t=361s) ​**基础设施挑战与创新解决方案**：

    - 团队面临非理想的基础设施环境：数据存储（圣何塞）与计算集群（全球分布）分离，导致高延迟连接问题。
    - 集群本地存储有限且缺乏自动扩缩容支持，需要在资源受限条件下保证管道稳定运行。
    - 开发了P2P制品分发系统，类似Dragonfly的设计，通过节点间协作下载减少对对象存储的直接依赖，有效解决跨区域数据传输问题。
    - 数据湖架构目前采用PostgreSQL到Delta Lake的复杂同步方案，未来计划转向Glance等更优化的解决方案。
5. [00:07:04](https://youtu.be/j0OLbJEL_yE&t=424s) ​**流式执行的重要性与实现挑战**：

    - 流式执行对异构资源（特别是CPU和GPU）的高效利用至关重要，相比传统的批处理执行方式能显著提升资源利用率。
    - 流式执行避免了阶段间的大规模中间存储需求，这对存储资源有限的集群环境尤为重要。
    - 团队需要精心处理背压控制和队列管理，因为处理的是大型视频和图像样本，内存队列管理变得极其关键。
    - 管道平衡是流式执行的核心挑战，团队投入了大量代码（约3000行Python和Rust）来优化自动平衡算法。
6. [00:09:45](https://youtu.be/j0OLbJEL_yE&t=585s) ​**云存储作为中间层的优化思路**：

    - 提出使用云存储作为中间结果存储的创新方案，既能保持流式执行的优点，又能避免内存队列的限制。
    - 云存储方案提供了近乎无限的存储容量，从根本上解决了背压和平衡问题，但需要同区域对象存储的支持。
    - 这种方案会增加下载上传的复杂性和对象存储成本，需要在性能收益和成本复杂度之间进行权衡。
    - 团队认为这是未来优化的重要方向，特别是对于超大规模数据处理场景。
7. [00:12:46](https://youtu.be/j0OLbJEL_yE&t=766s) ​**可观测性系统的构建与实践**：

    - 针对复杂管道环境构建了专门的可观测性系统，将硬件指标与逻辑阶段进行关联。
    - 系统支持按阶段聚合CPU、内存等资源使用情况，即使这些资源分布在不同的进程和节点上。
    - 暴露管道内部状态指标，如每个阶段的队列状态和actor状态，帮助识别性能瓶颈。
    - 开发了自动化的仪表盘，在运行Cosmos时自动填充监控数据，大大简化了调试和优化过程。
8. [00:16:13](https://youtu.be/j0OLbJEL_yE&t=973s) ​**可靠性保障与实战经验积累**：

    - Zenna经过大规模多样化数据的实战检验，积累了处理各种极端情况的经验。
    - 通过客户合作处理了从30MB小视频到10GB大视频的混合数据流，逐步优化了内存管理和处理逻辑。
    - 提供了丰富的可调参数来应对新的使用场景，包括自动平衡参数、actor定期重启等容错机制。
    - 能够透明处理内存泄漏等难以调试的问题，通过定期重启actor保证系统长期稳定运行。
9. [00:18:14](https://youtu.be/j0OLbJEL_yE&t=1094s) ​**Cosmos Curate开源框架与应用案例**：

    - Cosmos Curate是基于Ray构建的数据管道框架，提供视频处理和自动驾驶数据处理的参考管道。
    - 包含CRI启动器，支持在Docker本地、Slurm集群或Kubernetes平台上部署和运行。
    - 视频标注管道包含三个核心阶段：语义分割短视频、基于运动和美学的过滤、VRM标注和嵌入生成。
    - 支持语义去重和数据集分片，最终生成可直接用于训练的WebDataset格式数据。
    - 除了开源版本，还提供基于DGX Cloud的托管服务，方便客户快速试用和部署。

## **Wisedocs’ Journey: Rebuilding &amp; Accelerating ML with KubeRay | Ray Summit 2025**

[https://www.youtube.com/watch?v=H2PYMnjdDQI](https://www.youtube.com/watch?v=H2PYMnjdDQI)

视频介绍：本视频分享了医疗文档处理领域一个团队在过去一年中使用Ray框架重建机器学习管道的实践经验。演讲者详细介绍了从传统模块化架构迁移到基于Ray的分布式服务架构的全过程，包括技术选型考量、架构设计、部署策略以及迁移过程中遇到的实际挑战和解决方案。

结论：通过采用Ray Serve重构ML管道，团队成功实现了代码量减少50%、处理时间显著提升、支持更大文件规模的目标。新架构通过统一的服务层、模块化设计和Kubernetes原生部署，解决了原有系统在扩展性、部署效率和资源利用方面的瓶颈。迁移过程中采用的分阶段影子部署、功能标志和客户分段策略确保了零停机迁移。未来团队将继续优化硬件配置、完善自动扩缩容规则，并在保持开发效率的前提下进一步提升系统性能。

关键点：

1. [00:00:31](https://youtu.be/H2PYMnjdDQI?t=31) ​**业务背景与挑战**：

    - 医疗文档处理业务涉及医生、保险公司、政府等客户上传的医疗索赔文档，文档规模从几百页到数万页不等
    - 处理流程包括文档分类、关键信息提取、摘要生成，并包含人工审核环节以确保医疗和法律数据的准确性
    - 主要挑战是需要支持100倍规模扩展，同时满足严格的SLA要求，支持超过1500种文档类型
    - 不同客户对同一文档可能有不同的处理需求和业务含义，增加了系统复杂性
2. [00:02:26](https://youtu.be/H2PYMnjdDQI?t=146) ​**原有架构的局限性**：

    - 采用自包含模块化架构，所有业务逻辑、模型和后处理都封装在独立模块中
    - 使用共享库在模块间传递状态，通过自定义事件平台进行协调
    - 随着规模扩大暴露出三个主要问题：

      - 无法按需更新模块
      - 缺乏中央编排器导致新增模块缓慢
      - 规模经济效应受限
    - 性能问题日益突出，协调变得笨重
3. [00:03:22](https://youtu.be/H2PYMnjdDQI?t=202) ​**基于Ray的新架构设计**：

    - 引入上层编排器构建DAG工作流，连接预处理、模型模块和后处理步骤
    - 使用Ray作为模型服务层，配合特征存储加载特定特征和文档
    - 各组件职责分离，支持独立更新周期
    - 模块通过调用Ray服务实现功能，使模块更小、更动态
    - 支持商业LLM和传统业务逻辑的混合使用
4. [00:04:19](https://youtu.be/H2PYMnjdDQI?t=259) ​**选择Ray Serve的技术考量**：

    - 与Kubernetes标准化战略一致，减少工具学习成本
    - 支持异构模型部署（CPU/GPU混合、不同架构）
    - 提供自动扩缩容能力、分式GPU利用
    - 支持批处理和在线服务的统一平台
    - 为长期平台发展提供技术基础
5. [00:05:19](https://youtu.be/H2PYMnjdDQI?t=319) ​**迁移成果与收益**：

    - 代码量减少50%，从15个代码库整合，提升开发效率
    - 处理时间显著降低，通过统一服务层优化性能
    - 支持更大文件规模，满足客户上传数十万页PDF的需求
    - 开发人员更愿意参与代码修改，加速迭代速度
    - 改进单元测试能力，支持准确性、延迟等多维度测试
6. [00:06:12](https://youtu.be/H2PYMnjdDQI?t=372) ​**代码库与部署架构**：

    - 采用单体代码库设计，便于模块间链接和问题追踪
    - 包含共享逻辑模块、编排器专用模块、Ray服务层
    - 不同服务采用独立部署模式，支持按需部署
    - 使用Argo进行Kubernetes原生部署，配合配置管理和PR流程
    - 实现细粒度的包重建检查和环境调度
7. [00:07:55](https://youtu.be/H2PYMnjdDQI?t=475) ​**Ray Serve高级功能实践**：

    - 多路复用功能：相似模型共享工作负载，避免重复部署

      - 自动路由请求到正确的worker或actor
      - 支持基础模型加载多个适配器
      - 跨集群负载均衡
    - 多部署和应用：支持状态在不同模型间传递

      - 预处理逻辑复用（如图像与文本处理）
      - 按生命周期分组部署，降低更新风险
    - 多集群部署：使用KubeRay操作符管理

      - 蓝绿部署支持，新版本部署在新集群
      - 在开发效率与资源利用率间取得平衡
8. [00:12:30](https://youtu.be/H2PYMnjdDQI?t=750) ​**迁移策略与测试挑战**：

    - 零停机部署要求，不同客户有不同的SLA容忍度
    - 使用功能标志分客户迁移
    - 单元测试难以捕获百万分之一几率的边缘情况
    - 多区域分阶段部署策略：

      - 从最低风险区域开始，影子模式运行
      - 逐步增加流量至生产级别
      - 持续一个月的错误修复，发现上百个边界问题
      - 按客户风险等级分段迁移
      - 保持v1备用直至完全验证
9. [00:15:54](https://youtu.be/H2PYMnjdDQI?t=954) ​**实际技术问题与解决方案**：

    - 页面流分割（PSS）问题的二进制分类bug
    - 单元测试与生产环境结果不一致，排查数周
    - 发现Ray中模型分配需要深度复制，与PyTorch不同
    - 多路复用与模型加载的交互问题
    - 经验教训：需要深入理解框架抽象层的实现细节
    - 外部服务调用优化：S3连接并发控制，采用微批处理模式
10. [00:17:18](https://youtu.be/H2PYMnjdDQI?t=1038) ​**技术决策与未来规划**：

     - 暂缓在Ray中服务LLM，专注于云API集成
     - 考虑工作负载的突发性和开源模型快速演进特性
     - 优先保证团队开发速度、成本和API稳定性
     - 持续优化硬件配置，根据规模和成本选择GPU类型
     - 改进瓶颈分析和自动扩缩容规则
     - 计划发布详细技术博客分享完整经验

## **Exabyte-scale Streaming Iceberg IO with Ray, Flink, and DeltaCAT | Ray Summit 2025**

[https://www.youtube.com/watch?v=PvwPSvCCMTg](https://www.youtube.com/watch?v=PvwPSvCCMTg)

视频介绍: 本视频由亚马逊首席工程师Patrick分享，详细介绍了亚马逊在构建EB级数据湖仓过程中的技术演进，特别是从Spark迁移到Ray框架的实践经验，以及为解决Iceberg表格式中的流式写入问题而开发的开源解决方案Deltacat项目。

结论: 亚马逊通过构建基于Ray的分布式计算平台，成功解决了传统Spark在处理EB级数据湖仓 compaction 操作时的高成本和性能瓶颈问题。Deltacat项目不仅实现了82%的成本节约，还创新性地解决了Iceberg表格式中流式框架与批处理框架在删除操作上的兼容性问题。该系统通过高效的delete转换机制、多表格式同步能力和轻量级Python API，为大规模数据湖仓管理提供了完整的解决方案。未来的发展方向包括更完善的Flink集成、Python原生Beam连接器以及全栈Ray化，展现了分布式计算在数据湖仓领域的广阔应用前景。

关键点:

1. [00:01:32](https://youtu.be/PvwPSvCCMTg?t=92s) ​**亚马逊数据湖仓的技术演进历程**：

    - **Oracle数据仓库迁移**：2016年亚马逊开始从PB级的Oracle数据仓库迁移到基于S3的数据目录，目标是实现存储与计算的解耦，让数据能够被任何计算引擎查询。
    - **数据湖仓概念演进**：从最初无规则的"数据沼泽"发展到具有ACID特性的数据湖仓，结合了数据湖的灵活性和数据仓库的可靠性。
    - **技术架构演进**：最初依赖Redshift进行查询，后来逐步引入更多计算引擎，实现真正的存储计算分离架构。
2. [00:05:16](https://youtu.be/PvwPSvCCMTg?t=316s) ​**Compaction问题的本质与挑战**：

    - **小文件问题**：在数据湖仓中，持续的追加操作会产生大量小文件，导致读取效率低下，类似于从网络下载大量小文件时的延迟问题。
    - **基础Compaction**：最初通过Spark将小文件合并为256MB-1GB的标准化文件，创建读优化的表版本。
    - **去重Compaction**：更复杂的场景涉及基于主键的upsert操作，需要解决PB级甚至更大规模的数据去重问题。
    - **成本挑战**：在EB级数据湖中使用Spark进行compaction操作成本过高，促使团队寻找更高效的解决方案。
3. [00:07:47](https://youtu.be/PvwPSvCCMTg?t=467s) ​**从Spark到Ray的技术迁移**：

    - **技术选型过程**：2019年接触Ray框架，认为其任务和Actor核心原语能够为构建定制化compactor提供所需能力。
    - **验证阶段**：2019-2023年进行基础分布式系统测试，证明Ray作为分布式系统的可行性，同时满足机器学习工程师对Pythonic API的需求。
    - **影子工作流**：采用Spark和Ray并行运行的验证方式，通过数据核对服务确保两边结果一致，虽然短期内成本翻倍，但为长期优化奠定基础。
    - **显著成果**：迁移到Ray后平均降低成本82%，相当于每年节省1.2亿美元的EC2按需费用。
4. [00:12:24](https://youtu.be/PvwPSvCCMTg?t=744s) ​**Deltacat项目的架构与特性**：

    - **可移植多模态湖仓**：Deltacat设计为从笔记本到云环境都可运行的便携式解决方案。
    - **四层架构**：

      - 目录组件：提供高级管理和发现API
      - 计算组件：管理compaction作业、delete转换作业等
      - 存储层：支持内存和磁盘多模态数据集格式
      - 同步层：在不同表格式间同步元数据
    - **轻量级API**：仅需几行代码即可完成表的创建和读写操作，支持Pandas、Ray Data、Daft等多种数据格式。
    - **高级特性**：实时特征增强、多表事务、无模式表支持、有序无序写入等。
5. [00:19:50](https://youtu.be/PvwPSvCCMTg?t=1190s) ​**Iceberg表格式的核心问题与解决方案**：

    - **Iceberg架构**：基于根表元数据文件，通过快照管理表状态，支持Java和Python接口。
    - **删除操作类型**：

      - 等值删除：基于谓词的删除，需要在读取时过滤
      - 位置删除：写入时指定记录位置，读取时跳过
      - 二进制删除向量：v3规范引入的二进制表示，提高读取效率
    - **写入模式对比**：

      - 写时复制：数据生产者成本高，但消费者成本低
      - 读时合并：成本在生产和消费间分摊，可定期重写数据文件优化
    - **实际问题**：流式框架偏好等值删除但可能导致读取OOM，批处理框架只能写位置删除但存在写冲突风险。
6. [00:25:04](https://youtu.be/PvwPSvCCMTg?t=1504s) ​**创新的Delete转换解决方案**：

    - **问题本质**：Flink流式写入大量等值删除导致表无法读取，Spark重写作业也因OOM失败，形成"死表"。
    - **技术方案**：在Deltacat中新增专门作业类型，将等值删除转换为位置删除，实现分层处理：

      - 第一层：直接读取等值删除表（成本高）
      - 第二层：转换后读取位置删除表（成本中等）
      - 第三层：重写数据文件后读取（成本最低）
    - **扩展应用**：支持Apache Beam用户通过异步作业实现upsert操作，无需直接处理复杂的删除逻辑。
    - **性能表现**：在TB到PB级表上平均执行时间27秒，最大集群支持4000 vCPU，吞吐量达97TB/小时，成本仅0.55美元/TB（按需）或0.21美元/TB（Spot实例）。
7. [00:28:23](https://youtu.be/PvwPSvCCMTg?t=1703s) ​**未来发展方向与社区贡献**：

    - **集成完善**：需要开发完整的Flink IO连接器和Python原生Beam连接器，目前集成方式较为粗糙。
    - **全栈Ray化**：希望实现端到端的Ray解决方案，避免运行多个竞争框架。
    - **湖仓统一**：探索多表格式协同工作，利用不同格式的优势，实现真正的数据可移植性。
    - **开源贡献**：项目以Apache v2许可证开源，欢迎社区参与和反馈，共同推动技术发展。

## **Agentic Workload Inference at Scale: ByteDance’s AIBrix &amp; DeerFlow | Ray Summit 2025**

[https://www.youtube.com/watch?v=PIugUOQBHsI](https://www.youtube.com/watch?v=PIugUOQBHsI)

视频介绍：本视频由Bance计算基础设施团队的工程师李光分享，重点介绍了两个开源项目AirBricks和DFlow如何协同工作，以优化大规模语言模型的推理部署和管理。AirBricks是一个云原生的解决方案，专门用于部署、管理和扩展大语言模型推理，而DFlow则是一个多智能体研究框架，结合了自动化和人工参与的任务规划。

结论：AirBricks通过其创新的分层架构、分布式KV缓存卸载、预填充与解码分离、前缀感知路由等核心技术，显著提升了大语言模型推理的性能和成本效益。与DFlow框架的结合展示了从复杂问题研究到高效推理的完整工作流。该项目自开源以来获得了社区的广泛认可，并与多家行业领导者展开合作，展现了在云原生AI推理领域的强大潜力。未来，AirBricks将继续专注于降低推理成本和提升系统吞吐量，推动大规模AI推理技术的发展。

关键点：

1. [00:01:03](https://youtu.be/PIugUOQBHsI&t=63s) **AirBricks与DFlow项目概述及协同演示**

    - AirBricks是专为LLM推理设计的云原生解决方案，支持vLLM和SGLens等推理引擎，具备部署、管理和扩展模型推理的能力
    - DFlow是由Bance开源的的多智能体研究框架，构建在流行的Len生态系统之上，结合了自动化和人工参与的任务规划
    - 两个项目的协同工作演示展示了DFlow作为前端生成研究计划，AirBricks作为后端提供推理服务的完整工作流
    - 在演示中，DFlow处理了"埃菲尔铁塔比世界最高建筑高多少倍"的复杂问题，展示了其规划能力和与AirBricks后端的无缝集成
2. [00:03:22](https://youtu.be/PIugUOQBHsI&t=202s) **AirBricks的开源进展与社区生态**

    - 自2025年2月发布以来，AirBricks在GitHub趋势榜排名第一，获得了超过4000个星标和300多位贡献者
    - 项目已发展成社区驱动项目，超过50%的贡献者来自Bance外部，体现了广泛的社区认可
    - 与Google、Red Hat在Kubernetes社区的合作推动了AI推理工作负载在K8s生态系统中的发展
    - 与AWS团队的集成使AirBricks能够部署在AWS EKS上，并在KubeCon Europe 2025会议上展示了相关用例
3. [00:05:03](https://youtu.be/PIugUOQBHsI&t=303s) **AirBricks架构创新与核心组件**

    - 顶层提供OpenAI兼容的API，作为统一可扩展的入口点
    - AI网关基于Envoy网关扩展，优化实例路由并支持多种路由策略，通过分析token模式、预缓存可用性来提升路由效率
    - 模型服务层具备高密度LoRA管理能力，支持动态LoRA注册和链接，大幅降低微调模型管理成本
    - 资源编排层支持多种场景驱动的自动扩缩容策略，采用滑动窗口指标聚合减少实时指标传播延迟
    - 底层分布式解耦KV缓存层引入分布式KV缓存，实现跨KV重用，同时优化网络和内存消耗
4. [00:07:13](https://youtu.be/PIugUOQBHsI&t=433s) **版本演进路线与功能特性发展**

    - 0.1版本专注于云原生基础建设，解决冷启动延迟问题，引入GPU流式加载器和自动扩缩容
    - 0.2版本迈出分布式架构第一步，支持分布式KV缓存和异构服务编排
    - 0.3版本引入先进的KV缓存卸载解决方案，显著改善首次token延迟，增加前缀缓存感知等高级路由策略
    - 0.4版本专注于预填充解码分离、KV缓存v1连接器和多租户支持
    - 即将发布的0.5版本将包含批处理API、多模型支持和Storm服务等新特性
5. [00:10:34](https://youtu.be/PIugUOQBHsI&t=634s) **KV缓存卸载技术深度解析**

    - KV缓存在大语言模型推理中具有高度可重用性，对于提升性能至关重要
    - 传统方案中KV缓存会消耗大量GPU内存，以700亿参数模型为例，100k上下文窗口需要数百GB GPU内存
    - AirBricks引入两级KV缓存卸载解决方案：L1缓存基于本地DRAM实现单节点内快速访问，L2缓存作为外部服务缓存
    - 支持与Bance内部KV缓存解决方案（HPKV、PISDB）和开源方案（Infinity Store）的集成
    - 通过RDMA后端存储，能够在1毫秒内卸载和检索300-500个token的块，对低延迟工作负载至关重要
6. [00:14:59](https://youtu.be/PIugUOQBHsI&t=899s) **预填充与解码分离技术实现**

    - PD分离已成为行业标准，通过专用预填充和解码工作器优化各自的工作负载模式
    - 在测试中，AirBricks启用PD分离可将P95 TPOT延迟降低高达80%，特别适用于大型模型如DeepSeek R1
    - 支持两种架构：集中式KV存储架构和P2P架构，用户可根据性能与灵活性需求选择合适方案
    - 引入Row Group抽象概念，在Kubernetes上协调预填充、解码和可选本地调度器作为一个逻辑单元
    - 支持滚动升级、网络亲和性和GPU调度等高级特性，确保协同组件的性能优化
7. [00:19:10](https://youtu.be/PIugUOQBHsI&t=1150s) **自动扩缩容与高级路由策略**

    - 前缀感知路由算法（Prebam）动态决定在KV缓存共享环境中每个请求的路由目标
    - 路由决策需要平衡缓存命中率、延迟和负载分布之间的权衡
    - 架构上通过Envoy网关转发到AirBricks路由器，计算路由规则后确定目标实例
    - 支持灵活可扩展的策略定义，用户可自定义路由策略而无需修改其他代码
    - 0.4版本引入KV感知路由策略，从路由器驱动模式切换到引擎驱动模式，提高KV信息的准确性和实时性
8. [00:25:53](https://youtu.be/PIugUOQBHsI&t=1553s) **完整系统演示与性能监控**

    - 综合演示展示了AirBricks与DFlow在多轮对话中的协同工作
    - DFlow通过专门的智能体处理规划、研究和编码等特定功能，能够处理复杂任务
    - 自动扩缩容测试显示系统能在1分钟内启动新实例应对突发流量，维持系统稳定性
    - 冷启动过程分解：CUDA扩展加载5-10秒，远程GPU模型加载约15秒，CUDA图捕获和安装20-25秒
    - 系统提供完整的监控指标，包括每秒请求数、TTFT、TPOT的P50/P95分位数以及缓存命中率等关键指标

## **Pinterest’s Approach to Real-Time ML Experimentation Using Ray | Ray Summit 2025**

[https://www.youtube.com/watch?v=bxkQiaUdtJU](https://www.youtube.com/watch?v=bxkQiaUdtJU)

视频介绍：本视频由Pinterest的机器学习工程师团队分享，详细介绍了他们如何利用Ray框架显著提升机器学习开发效率，特别是在数据处理和模型训练方面实现的突破性改进。演讲者通过三个具体案例——采样优化、特征回填和下游奖励标签生成，展示了Ray如何帮助团队在竞争激烈的注意力经济中保持技术领先优势。

结论：Pinterest通过构建基于Ray的快速机器学习技术栈，成功解决了传统Spark工作流在数据处理上的瓶颈问题，将模型迭代周期从数周缩短到数天。这一技术转型不仅带来了每年超过35万美元的成本节约，更重要的是大幅提升了团队的技术创新能力，使工程师能够快速实验复杂的采样策略和下游奖励建模技术。Ray异构集群与智能分区的结合为大规模机器学习工作负载提供了灵活高效的解决方案，展现了现代ML基础设施在加速AI应用迭代中的关键作用。

关键点：

1. [00:01:46](https://youtu.be/bxkQiaUdtJU?t=106s) ​**机器学习生产力在注意力经济中的战略价值**：

    - 竞争背景分析：Pinterest作为社交媒体平台，在注意力经济中与Snapchat、TikTok等平台竞争用户参与度，这种竞争本质上是一场技术军备竞赛。视频通过2023年底的具体案例说明，即使率先推出能提升参与度的新技术，一旦竞争对手快速跟进，先发优势也会迅速消失。
    - 三要素框架：团队识别出影响竞争力的三个关键因素——计算资源（GPU数量）、技术先进性（最先进的推荐系统技术）和迭代速度（从想法到实现的周期）。其中迭代速度往往被忽视，但实际上对保持竞争优势至关重要。
    - 数据层瓶颈：多年观察发现，机器学习数据层的生成过程成为主要瓶颈。常见的操作如添加序列特征、探索采样策略、设计未来奖励等都需要大量时间生成数据集，这些过程既耗时又昂贵，严重制约了创新速度。
2. [00:02:07](https://youtu.be/bxkQiaUdtJU?t=127s) ​**快速机器学习技术栈的核心创新**：

    - Ray异构集群的应用：团队开发了基于Ray的快速机器学习技术栈，通过两个关键创新实现突破。首先是利用Ray异构集群，将常见的用例重写为Ray Data工作流，使用map_batches和Ray UDF替代传统的前向日志记录和Spark工作流。
    - 智能分区标准化：第二个创新是采用基于Iceberg的智能分区标准化，通过用户ID分桶实现数据集的协同定位。这种分区策略在不同表之间建立了已知的数据模式，极大优化了连接操作的性能。
    - 范式转变效果：这两项技术的结合带来了开发速度的突破性提升，几乎实现了范式转变。传统需要数天完成的数据处理任务，现在可以在几小时内完成，为团队创造了显著的竞争优势。
3. [00:03:13](https://youtu.be/bxkQiaUdtJU?t=193s) ​**批处理与流式处理范式的对比分析**：

    - 传统批处理范式：在Spark的批处理范式中，未采样数据集可用后，需要运行Spark作业按照正负样本比例选择数据。由于这是大数据作业，生成最终数据集需要相当长时间，只有数据集物化完成后才能开始训练，导致采样迭代需要数天时间。
    - 流式处理优势：流式处理范式完全不同，一旦未采样数据集生成，不再物化另一个样本数据集，而是直接将采样工作流重写为Ray UDF，在训练时实时使用。这使得训练可以立即开始，新采样迭代的时间从数天缩短到数小时。
    - 技术解锁关键：公司长期以来一直尝试实现这种转变，但Ray异构集群才真正解锁了这一能力。早期尝试将采样移到训练前时会出现吞吐量下降，而异构集群能够通过添加更多CPU节点来最大化GPU利用率，支持任意复杂的工作负载。
4. [00:05:24](https://youtu.be/bxkQiaUdtJU?t=324s) ​**智能分区在特征工程中的革命性应用**：

    - 基于用户ID的分桶策略：团队在Iceberg基础上添加了用户ID分桶，将属于特定用户的数据集协同定位到相同的桶中。这种设计使得在不同表之间能够快速定位特定用户ID相关的数据行，在数据中创建了已知的模式。
    - 连接操作优化：智能分区对最常见的连接操作特别有帮助。在Spark世界中，执行连接需要比较所有文件与所有文件，进行Spark连接操作，生成新表并完全物化后才能开始训练，这增加了显著延迟。
    - 实时连接实现：通过Ray和基于Iceberg的智能分区，团队实现了更好的解决方案。由于知道在用户ID上执行连接时只需要连接相同的用户桶，可以同时在Ray CPU集群中加载两个表的对应桶，将连接作为CPU操作的一部分执行。这种约束大幅减少了等待时间，且无需物化数据集，实现了快速特征回填。
5. [00:07:55](https://youtu.be/bxkQiaUdtJU?t=475s) ​**采样基础设施的重构与优化**：

    - 采样重要性分析：推荐系统数据集通常非常不平衡，包含少量正例（用户明确交互的内容）和大量负例（用户滑过的展示内容）。由于负例数量庞大，通常需要通过下采样来处理。
    - 采样策略复杂性：最佳下采样策略涉及多个可调节的参数杠杆，包括下采样比例、后处理加权策略、随机采样与智能采样选择等。每个采样参数的更改都对应一个新的数据集，构建和管理这些数据集非常困难。
    - 多任务学习场景：在Pinterest首页排序的多任务学习模型中，添加新动作头（如下载和截图）时需要相应更改采样策略，确保新添加动作的所有实例都被采样并包含在训练数据集中。当需要同时实验多个动作时，需要为每个变体生成单独的数据集，传统方法需要长达一周的时间。
6. [00:11:52](https://youtu.be/bxkQiaUdtJU?t=712s) ​**Ray最后一公里处理的实施效果**：

    - 逻辑迁移方案：团队将完全相同的采样逻辑从Spark作业转移到训练器作业本身，解决了最初遇到的所有问题。通过将采样配置与训练器配置一起指定，消除了数据生成的延迟，实现了实时处理。
    - 成本效益分析：传统批处理采样需要承担批处理计算作业和数据集存储的成本，而训练器内采样完全消除了这些成本。虽然训练成本因部分处理转移到训练作业而略有增加，但由于使用此范式的模型数量较少（少于750个），每年仍能节省超过35万美元。
    - 可观测性挑战与解决方案：采样策略更改本质上改变了训练数据集中示例的分布，由于所有处理都在训练作业内进行，无法再查询训练表来查看更改效果。团队通过记录重要列（行ID和布尔采样掩码）并在后处理中与原始未采样数据集连接，成功恢复了这一可观测性。
7. [00:16:50](https://youtu.be/bxkQiaUdtJU?t=1010s) ​**下游奖励标签生成的创新方法**：

    - 下游奖励概念：下游奖励旨在通过首页推荐促进用户的长期行为，驱动用户进入"兔子洞"（在相关Pin表面之间的遍历序列）。这与传统的即时动作建模不同，涉及用户在未来采取的一系列动作。
    - 标签生成挑战：由于涉及未来动作序列，需要正确归因以创建标签，这带来了巨大挑战。传统方法在基于Spark的工作流中计算这些标签，每次实验新标签都需要修改生产工作流并等待数据前向记录，耗时数周。
    - 序列数据基础转变：团队转向基于序列的数据，引入了DRB2系统，将用户单日动作捕获为序列数据。这些序列包括Pin ID序列、遍历的表面序列和动作序列，所有序列在同一索引下对齐。通过组合这些序列，可以推导出任何下游标签。
8. [00:23:46](https://youtu.be/bxkQiaUdtJU?t=1426s) ​**最后一公里标签聚合框架的技术实现**：

    - 框架架构：使用Ray构建了最后一公里标签聚合框架，允许用户在单行代码中推导任何下游奖励标签。从Iceberg读取包含DRB2序列的训练数据，通过Ray数据加载器中的多个用户定义函数进行处理，最终丢弃DRB2序列并用推导出的标签替换。
    - 性能优化：通过大量向量化和代码编译，团队实现了净零训练成本。标签推导基于向量化操作，类似于之前示例中展示的布尔掩码范式，通过取这些掩码的交集来识别满足所有条件的示例。
    - 业务影响：这一技术将在线实验时间从6周缩短到2天，实现了巨大的开发速度提升。工程师现在可以在周一获得想法，周二上线模型，周五就能获得模型性能的初步结果，极大加速了创新周期。
9. [00:25:18](https://youtu.be/bxkQiaUdtJU?t=1518s) ​**多日板卡重访标签的高级应用**：

    - 复杂挑战：板卡重访行为比大多数下游奖励更具前瞻性，数据科学团队确定平均重访Pin的时间通常为2-3天，这需要使用多日的DRB2序列进行标签推导以获得代表性行为。
    - 技术集成解决方案：通过使用完整的Ray技术栈，团队实现了多日板卡重访标签的生产部署。首先使用智能分桶连接将多日的DRB2序列实时连接到训练数据，然后通过最后一公里标签聚合框架推导所有下游奖励标签，最后结合训练器内采样创建所需组合的训练数据集。
    - 能力突破：这是团队之前无法实现的功能，通过整合所有Ray工具成功部署到生产环境。这不仅极大提升了开发速度，还开始优化之前不可能实现但非常重要的业务目标，展现了Ray技术在解决复杂机器学习问题上的强大能力。

## **How Prime Intellect Builds Scalable Infrastructure for Agentic RL | Ray Summit 2025**

[https://www.youtube.com/watch?v=NGxxxFlS4l0](https://www.youtube.com/watch?v=NGxxxFlS4l0)

视频介绍：本视频由Prime Intellect联合创始人兼CTO Johannes和研究负责人Will Brown共同分享，详细介绍了他们在构建开源分布式基础设施方面的最新进展，重点聚焦于测试时扩展和智能体强化学习环境的创新框架。

结论：Prime Intellect通过构建完整的开源强化学习技术栈，成功实现了从计算层到环境开发的端到端解决方案。其核心创新包括基于异步强化学习的Primer训练框架、支持多样化环境开发的Verifiers工具包，以及促进社区协作的Environments Hub平台。这些技术已在Intellect 2和正在开发的1000亿参数混合专家模型Intellect 3中得到验证，展现了开源强化学习基础设施在大规模智能体训练中的可行性和扩展性。通过开发者友好的设计理念和开放的社区协作模式，Prime Intellect正在推动强化学习技术从封闭实验室向更广泛的研究者和开发者社区普及。

关键点：

1. [00:01:37](https://youtu.be/NGxxxFlS4l0?t=97s) **Prime Intellect的开源愿景与基础设施架构**

    - **开源前沿实验室定位**：Prime Intellect将自己定位为开源前沿实验室，不仅构建自有模型，更重要的是将构建前沿模型的整个基础设施开源，包括从预训练到后训练的全流程工具链。
    - **端到端开源基础设施**：从计算层开始构建完整的开源基础设施，旨在让任何人都能从头开始训练自己的模型，覆盖从数据预处理到模型部署的完整流程。
    - **技术栈分层架构**：强化学习技术栈分为四个关键层次：计算层负责聚合全球数据中心资源；开源强化学习库Primer提供大规模异步训练能力；Verifiers工具包支持环境开发；环境中心作为平台整合整个生态系统。
2. [00:03:58](https://youtu.be/NGxxxFlS4l0?t=238s) **异步强化学习与全球分布式训练突破**

    - **Intellect 2的创新训练范式**：通过Intellect 2模型验证了全球分布式强化学习的可行性，采用单一训练集群进行参数更新，配合分布全球的推理回放工作节点。
    - **通信隐藏技术**：通过异步运行几个步骤（通常为两步）来完全隐藏模型权重广播的互联网通信延迟，实现了在分布式环境下的稳定训练。
    - **技术验证与扩展**：技术报告证实异步强化学习在大规模环境下不仅能够稳定运行，还能实现有效扩展，相比同步基线方法展现出明显优势。
3. [00:04:49](https://youtu.be/NGxxxFlS4l0?t=289s) **Primer框架的核心设计理念与特性**

    - **完全解耦架构**：Primer框架将模型训练和推理完全解耦，训练部分基于FSTP2框架，推理工作节点则托管开放API服务器。
    - **动态权重更新**：支持运行时动态权重更新，无需等待特定检查点的所有回放完成，当训练完成时即可立即更新权重。
    - **多后端广播系统**：支持多种广播后端，包括面向互联网的Shortcast和面向单一数据中心的Nickel，确保在不同网络环境下的高效通信。
    - **大规模扩展能力**：已在1000亿参数规模上验证了异步强化学习的稳定性，采用专家并行等先进技术优化训练效率。
4. [00:07:46](https://youtu.be/NGxxxFlS4l0?t=466s) **Verifiers工具包与环境中心的设计哲学**

    - **开发者友好优先**：Verifiers工具包将开发者体验作为最高优先级，采用OpenAI标准作为环境开发的基础，降低开发门槛。
    - **抽象层设计**：通过抽象化分词器、张量处理和聊天模板等技术细节，让开发者能够专注于智能体逻辑的实现。
    - **环境即包的理念**：环境中心作为包注册表运行，每个环境都是独立的Python包，支持安装、分享和复用。
    - **组合性架构**：采用面向对象设计原则，构建了从基础环境到多轮环境、工具环境、沙箱环境的分层抽象体系。
5. [00:15:04](https://youtu.be/NGxxxFlS4l0?t=904s) **评分规则抽象与奖励函数生态系统**

    - **评分规则核心地位**：评分规则抽象是Verifiers库的核心，支持轻松添加新的奖励函数，并支持在不同环境间复用奖励函数。
    - **灵活度量系统**：支持权重为零的格式奖励函数，仅用于日志记录而不影响实际奖励，便于监控和调试训练过程。
    - **LM评判集成**：提供默认的评判提示模板，同时支持自定义评判提示，将评判结果解析为具体分数。
    - **状态管理创新**：针对有状态和无状态工具的不同需求，提供相应的环境抽象，支持容器化部署和自动扩展。
6. [00:20:11](https://youtu.be/NGxxxFlS4l0?t=1211s) **环境中心的平台功能与社区生态建设**

    - **完整工具链**：提供CLI工具prime，支持环境安装、评估运行、上传分享等完整工作流。
    - **评估标准化**：通过持续集成确保环境开箱即用，验证实现正确性并与基准报告进行比对。
    - **推理服务集成**：提供推理作为平台的一等公民功能，支持闭源模型和自有API模型，即将支持训练模型的推理。
    - **计算资源支持**：作为计算平台，同时提供GPU和CPU资源，支持代码沙箱和Docker容器管理。
7. [00:24:20](https://youtu.be/NGxxxFlS4l0?t=1460s) **开放协作与生态系统集成策略**

    - **多框架支持**：除了自有Prime RL训练器，还集成支持Tinker、Sky RL、Nemo RL等多个流行开源框架。
    - **标准化接口**：要求训练器实现OpenAI端点即可接入环境，确保框架间的互操作性。
    - **社区激励计划**：实施大规模奖励计划，资助论文重实现和平台建设项目，推动社区贡献。
    - **研究驻留计划**：设立研究驻留项目，支持研究人员在平台上创建新颖的基准和环境。
8. [00:28:59](https://youtu.be/NGxxxFlS4l0?t=1739s) **技术扩展与应用前景展望**

    - **超越强化学习的应用**：异步训练协议不仅适用于强化学习，还可扩展到蒸馏、SFT和合成数据生成等领域。
    - **合成数据引擎**：环境抽象可作为一等公民的合成数据引擎，利用奖励函数作为过滤机制生成训练数据。
    - **现实世界应用映射**：支持构建映射真实世界目标的环境，从实际部署中收集轨迹并识别模式。
    - **错误修复与重训练**：支持重放用户对话中的错误轮次，生成改进版本用于训练，提升模型性能。

## **How Workday Achieved 50x Cheaper Model Serving with Ray Serve | Ray Summit 2025**

[https://www.youtube.com/watch?v=MfK1FgxCgPA](https://www.youtube.com/watch?v=MfK1FgxCgPA)

视频介绍：本视频由Workday机器学习工程师Josh Carpel分享，详细介绍了他们如何利用Ray Serve框架将模型服务成本降低50倍，并解决了在单个Ray集群中部署数千个租户模型时遇到的技术挑战。

结论：Workday通过构建基于Ray Serve的新型模型服务平台，成功解决了传统租户分片系统存在的资源浪费、扩展性差和功能限制等问题。该系统通过动态创建模型应用、分离业务逻辑与模型服务、优化控制器性能等创新方法，实现了成本的大幅降低和系统能力的显著提升。虽然在某些优化功能完全合并到上游前仍存在一些限制，但整体架构已证明能够高效支持大规模多租户模型服务场景，为类似企业级AI平台提供了宝贵的技术参考。

关键点：

1. [00:00:52](https://youtu.be/MfK1FgxCgPA&t=52s) ​**Workday的多租户模型服务挑战**：

    - **业务背景**：Workday作为HR和财务SaaS提供商，服务于全球数千家不同规模的客户（公司、医院、大学等），这些客户购买的功能中有许多由AI驱动。
    - **技术挑战**：为实现所需精度，AI功能混合使用全局模型和租户专属模型。租户模型是专门为单个客户使用其专属数据创建的，模型之间完全隔离不交叉。
    - **规模问题**：随着数千客户各自拥有多种功能的模型，系统需要管理数万甚至数十万个模型，如何经济高效地服务所有这些模型成为核心挑战。
2. [00:01:44](https://youtu.be/MfK1FgxCgPA&t=104s) ​**传统租户分片系统的问题**：

    - **系统架构**：用户编写加载和执行模型的Python应用，租户分片控制器克隆Kubernetes部署，每个分片加载分配的租户模型子集和所有必需的全局模型。
    - **主要缺陷**：

      - 资源浪费：每个分片的每个Pod都加载所有全局模型，导致全局模型副本数量过多。
      - 缺乏水平自动扩展：所有分片具有相同副本数，每个模型的扩展都耦合在一起，必须按最大客户需求配置资源。
      - 功能限制：系统一次只能在一个模型上进行分片，导致蓝绿部署、A/B测试等高级功能实现困难，需要为所有租户构建B模型即使他们不参与实验。
3. [00:04:21](https://youtu.be/MfK1FgxCgPA&t=261s) ​**基于Ray Serve的新解决方案**：

    - **架构革新**：将模型服务隔离到单一应用中，业务逻辑应用通过调用该应用实现功能，类似于内部无服务器平台。
    - **核心组件**：

      - 模型控制器：持续协调模型注册表与Serve控制器，为每个要运行的模型创建包含单个部署的Serve应用。
      - 入口应用：处理所有非模型执行相关功能，包括公共HTTP API、认证授权、速率限制等。
    - **成本优势**：通过Ray Serve的自动扩展能力，各部署独立扩展，自然实现了50倍成本降低，特别是从过度配置的初始状态优化而来。
4. [00:07:19](https://youtu.be/MfK1FgxCgPA&t=439s) ​**非标准Ray Serve使用模式**：

    - **与传统用法的差异**：标准Ray Serve配置通常包含少量应用，每个应用有几个部署，而Workday的配置只有两个核心组件（模型控制器和入口应用），所有模型都通过运行时调用serve.run和serve.delete动态创建。
    - **规模特点**：系统包含数千个Serve应用，每个应用只有一个部署，许多部署只有很少的副本，这与Ray Serve团队预期的使用模式有很大不同。
5. [00:08:01](https://youtu.be/MfK1FgxCgPA&t=481s) ​**Ray Serve内部工作机制与性能瓶颈**：

    - **请求流**：客户端请求通过Serve代理路由到入口应用，然后通过部署句柄发送到实际模型部署副本。
    - **后台数据流**：

      - 路由元数据流：句柄持续向Serve控制器查询副本位置信息。
      - 自动扩展指标流：副本和句柄都向控制器推送处理指标用于自动扩展决策。
    - **性能问题**：在10个入口副本和100个模型（300个模型副本）的配置下，系统产生了每秒30个路由元数据请求和130个自动扩展指标请求，这些后台流量即使在没有客户端请求时也持续存在，最终导致Serve控制器过载。
6. [00:13:16](https://youtu.be/MfK1FgxCgPA&t=796s) ​**Serve控制器过载的根本原因**：

    - **控制循环结构**：Serve控制器采用典型的控制循环结构，包含"执行控制器任务"和"睡眠"两个阶段。
    - **关键假设破坏**：系统假设控制器任务执行时间远小于睡眠间隔（默认100毫秒），但当控制器任务变得耗时较长时，睡眠时间减少，无法及时处理排队请求。
    - **严重后果**：请求积压导致错过Kubernetes健康检查（集群重启）或内存耗尽，系统在无流量情况下崩溃。
7. [00:16:33](https://youtu.be/MfK1FgxCgPA&t=993s) ​**性能优化策略与上游贡献**：

    - **临时修复**：通过上游PR使睡眠间隔可配置，缓解了初始问题。
    - **根本解决方案**：采用分摊处理策略，将大量小操作合并为批量操作：

      - 共享指标推送器：在每个Python进程中集中收集所有句柄的自动扩展指标，然后批量推送到控制器，将控制器CPU使用率从超过100%降至接近0%。
      - 共享LongPoll客户端：集中获取所有句柄的路由元数据，显著降低控制器负载。
    - **其他优化**：包括使用字典替代列表查找等基础算法优化，以及解决Ray与Istio端口兼容性等问题。
8. [00:21:14](https://youtu.be/MfK1FgxCgPA&t=1274s) ​**开源协作的价值与成果**：

    - **团队支持**：Ray Serve团队提供了重要帮助，包括直接实施某些修复和开放接受贡献。
    - **开源优势**：Ray Serve基于Ray核心构建，代码可读性强，使外部团队能够深入理解系统并做出有意义的改进。
    - **最终成果**：系统现在能够支持单个集群中数千个Serve应用、数万个总副本、数百个入口，每个入口可以访问所有应用的句柄，实现了比初始状态数量级提升的扩展能力。

## **Scaling LLM Post-Training at Character.AI | Ray Summit 2025**

[https://www.youtube.com/watch?v=jVIcBVqPA0g](https://www.youtube.com/watch?v=jVIcBVqPA0g)

视频介绍：本视频由Character AI的后训练与数据研究团队负责人Han分享，详细介绍了该公司如何基于Ray生态系统构建可扩展的大语言模型后训练技术栈，以及如何利用海量用户交互数据进行强化学习来持续提升模型质量和用户参与度。

结论：Character AI通过构建基于Ray的Raymon后训练框架和创新的强化学习系统，成功实现了对6000亿参数以上大语言模型的高效微调。该系统在分布式训练、数据处理、混合精度优化等方面都有显著创新，特别是Pipeline SFT训练框架和自定义混合精度优化器的开发解决了大规模模型训练中的稳定性问题。通过利用每日数亿条用户交互数据进行的SFT、DPO和RL训练，Character AI能够持续优化模型性能，在实际A/B测试中实现了用户参与度指标的显著提升。未来工作将继续探索多奖励模型组合、角色扮演和创意故事生成等方向，展现了利用真实用户数据驱动大模型优化的巨大潜力。

关键点：

1. [00:01:42](https://youtu.be/jVIcBVqPA0g?t=102) ​**Raymon后训练框架架构**：Character AI构建了基于Ray生态系统的内部后训练框架Raymon，专门用于支持大规模语言模型的微调工作。

    - 框架核心采用Ray Train作为分布式训练编排层，支持动态扩缩容配置和自动化模型检查点管理，能够灵活调整训练资源并保留最佳模型版本。
    - 数据处理层使用Ray Data实现并行化处理，所有数据集操作都能在所有Ray工作节点上并行执行，自动跨节点分配资源，显著提升数据处理效率。
    - 框架支持从GCS Parquet文件高并发读取数据，这是公司的主要数据存储格式，同时支持流式读取和惰性处理，避免了训练开始前对整个数据集进行预标记的耗时操作。
    - Raymon框架集成了内部数据存储系统，支持高效的数加载和处理流程，并通过Hydra配置系统轻松修改训练配置和启动参数扫描，大大提升了实验迭代速度。
2. [00:04:07](https://youtu.be/jVIcBVqPA0g?t=247) ​**Pipeline SFT训练框架创新**：为了解决开源方案在扩展DeepSeek等大模型时的局限性，Character AI开发了自研的Pipeline SFT训练框架。

    - 该框架支持多级并行策略，包括跨节点的流水线并行、节点内8个rank间的专家并行和张量并行，通过合理的分片策略优化了通信效率。
    - 在混合精度训练方面，实现了自定义的混合精度AdamW优化器，在保持前向和反向计算使用BF16的同时，主权重维持在FP32精度，在几乎不增加训练时间和内存开销的情况下确保了低学习率下的稳定收敛。
    - 为了解决分布式训练中的梯度发散问题，在MLA层和MOE层都添加了梯度同步功能，通过后向钩子确保在张量并行和专家并行组内的复制参数梯度一致性，有效防止了训练不稳定和梯度爆炸问题。
    - 框架与Hugging Face生态系统无缝集成，支持直接从Hugging Face模型目录加载训练，并能导出为Hugging Face检查点格式，大大提升了框架的易用性和兼容性。
3. [00:09:33](https://youtu.be/jVIcBVqPA0g?t=573) ​**FP8量化与推理优化**：为了解决BF16训练模型推理成本高的问题，Character AI探索了FP8量化技术。

    - BF16版本的DeepSeek模型推理需要多个节点，而节点间通信速度远慢于节点内通信，FP8量化版本可以单节点部署，显著降低推理成本。
    - 研究发现简单的训练后量化会削弱微调效果，因此实现了基于DeepSeek技术报告的FP8混合精度训练方案，使用DeepJAM库进行高效的FP8矩阵乘法运算和FP32累加来缓解精度损失。
    - 初始的FP8线性层实现效率较低，比原生PyTorch BF16线性层慢约10倍，通过实现融合量化操作版本，获得了2-3倍的性能提升，为实际生产部署奠定了基础。
4. [00:11:42](https://youtu.be/jVIcBVqPA0g?t=702) ​**用户数据驱动的SFT与DPO训练**：Character AI充分利用海量用户交互数据进行监督微调和直接偏好优化。

    - 通过用户滑动选择不同回复的功能，每日收集约2亿条带滑动数据的对话回合，经过质量过滤后形成用户引导的拒绝采样数据，用于SFT训练。
    - 用户聊天数据还可用于从高质量昂贵模型到成本效益更高模型的知识蒸馏，研究发现数据质量过滤和数据多样性对蒸馏效果至关重要。
    - DPO训练利用用户滑动数据、用户并排偏好数据和针对模型问题的合成偏好数据，通过多种过滤策略处理用户偏好数据的噪声问题，包括确保用户安全性、限制个体用户影响、避免频繁滑动对话等。
    - 这些基于真实用户交互的训练数据为模型优化提供了丰富且高质量的信号，是Character AI能够持续改进模型性能的关键因素。
5. [00:14:34](https://youtu.be/jVIcBVqPA0g?t=874) ​**基于用户交互的强化学习系统**：Character AI构建了完整的RLHF流水线，利用丰富的用户交互数据进行强化学习。

    - 核心挑战在于开放式创意对话缺乏可验证的奖励信号，公司设计了基于专业写作原则的多个评估指标，结合创意写作技巧和客观维度如新颖贡献度等。
    - 奖励建模不仅包括常见的质量和用户偏好奖励模型，还重点构建了基于参与度信号的奖励模型，如预测未来对话轮数、用户是否继续对话、到下次回复的时间等。
    - 特别创新的是训练了预测长期用户留存的奖励模型，如预测用户次日回归和7日后活跃度的模型，准确率分别达到84%和83% AUC，虽然可能学习到了一些捷径特征，但在优势计算中复合因素会被抵消。
    - 在线A/B测试是最终验证标准，关注会话数、花费时间和最重要的留存指标如L1 D1、L1 D7和DAU，确保优化目标与实际业务价值对齐。
6. [00:22:04](https://youtu.be/jVIcBVqPA0g?t=1324) ​**强化学习算法优化与实践**：Character AI在强化学习算法选择和改进方面进行了深入探索。

    - 比较了PPO、A-LoO和GRPO等多种算法，发现GRPO在稳定性方面表现更好，能够在相同奖励函数下训练更多步数，虽然其方差缩放项可能导致生成结果更加确定性。
    - 实现了多种自定义奖励函数，包括使用启发式权重的加权和、学习最大化真实KPI（如7日留存）的权重，以及带约束的多目标奖励函数，对安全性等关键维度设置硬约束或软约束。
    - 实验结果显示，使用参与度奖励模型进行RL训练后，模型在验证集上的参与度评分持续上升，在线A/B测试中人类消息数提升9%，花费时间提升9%，会话数提升4%，DAU提升超过3%。
    - 当前正在探索多奖励模型组合训练，以延长训练步数避免模式崩溃，并进一步扩展模型在角色扮演和创意故事生成方面的能力。

## **Meet verl: An RL Framework for LLM Reasoning &amp; Tool Use | Ray Summit 2025**

[https://www.youtube.com/watch?v=rkuwoKt357c](https://www.youtube.com/watch?v=rkuwoKt357c)

视频介绍：本视频由字节的研究科学家Hongong分享，详细介绍了他们开发的强化学习训练框架verl。演讲重点探讨了大规模语言模型强化学习的重要性、系统挑战，以及verl框架如何通过混合控制器架构解决这些挑战，同时展示了最新的代理式RL训练和大模型训练能力。

结论：verl框架通过创新的混合控制器架构，成功平衡了强化学习训练中的灵活性与效率需求。该框架不仅支持多种经典和新兴RL算法，还针对代理式任务和大规模模型训练进行了专门优化。通过单控制器的算法灵活性和多控制器的高效执行，verl为复杂RL工作流提供了强大支持。未来的路线图包括更精细的组件重构、异步训练管道优化以及对多模态数据的支持，展现了其在推动RL技术发展中的重要价值。

关键点：

1. [00:01:11](https://youtu.be/rkuwoKt357c&t=71s) ​**大规模RL对语言模型的重要性**：

    - **增强推理能力**：通过对比未使用RL训练的GPT-4O与使用RL训练的推理模型（如01模型），在数学、编程等需要复杂推理的任务上存在显著差距。RL训练能大幅提升语言模型的推理能力，使其在处理复杂问题时表现更优。
    - **实现代理式工具使用能力**：RL后训练能够赋予模型代理式行为能力，如深度研究浏览器使用、代码编写代理等应用。这些能力使得模型能够与环境进行多轮交互，完成更复杂的任务。
    - **日益增长的重要性**：由于RL带来的推理能力和代理式能力，越来越多的研究者和工程师关注如何使用RL训练语言模型，使其成为LM训练流程中的重要组成部分。
2. [00:02:53](https://youtu.be/rkuwoKt357c&t=173s) ​**大规模RL训练的系统挑战**：

    - **复杂工作流编排**：RL框架包含多种算法模块，如策略模型（生成轨迹和模型更新）、奖励模型（提供反馈）、参考模型（防止模型偏离基础）和价值模型（预测长期价值）。这些模块运行不同的程序，具有不同的输入输出，协调这些多样化的工作流极具挑战性。
    - **多样化工作负载**：不同模块执行不同类型的任务，如策略模型进行轨迹生成（主要是解码）和训练（计算密集的前向和后向步骤），而参考和奖励模型主要进行推理，同时还需要定期的权重同步。这种工作负载的多样性增加了RL训练的复杂性。
    - **大规模模型并行化**：当训练大型语言模型时，所有算法模块都自然成为大量进程的集合。需要为不同模块协调具有不同并行化策略的进程组，并充分利用硬件资源以实现高效运行。
3. [00:05:48](https://youtu.be/rkuwoKt357c&t=348s)​**verl框架的核心优势：灵活且高效**：

    - **混合控制器架构**：verl创新性地结合了单控制器和多控制器的优势。单控制器负责算法逻辑和RL数据流控制，提供编程灵活性；多控制器负责繁重的训练和生成工作负载，保证执行效率。
    - **单控制器的灵活性**：开发者可以像编写单进程程序一样在单控制器中指定算法逻辑，仅需约10行代码就能实现PPO训练的关键组件。这种设计使得新算法的集成变得非常容易，目前verl已支持PPO、GRPO等多种经典和新算法。
    - **多控制器的高效性**：verl深度集成了多种多控制器训练和生成后端，如FSDP、Megatron等训练框架，以及vLLM、SGL等生成引擎，支持各种并行化策略和高效内核，确保模型训练和序列生成的速度和效率。
4. [00:12:54](https://youtu.be/rkuwoKt357c&t=774s) ​**代理式RL训练的新能力**：

    - **与传统RL训练的差异**：传统RLHF训练中，策略模型仅生成文本标记，然后由奖励模型评分。而在代理式训练中，策略LLM不仅生成文本，还生成更丰富的上下文和特殊标记，如代码编写代理需要与环境（如代码沙箱）进行多轮交互。
    - **异步rollout解决方案**：针对代理式任务中样本时间线多样性的挑战，verl引入了异步rollout场景，使每个样本具有独立的时间线，消除了批量处理中的气泡问题，显著提高了硬件利用率。
    - **代理循环接口**：作为LM生成引擎与环境之间的桥梁，代理循环接口允许用户指定自定义逻辑，支持高并发异步API调用，为各种代理式任务提供了灵活的编程接口。
5. [00:17:47](https://youtu.be/rkuwoKt357c&t=1067s) ​**大规模模型训练实践与框架重构**：

    - **大模型训练验证**：verl已成功支持6710亿参数的DeepSeek-V3模型训练，使用96个H100 GPU，结合Megatron训练后端和跨节点张量并行，验证了训练曲线的良好表现。
    - **框架组件化重构**：计划将verl重构为五个核心组件：rollout引擎（轨迹生成）、模型引擎（模型定义和训练）、权重传输引擎、代理循环和数据传输系统，目标是提供更清晰的API和更高的可定制性。
    - **标准化与部署能力**：通过提供初始化模型、前向传播、后向传播和优化器步骤等标准化接口，使verl能够部署为服务，支持更广泛的应用场景。
6. [00:21:37](https://youtu.be/rkuwoKt357c&t=1297s) ​**未来发展路线图**：

    - **技术优化方向**：包括部分rollout、全异步训练管道、低比特量化以优化rollout性能、更好的代理式RL配方，以及多模态数据传输支持。
    - **社区发展现状**：自一年前发布以来，verl已获得约1.5万星标、2000多个分支和约400名贡献者，形成了丰富的算法配方和硬件支持能力。
    - **持续发展承诺**：团队致力于持续改进框架，欢迎更多开发者参与贡献，共同推动RL训练技术的发展。

## **Marin: Open Development of Open Foundation Models | Ray Summit 2025**

[https://www.youtube.com/watch?v=Q0Yy_prrlpY](https://www.youtube.com/watch?v=Q0Yy_prrlpY)

视频介绍: 本视频由前斯坦福大学基础模型研究中心研究工程负责人、现Open Athena非营利组织成员David Hall分享，详细介绍了Maren项目——一个致力于通过开源开发模式构建基础模型的开放实验室。演讲探讨了AI模型从开放到封闭的发展趋势，并展示了Maren如何通过实验驱动的方法、Ray分布式计算框架和社区协作来推动开放式AI模型开发。

结论: Maren项目成功证明了在开源环境下开发高性能基础模型的可行性，其发布的8B和32B参数模型在多项基准测试中表现出色，超越了部分主流开源模型。通过将软件工程的优秀实践（如GitHub工作流、数据流图表示）应用于机器学习研究，Maren建立了一个透明的、可复现的模型开发框架。该项目充分利用Ray处理TPU集群的调度挑战，特别是在抢占式计算环境下的稳定性问题，为社区驱动的AI开发提供了宝贵经验。随着多语言模型等社区项目的推进，Maren正在朝着"构建基础模型的Linux"这一愿景稳步前进。

关键点：

1. [00:01:52](https://youtu.be/Q0Yy_prrlpY?t=112) ​**AI模型的开放性与封闭化趋势**：

    - 历史对比：David将AI发展与开源软件运动进行类比，指出早期AI模型如ELMo、BERT（2017-2018年）完全开放，包含论文、代码、数据和权重，而现代前沿模型如GPT-5则越来越封闭，仅提供API和技术报告博客。
    - 开放程度谱系：详细区分了四种模型开放程度：

      - 封闭模型：如GPT-5、Claude、Gemini，仅提供API访问
      - 开放权重模型：如Llama、DeepSeek，提供模型权重但缺乏完整训练细节
      - 开源模型：如GPT-J、AI2的OLMo，提供权重、代码和数据配方
      - 开放开发：Maren采用的方式，不仅公开所有组件，还开放整个开发过程，允许社区在模型训练期间实时参与贡献
    - 核心问题：演讲者提出关键疑问——既然AI像软件一样从开放走向封闭，我们能否再次将其开放？这为Maren项目的使命奠定了理论基础。
2. [00:03:55](https://youtu.be/Q0Yy_prrlpY?t=235) ​**Maren项目的实际成果验证**：

    - 模型性能表现：Maren 8B基础模型在19个不同的英语和编程任务上超越了长期作为行业标杆的Llama 3.1 8B模型，证明了开源方法能够产生有竞争力的模型。
    - 规模扩展成功：新发布的32B参数模型性能位于当前最佳开源权重模型之间，具体处于Qwen 2.5和Gemma 27B模型之间，显示了项目在模型规模扩展上的能力。
    - 开发时间线：8B模型发布于4个月前，32B模型发布于演讲前一周，体现了项目持续快速的迭代能力。
    - 意义：这些成果有力地反驳了"只有封闭开发才能产生顶尖模型"的观点，为开源AI开发提供了实证支持。
3. [00:06:28](https://youtu.be/Q0Yy_prrlpY?t=388) ​**实验即代码的开发范式**：

    - GitHub工作流适配：Maren将软件开发的issues和PRs概念应用于机器学习研究，将实验视为机器学习研究的"变更列表"或"拉取请求"。
    - 实验流程标准化：

      - 预注册：通过GitHub issue明确记录假设和测试方案
      - 实验定义：通过PR提交定义实验脚本的代码
      - 执行管道：自动化的实验执行流程
      - 结果追踪：集成Weights & Biases等工具进行后分析
    - 具体案例：优化器选择实验展示了这一流程的实际应用，测试了11种不同优化器，包括AdamW基线、各种Adam变体、内存高效优化器如Lion以及基于矩阵的优化器如Muon和SOAP。
    - 重要发现：实验揭示了许多论文中表现优异的优化器在实践中失效的原因——基线模型调参不足，仅通过适当调整AdamW的学习率就能获得2倍加速，超过了某些已发布优化器的增益。
4. [00:09:47](https://youtu.be/Q0Yy_prrlpY?t=587) ​**数据流图表示的实验框架**：

    - 图形化表示：Maren将实验表示为数据流图，每个节点代表需要完成的原子工作单元，产生特定结果，节点之间通过依赖关系连接。
    - 实际示例：展示了从预训练语料库tokenization开始，到在Paloma上评估监控训练进度，再到模型训练、在Tulu v3上微调，最后在MMLU等评估套件上测试的完整流程。
    - Python实现：整个实验作为Python脚本中的数据流图实现，提供包含强基线的组件默认值库，常见数据集和默认配置可直接导入使用。
    - 依赖管理：步骤间存在依赖关系，会自动阻塞执行直到前置条件满足，Ray负责正确调度任务执行顺序。
    - 扩展能力：从单机几小时完成的3.5亿参数玩具模型，到训练约200个不同模型直至10^20 FLOPS的完整等计算量测试套件，都能通过单一文件定义。
5. [00:13:58](https://youtu.be/Q0Yy_prrlpY?t=838) ​**32B模型训练中的挑战与解决方案**：

    - 初始设置：使用Nvidia的Neatron CC高质量网络文本数据集、StarCoder高质量编程数据集和ProofPile数学数据集，采用Llama风格的32B参数架构。
    - 训练问题：损失曲线出现异常尖峰，虽然模型大多能快速恢复，但出现了"坏尖峰"——损失上升后无法恢复，表明参数中出现了无法修复的损坏。
    - 调试尝试：

      - 跳过坏步骤前的某些步骤
      - 收紧梯度范数裁剪（发现正常梯度范数通常小于0.2）
      - 裁剪更新范数限制优化器步长
      - 尝试基于之前实验成功的Muon优化器
    - 最终解决方案：添加QK Norm（已知能稳定模型的技术，被Qwen等模型使用），通过热重启从最后一个良好检查点继续训练，模型在约100亿token后重新达到原有水平。
    - 冷却混合：训练末期采用标准的中期训练技术，将高质量数据（Stack Exchange、Wikipedia、问答数据）以70:30比例与原始背景数据集混合，同时开始学习率冷却，在代码等特定评估集上观察到显著提升。
6. [00:19:49](https://youtu.be/Q0Yy_prrlpY?t=1189) ​**Ray与TPU集群的集成挑战**：

    - 计算基础设施：主要依赖Google TPU研究云提供的免费抢占式TPU资源，节点可能运行几分钟到几天后就被回收。
    - TPU特性挑战：TPU通过高速ICI互连紧密连接，每4个TPU由一个VM控制，所有VM需要同时运行完全相同的内核，这对Ray的调度能力提出挑战。
    - 解决方案：开发了简单的API包装器，处理群组调度、重试和多TPU切片协调，基于特殊资源标记实现：

      - TPU VM获得标记指示所属切片名称
      - 第0个VM获得TPU切片头节点特殊类型
      - 通过资源约束确保调度协调性
    - 抢占处理：当切片失效时，通常只有一个组件最初失败，但可以杀死整个任务、重试并重新开始。
    - 多切片训练：利用TPU的多切片功能协调多个非ICI连接的TPU组，通过数据中心网络连接，特别适合抢占式计算环境，因为小切片更容易找到空闲资源。
7. [00:25:33](https://youtu.be/Q0Yy_prrlpY?t=1533) ​**社区驱动的发展模式**：

    - 开放参与：通过Discord进行主要协调，实时讨论结果和调试问题，为想学习预训练、微调、数据整理的人员提供开放学习环境。
    - 社区项目：8B参数多语言版本模型完全由社区成员发起，社区负责寻找数据、设计评估，核心团队仅负责启动运行和监控。
    - 资源分配策略：虽然不能直接提供计算资源访问（避免产生高额费用），但通过提供框架和专业知识帮助社区成员进行模型训练，特别关注安全预训练相关方向。
    - 数据许可考虑：由于完全开放，只能使用广泛可用的数据（如Common Crawl），需要解决敏感版权数据的移除问题。
    - 最终愿景：将Maren定位为构建"基础模型的Linux"的共同努力，邀请更多人加入这一开放AI开发运动。

## **Applied Intuition’s Blueprint for Scalable RL + Batch Inference | Ray Summit 2025**

[https://www.youtube.com/watch?v=pkkV1US2IKc](https://www.youtube.com/watch?v=pkkV1US2IKc)

视频介绍：本视频由Applied Intuition公司的工程师团队分享，详细介绍了他们如何利用Ray框架构建大规模GPU基础设施，支持批量推理和强化学习等关键应用场景。

结论：Applied Intuition通过构建基于Ray的统一计算平台，成功解决了在自动驾驶领域面临的大规模数据处理和模型训练挑战。该平台采用多区域多集群架构，集成Kubernetes和Kueue批处理调度器，实现了资源利用率的显著提升。在批量推理方面，通过Ray Data实现CPU和GPU操作的流水线处理，获得了10倍成本优化；在强化学习场景中，通过RLlib框架和placement groups优化，有效支持了端到端驾驶模型的训练。Ray的统一运行时环境使得公司能够在单一框架下处理多样化的工作负载，为自动驾驶技术的研发提供了坚实的技术基础。

关键点：

1. [00:03:23](https://youtu.be/pkkV1US2IKc&t=203s) ​**基于Ray的基础设施架构设计**：

    - 多集群架构设计：采用管理集群和工作集群分离的架构模式。管理集群负责作业编排、监控和认证，工作集群分布在不同区域实际执行任务。这种设计抽象了多区域复杂性，用户只需提交作业到统一入口，系统自动选择资源可用区域执行。
    - Kubernetes集成：Ray运行在Kubernetes之上，通过自定义Ray Job资源实现作业管理。管理集群使用Kubernetes informer API跟踪作业状态，确保整个系统的可靠运行。
    - 监控体系：集成Prometheus和Thanos进行指标聚合，通过Grafana向用户提供统一的监控视图，用户无需关心作业具体运行位置。
    - 资源调度优化：采用Kueue批处理调度器，支持拓扑感知调度，将相关Pod在网络拓扑中紧密部署以减少延迟。团队间支持资源配额和抢占机制，引入抢占后集群利用率从75%提升至90%。
2. [00:08:27](https://youtu.be/pkkV1US2IKc&t=507s) ​**开发体验与迭代优化**：

    - 镜像构建集成：与Bazil构建系统深度集成，用户提供Python目标即可自动构建Docker镜像并推送到集群。虽然面临镜像内容不足和推送速度慢的双重挑战，但提供了标准化的部署流程。
    - 持久化集群加速迭代：针对传统方式需要5-10分钟启动时间的问题，开发了持久化Ray集群方案。通过Docker镜像封装第三方依赖，zip文件包含第一方代码，利用Ray运行时环境机制提交，将迭代时间从数分钟缩短至数秒。
    - Python路径优化：为实现与Bazil系统的无缝集成，团队进行了复杂的Python路径调整，确保代码能够正确加载和执行。
3. [00:11:18](https://youtu.be/pkkV1US2IKc&t=678s) ​**批量推理的技术演进与优化**：

    - 业务价值：批量推理在PB级视频数据上运行感知模型获取边界框，为下游流程提供关键数据。一方面节省人工标注成本，标注员只需验证和微调模型输出；另一方面为端到端模型训练提供指导信号。
    - 技术方案演进：

      - 初始方案：复用训练脚本的数据加载器，跳过反向传播只进行前向推理。优点是可重用现有代码，但CPU预处理成为瓶颈，且难以结构化后处理任务。
      - 工作流编排器方案：使用Flight工作流编排器，将预处理、推理、后处理作为独立阶段分别扩展。虽然解决了资源分配问题，但需要在Ray和Flight两个框架间切换，且阶段间数据需要写入对象存储，导致运行时延长。
      - Ray Data最终方案：通过Ray Data实现阶段间数据流水线处理，数据在Ray对象存储中本地传递，避免了远程存储的IO开销。通过yield产生更小的结果块，保持流水线持续运行，实现了10倍成本优化。
4. [00:17:00](https://youtu.be/pkkV1US2IKc&t=1020s) ​**Ray Data性能调优实践**：

    - 问题诊断：自动驾驶数据批次体积庞大，单个样本可达数百MB，导致处理块过大。通过Ray内置的Prometheus指标和仪表板发现对象存储内存使用和CPU利用率呈现振荡模式，表明流水线出现阻塞。
    - 根本原因：上游阶段快速产生大块数据并发布到Ray对象存储，下游消费者处理速度跟不上，导致系统需要限制块生产，处理变得近乎串行。
    - 解决方案：利用Ray Data的yield机制产生更小的处理结果，保持数据在流水线中的持续流动。通过精细控制块大小和生产消费节奏，实现了处理性能的显著提升。
5. [00:19:41](https://youtu.be/pkkV1US2IKc&t=1181s) ​**强化学习框架与应用**：

    - 自动驾驶RL场景：智能体为端到端驾驶模型，策略是通过训练学习的权重，环境使用自研模拟器和日志回放，奖励函数基于驾驶舒适度和车道保持等指标计算。
    - Ray RL基础设施：利用Ray的分布式特性解决RL固有的分布式系统问题。环境 rollout 和学习器之间通过权重同步和数据传递进行协作，Ray便于创建单控制器类型的RL应用。
    - RLlib新API栈评估：采用RLlib的模块化新API栈，算法类作为控制器，包含Env runners和learner groups。该框架支持模仿学习和强化学习的统一处理，满足行为克隆和精细调优的需求。
6. [00:23:28](https://youtu.be/pkkV1US2IKc&t=1408s) ​**GPU资源调度深度优化**：

    - 默认调度局限：传统方式通过指定num_gpus参数进行调度，无法精确控制不同组件在GPU上的分布，可能导致资源浪费。
    - 拓扑优化需求：希望在每个GPU上部署一个learner，同时搭配多个训练和评估runner，实现计算资源的充分利用。
    - Placement Groups解决方案：通过创建每个GPU对应的bundle，为各个actor指定极低的GPU需求（如0.0001），然后使用placement group index精确控制actor在特定GPU bundle上的部署。这种方法实现了细粒度的资源调度优化，提升了GPU利用率和数据传输效率。
    - 框架扩展：在RLlib基础上实现了placement group优化、数据传输优化、episode队列和权重队列等定制功能，虽然需要大量修改，但RLlib的模块化设计为这些扩展提供了良好基础。

## **Ray @ Robinhood: Distributed ML Training with KubeRay | Ray Summit 2025**

[https://www.youtube.com/watch?v=fUwAyH3VfWs](https://www.youtube.com/watch?v=fUwAyH3VfWs)

视频介绍：本视频由Robinhood的AI基础设施团队成员Robert和Lanting分享，详细介绍了他们如何采用Ray和Kubernetes Operator（KubeRay）构建分布式机器学习训练平台，以解决单节点训练面临的资源限制和性能瓶颈问题。

结论：通过采用Ray和KubeRay，Robinhood成功构建了一个灵活、安全的分布式训练平台，将可训练数据集规模提升了7倍，显著减少了GPU资源等待时间，同时保持了与现有开发流程的无缝集成。该解决方案通过按需创建Ray集群、严格的权限控制和统一的监控体系，为金融场景下的机器学习训练提供了可靠的基础设施支持，展现了开源技术在复杂企业环境中的成功应用。

关键点：

1. [00:01:28](https://youtu.be/fUwAyH3VfWs?t=88s) ​**Robinhood机器学习基础设施背景与挑战**：

    - **技术栈特点**：团队主要使用XGBoost进行梯度提升决策树训练，深度学习用例则统一采用PyTorch框架。作为金融机构，安全是首要考虑因素，所有训练都需遵循最小权限原则，确保模型仅访问授权数据。
    - **安全要求**：严格限制本地数据访问，所有训练作业和模型制品都需要具备完整的审计追踪能力。这种安全约束直接影响了训练作业的运行时长和资源使用策略。
    - **业务用例**：主要机器学习应用集中在欺诈检测、跨产品销售和信用卡业务等关键领域，这些场景对模型性能和训练效率提出了较高要求。
2. [00:02:45](https://youtu.be/fUwAyH3VfWs?t=165s) ​**原有单节点训练流程分析**：

    - **三阶段流程**：模型开发者在本地编写代码 → 通过CLI打包上传至存储 → 在云端VPC环境中部署执行。整个流程通过内部的King's Cross平台进行管理，用户可以通过Grafana和MLflow监控作业状态。
    - **Kubernetes集成**：平台将用户请求转换为Kubernetes API调用，指定用户角色权限、节点类型和资源需求（CPU、GPU、内存）。使用预构建的集中化镜像，确保依赖一致性。
    - **架构局限性**：采用基于Kubernetes Job的架构，包含初始化容器下载代码和主训练容器执行逻辑。虽然早期适用于树模型和小数据集，但随着深度学习用例增加和模型规模扩大，单节点架构逐渐无法满足需求。
3. [00:06:56](https://youtu.be/fUwAyH3VfWs?t=416s) ​**转向分布式训练的核心动因**：

    - **作业时长限制**：出于安全考虑，平台对临时开发作业设置了运行时长限制，结合单节点资源限制，严重制约了可训练的模型规模和数据集大小。
    - **资源需求增长**：部分训练任务需要大量GPU和内存资源，已超出单节点容量。从树模型向深度学习架构的转变，使得数据并行等分布式训练方案变得必要。
    - **资源利用率问题**：用户对GPU资源的激烈竞争，加上CPU、内存、GPU需求与节点配置不匹配，导致资源利用率低下。大型GPU节点难以按需扩展，经常出现容量不足错误。
4. [00:09:39](https://youtu.be/fUwAyH3VfWs?t=579s) ​**选择Ray作为分布式训练框架的理由**：

    - **灵活性优势**：Ray提供了灵活的分布式平台，能够满足Robinhood当前和未来的各种训练用例需求。其高层库（Ray Data、Ray Train）为流式数据处理和ML框架集成提供了强大工具。
    - **平滑迁移**：用户已有的单节点训练作业可以相对容易地迁移到多节点分布式训练环境，保持了开发体验的连续性。
    - **生态系统整合**：Ray集成了训练、超参数优化和服务等组件，为构建完整的模型开发生命周期体验奠定了基础，虽然当前主要专注于训练环节。
5. [00:11:03](https://youtu.be/fUwAyH3VfWs?t=663s)​**KubeRay在Kubernetes环境中的集成策略**：

    - **架构决策**：选择为每个作业运行创建独立的Ray集群，而非长期运行的共享集群。这种设计避免了"吵闹邻居"问题，确保一个作业的资源使用波动不会影响其他作业。
    - **权限控制**：利用Kubernetes命名空间作为权限边界，每个Ray集群挂载仅具有必要权限的服务账户，严格遵循最小权限原则。
    - **成本优化**：按需创建和销毁集群避免了闲置资源浪费，虽然带来了冷启动开销，但相对于训练作业总时长来说影响较小。
6. [00:14:50](https://youtu.be/fUwAyH3VfWs?t=890s) ​**代码分发与依赖管理机制**：

    - **开发流程优化**：为开发阶段用户提供快速迭代能力，允许通过S3上传代码包，所有Ray集群节点通过初始化容器从S3下载代码，无需代码审查和镜像重建。
    - **生产流程标准化**：生产作业通过Git提交、代码审查后自动构建容器镜像，将训练代码层叠在Ray基础镜像之上，包含框架特定的pip依赖。
    - **依赖预安装策略**：采用单代码库架构，预安装pip依赖确保版本兼容性。虽然允许用户添加新依赖并重建镜像，但遵循不在生产环境中动态安装组件的安全策略。
7. [00:18:16](https://youtu.be/fUwAyH3VfWs?t=1096s) ​**基于Archetype框架的集群管理集成**：

    - **自定义资源分离**：将Kubernetes原生Job和Ray Cluster资源分开创建，而非使用Ray Job自定义资源。这种设计便于与内部Archetype框架集成。
    - **统一管理界面**：通过Archetype的部署仪表板，用户可以查看作业状态、版本信息和监控链接，无需直接操作Kubernetes命令行工具。
    - **自动化部署**：当用户提交训练代码到Git后，生产环境中的作业会自动更新，实现了持续部署的自动化流程。
8. [00:22:49](https://youtu.be/fUwAyH3VfWs?t=1369s) ​**作业提交与集群通信架构**：

    - **API选择**：采用Ray Jobs API而非Ray Client进行作业提交，通过Python SDK实现更可读的代码。将提交逻辑打包到容器镜像中，在Kubernetes原生Job中运行。
    - **端到端流程**：用户请求分布式训练作业时，King's Cross服务器创建Archetype包装的Kubernetes Job和Ray Cluster。提交Job通过Kubernetes Service发现Ray集群头节点并提交作业请求。
    - **环境隔离**：支持在完全隔离的Kubernetes环境中运行开发和生产流程，开发环境运行未经代码审查的代码，生产环境运行经过审核的稳定版本。
9. [00:25:18](https://youtu.be/fUwAyH3VfWs?t=1518s) ​**自动化运维与资源清理机制**：

    - **垃圾收集器**：定期轮询Kubernetes API服务器，清理已完成训练作业的Ray集群和相关Kubernetes Job，确保资源及时释放。
    - **状态管理**：通过独立的垃圾收集组件保持King's Cross服务器无状态，提高系统可靠性和可扩展性。
    - **监控集成**：为用户提供统一的监控链接，通过部署仪表板集中查看训练作业日志和状态，简化运维复杂度。
10. [00:26:01](https://youtu.be/fUwAyH3VfWs?t=1561s) ​**实施成果与性能提升**：

     - **数据规模突破**：将可训练数据集规模提升了7倍，突破了单节点资源限制，为更大规模模型的训练提供了可能。
     - **资源利用率优化**：通过将大型作业分发到多个小型GPU节点，显著减少了GPU资源等待时间，实现了更可预测的排队时间。
     - **成本效益**：避免了为满足峰值需求而过度预留大型GPU节点，提高了整体资源利用效率，同时保持了开发体验的一致性。

## **How Roblox Trains 3D Foundation Models with Ray | Ray Summit 2025**

[https://www.youtube.com/watch?v=FhZaVU5-2xE](https://www.youtube.com/watch?v=FhZaVU5-2xE)

视频介绍：本视频由Roblox机器学习平台团队分享，详细介绍了他们如何利用Ray框架构建3D基础模型训练平台，并展示了其开源的Cube 3D模型实时生成能力。演讲通过现场演示、技术架构解析和性能优化案例，全面展现了在Roblox海量用户规模下实现高效分布式训练的工程实践。

结论：Roblox通过构建基于Ray的混合云训练平台，成功解决了3D基础模型训练中的数据预处理、资源调度和计算扩展等核心挑战。该平台实现了从开发环境快速迭代到大规模训练任务的无缝衔接，通过异构集群管理、智能资源调度和多云架构，为创作者提供了强大的AI辅助创作能力。未来将继续优化训练效率和支持更复杂的3D生成任务，展现了Ray在工业级AI系统中的广泛应用价值。

关键点：

1. [00:00:05](https://youtu.be/FhZaVU5-2xE&t=5s) ​**Cube 3D模型的实时生成演示与愿景**：演讲开场通过现场互动演示展示了Roblox开源的3D基础模型的强大生成能力。

    - **实时生成体验**：在Roblox环境中，团队现场根据观众提议生成了"可爱外星四轮车"、"空气炸锅"、"战斗机甲"等多种3D物体，每个生成对象不仅包含精细的3D网格，还具备完整的纹理贴图和交互功能。
    - **技术特性展示**：生成的3D模型可以直接在虚拟环境中驱动和交互，体现了模型输出的实用性和完整性。该模型已在Hugging Face开源，供开发者体验和使用。
    - **未来愿景**：这代表了Roblox向"随需创造"愿景迈出的重要一步，未来用户只需简单提示就能与AI协同创建完整的虚拟场景和物体。
2. [00:03:30](https://youtu.be/FhZaVU5-2xE&t=210s) ​**Roblox的机器学习规模与应用场景**：详细介绍了Roblox平台的巨大规模和各种机器学习应用场景。

    - **平台规模数据**：Roblox拥有1.5亿日活跃用户，峰值并发用户达4500万，机器学习平台需要服务超过100万QPS，在生产环境运行400多个模型。
    - **核心应用领域**：安全审核是首要任务，每天需要实时审核数十亿条跨25种语言的内容；为创作者提供AI助手支持，包括代码辅助和纹理生成；为玩家提供搜索推荐服务。
    - **工程挑战**：在如此大规模下实现稳定的模型训练和服务部署，面临着巨大的工程复杂度挑战。
3. [00:04:30](https://youtu.be/FhZaVU5-2xE&t=270s) ​**Ray开发基础设施的演进历程**：系统阐述了Roblox如何优化Ray作业的开发迭代体验。

    - **初始阶段**：基于KubeRay API服务器构建生产作业基础设施，但每次代码变更都需要重建Docker镜像并重新启动作业，研究人员对此流程极为不满。
    - **代码快照优化**：通过SDK将代码快照保存到S3存储桶，在运行时动态拉取，减少了Docker镜像构建频率，但作业启动仍需等待时间。
    - **持久化集群方案**：在Ray服务器头节点运行代码服务器UI，支持在浏览器中直接迭代代码，配合可缩放到零的GPU工作节点组，实现了真正的快速开发体验。
    - **安全与调试优化**：使用Envoy边车容器实施安全策略，并通过解决mTLS通信导致的性能问题（某些作业减速10倍），优化了集群稳定性。
4. [00:07:06](https://youtu.be/FhZaVU5-2xE&t=426s) ​**Coder集成与本地开发体验**：介绍了如何将Coder云开发环境与Ray集群集成，提供熟悉的本地IDE开发体验。

    - **架构集成**：通过创建Ray集群而非单个Pod，在集群头节点运行Coder代理，建立到开发者笔记本电脑的SSH隧道，支持本地IDE开发。
    - **功能映射**：将Coder的自动停止功能映射到Ray集群的挂起字段，实现集群空闲时自动暂停；配置Coder代理抓取Ray集群状态在UI显示。
    - **依赖管理**：利用Ray与UV的深度集成，确保作业启动时所需的软件包在工作节点上可用。
    - **用户体验**：开发者可以创建工作区、选择H200等GPU类型、配置Docker镜像，在熟悉的开发环境中编码，同时实时查看集群状态。
5. [00:08:44](https://youtu.be/FhZaVU5-2xE&t=524s) ​**Ray Train分布式训练框架的采用与优化**：详细介绍了Roblox如何基于Ray Train构建统一的分布式训练平台。

    - **平台迁移**：自2025年起使用Ray Train替代传统的MPI方法，构建内部SDK简化分布式训练任务的启动过程，支持AWS和本地集群。
    - **环境管理**：提供预配置的Docker镜像，包含网络、CUDA驱动等容器环境，预安装PyTorch和Ray等核心软件包。
    - **计算模板**：通过计算模板概念，让用户无缝选择不同的GPU类型和数量组合，帮助小型训练任务的资源打包优化。
    - **平台特性**：在SDK中提供集群健康检查、异步检查点保存、自动重启等特性，所有训练任务均可通过UI监控利用率和访问日志。
    - **采用成果**：Ray Train作业现已占据主要开发集群近三分之二的GPU小时使用量，成功迁移了所有多节点训练任务。
6. [00:10:55](https://youtu.be/FhZaVU5-2xE&t=655s) ​**3D基础模型训练的数据处理挑战与解决方案**：深入分析了训练3D模型时面临的大规模数据处理问题及两种解决方案。

    - **数据规模挑战**：典型的3D对象原始文件可达70MB，数据集包含数千万个文件，直接使用原始文件训练不现实，需要大量预处理。
    - **离线预处理方案V1**：使用Ray Data构建标准流水线，从S3下载原始文件、渲染3D对象、随机采样数百个点，然后保存回S3，但初始运行速度较慢。
    - **离线预处理优化V2**：直接使用Ray Core框架特性，将流水线分解为组件，重叠网络密集型和处理密集型操作，使用s5cmd最大化网络吞吐量，Python多线程实现最大CPU利用率。
    - **资源调度优化**：使用Ray的临时资源特性控制和平衡各节点工作负载，实现最佳重叠效果，最终将数据处理时间减少75%，资源利用率显著提升。
7. [00:13:59](https://youtu.be/FhZaVU5-2xE&t=839s) ​**在线流式数据处理架构**：介绍了为解决离线预处理局限性而构建的在线数据处理方案。

    - **方案动机**：研究人员想要尝试新的采样算法时，需要修改预处理逻辑并从头运行整个流水线，消耗大量计算和存储资源。
    - **架构设计**：在GPU训练的同时对原始数据集进行实时预处理，需要大量网络带宽和CPU计算能力，仅靠GPU节点资源不足。
    - **解决方案**：向集群添加更多CPU节点，将主要数据加载任务卸载到这些节点，使用Ray Core框架直接构建解决方案。
    - **技术实现**：利用Ray轻松设置包含GPU训练节点和CPU数据加载节点的异构集群，通过Ray Core API让GPU节点在远程CPU节点上启动异步数据流和预处理任务。
    - **数据传输优化**：数据准备就绪后通过Ray对象存储高效传输回GPU节点，使用放置组和临时资源实现更好的资源调度和工作负载放置控制。
    - **扩展性优势**：通过简单添加更多CPU节点即可水平扩展数据加载能力，完美解决了处理更大原始数据集时的网络和CPU挑战。
8. [00:16:26](https://youtu.be/FhZaVU5-2xE&t=986s) ​**混合云计算基础设施架构**：详细介绍了支撑整个平台的底层计算基础设施设计。

    - **核心挑战**：GPU可用性是在Roblox进行AI和机器学习工作负载的持续瓶颈，依赖单一云提供商在成本、区域中断和供应商锁定方面存在风险。
    - **双策略方案**：动态调度跨AWS和本地环境的工作负载，根据需求向云爆发获取机器容量或使用自有硬件实现成本效益，同时向研究人员抽象此复杂性。
    - **架构组成**：用户向中央控制平面提交Ray作业，管理集群运行联邦作业调度器MUT，智能将作业路由到弹性AWS EKS工作集群和本地高性能GPU工作组。
    - **资源管理**：每个工作组使用集群队列管理团队特定的CPU和GPU配额，MUT根据作业需求评估各队列可用容量，基于配置将作业路由到最佳可用位置。
    - **系统优势**：通过跨云和本地环境聚合资源，既增加了GPU访问能力，又提高了系统可靠性，不依赖单一云提供商。
9. [00:19:16](https://youtu.be/FhZaVU5-2xE&t=1156s) ​**GPU调度挑战与队列架构重构**：分析了大规模GPU调度面临的问题及相应的架构改进。

    - **调度问题**：团队需要特定GPU类型的配额，GPU使用模式极其尖峰，团队可能要求立即访问大量GPU但实际使用时间很短，导致资源碎片化。
    - **架构演进**：从团队直接拥有物理节点转变为"配额即代码"模式，资源配额在Git中集中管理，团队拥有逻辑资源配额而非特定物理机器。
    - **用户体验优化**：用户不再需要请求特定GPU型号，而是在Ray作业中指定GPU类型和层级范围，如第0层可抢占第1层资源。
    - **系统集成**：通过队列原生集成，Ray作业工作负载使用标签指定目标本地队列，队列使用资源风格和工作负载优先级类来表示用户可用的GPU层级。
    - **自动化管理**：系统自动注入节点亲和性规则并管理支撑Ray作业的Pod的配额，简化了用户的资源申请流程。
10. [00:21:36](https://youtu.be/FhZaVU5-2xE&t=1296s) ​**扩展速度优化与性能提升**：详细介绍了解决Pod启动速度瓶颈的各种优化措施。

     - **性能瓶颈分析**：Ray Pod扩展缓慢的根本原因是容器镜像拉取，这在节点上创建了显著的磁盘IO瓶颈，并使中央Docker注册表过载。
     - **问题影响**：每个新Pod尝试拉取相同的多GB大镜像，导致注册表流量和本地磁盘IO激增，获取特定实例类型的可靠访问也严重延迟了Pod放置能力。
     - **优化前状态**：平均Pod启动时间超过8分钟，有些甚至达到近17分钟，这对期望作业快速启动的用户来说不可接受。
     - **优化措施**：基于基准测试优化EBS卷以获得更高磁盘吞吐量和更低延迟；实施DaemonSet预拉取常见容器镜像；为Ray作业镜像引入点对点镜像分发。
     - **技术实现**：使用AWSC支持AWS集群，允许容器在完整镜像下载完成前启动，实现懒加载和更快启动；Pod之间共享公共层，大幅减少拉取时间和注册表负载。
     - **优化成果**：Pod启动时间从17分钟急剧下降到略超过2分钟，为用户提供了接近实时的作业启动体验。

## **Scaling Multimodal Data Curation with Ray and LanceDB | Ray Summit 2025**

[https://www.youtube.com/watch?v=1hBesu2Erg0](https://www.youtube.com/watch?v=1hBesu2Erg0)

视频介绍: 本视频由Netflix机器学习工程师Pablo和Lance DB联合创始人Lei共同呈现，深入探讨了大规模多模态数据集构建中的数据管理、批量推理和存储解决方案。演讲详细介绍了从原始数据到高质量训练数据集的完整数据管理流程，以及如何利用Ray框架和Lance DB技术栈实现高效的数据处理。

结论: 构建大规模多模态数据集需要模型管理、高效批量推理和智能存储系统的紧密结合。通过采用Ray的批量推理机制和Lance DB的零拷贝数据演化技术，团队能够有效处理数亿级别的图像和视频数据，实现数据集的持续优化和版本管理。这种技术组合为现代AI训练提供了可扩展、高效的数据基础设施解决方案，特别适用于需要频繁更新和迭代的大规模多模态数据集场景。

关键点:

1. [00:00:44](https://youtu.be/1hBesu2Erg0&t=44s) ​**数据管理流程与挑战**：演讲者详细阐述了构建大规模多模态数据集所需的完整数据处理流程。

    - 数据管理流程包括多个阶段：从原始数据收集开始，经过提取（如视频分解为帧）、过滤（使用机器学习模型评估质量）、标注（生成文本描述）到最终的数据分布平衡。
    - 质量分级机制：通过多轮过滤可获得不同质量等级的数据集，包括低分辨率训练集、中等分辨率数据集和用于监督微调的高质量数据集。
    - 迭代优化过程：每个阶段都需要反复查询、调查和过滤数据，通过不断调整过滤机制来获得最终适合训练的数据池。
    - 规模挑战：处理的数据集规模可达1亿张图像或10亿个视频，需要强大的计算资源和高效的存储管理系统。
2. [00:04:18](https://youtu.be/1hBesu2Erg0&t=258s) ​**数据管理中的模型应用**：详细介绍了在数据管理过程中使用的各类模型及其功能。

    - 美学评分模型：用于评估图像的美学质量，区分高质量和低质量内容。
    - 视觉语言模型：能够对图像进行详细标注，通过不同的问题提示提取多样化的文本信息。
    - 嵌入模型：包括CLIP、DINO等模型，可将图像转换为语义表示向量，便于相似性搜索和聚类分析。
    - 多模态模型：如CLIP核心模型，能够同时处理图像和文本输入，通过余弦相似度评估图文对齐程度。
    - 视频处理技术：将视频分解为帧序列，通过帧级处理获得视频的整体表示，或使用支持多图像输入的视觉语言模型生成视频标注。
3. [00:11:22](https://youtu.be/1hBesu2Erg0&t=682s) ​**批量推理技术对比**：深入分析了不同批量推理方法的优缺点和适用场景。

    - 在线推理模式：基于请求-响应机制，适合使用闭源模型API的场景，但存在协调多个模型时的复杂性问题和中间状态管理挑战。
    - 批量同步并行：在Spark等框架中常见，模型与数据读取在同一机器上，但会产生大量中间数据副本，且需要等待前一阶段完成后才能开始下一阶段。
    - Ray批量推理优化：利用共享内存和指针交换机制，通过Actor Pool构造实现高效的数据处理流水线，支持动态调整并发度和弹性扩展。
    - 模型链式处理：通过映射操作将多个模型连接成处理流水线，每个阶段可以独立调整工作节点数量以适应不同的处理需求。
4. [00:16:34](https://youtu.be/1hBesu2Erg0&t=994s) ​**Lance DB存储解决方案**：重点介绍了专为多模态数据设计的Lance DB存储系统的核心特性。

    - 零拷贝数据版本化：采用列式存储格式，支持原子性的模式修改，每次添加新列时只存储增量变化而非复制整个数据集。
    - 二维存储布局：支持同时添加列和行的数据增长模式，通过片段（fragments）逻辑分区管理数据，实现高效的数据演化。
    - 高性能随机访问：支持按行ID精确获取单行数据，无需读取整个行组，提供快速的单点查询能力。
    - 内置索引支持：原生支持向量搜索、全文搜索和标量索引，所有索引都在数据存储的同一位置构建，无需额外的系统迁移。
    - 版本管理机制：通过清单文件原子性地记录所有数据变更，采用仅追加方式跟踪数据集的演进历史。
5. [00:24:09](https://youtu.be/1hBesu2Erg0&t=1449s) ​**新一代数据格式设计理念**：阐述了Lance DB针对现代AI工作负载的优化设计。

    - 工作负载特性分析：传统OLAP系统优化大规模扫描和聚合操作，而AI工作负载更注重快速随机访问和Top-K查询。
    - 统一数据湖理念：旨在构建支持检索、评估、分析和训练等多种工作负载的统一存储层，替代多个专用系统的复杂架构。
    - 核心优化特性：提供恒定时间的单单元格访问能力，结合二维存储布局支持零拷贝数据演化。
    - 大对象存储优化：鼓励在系统中存储大型二进制对象，通过Blob API实现高效的范围读取，无需全量扫描。
    - 生态系统集成：与Ray深度集成，提供分布式索引、数据压缩和大规模特征工程能力，支持Python UDF注册和作业编排。

## **Scaling Multi-Modal Datasets to Petabytes with Ray at Apple | Ray Summit 2025**

[https://www.youtube.com/watch?v=w4N0JmJo6jQ](https://www.youtube.com/watch?v=w4N0JmJo6jQ)

视频介绍：本视频由技术团队分享他们如何利用Ray框架构建大规模多模态数据处理平台的经验。演讲者详细介绍了平台架构设计、性能优化策略以及在35,000个CPU核心上处理PB级数据时遇到的实际挑战和解决方案。

结论：通过构建基于Ray的统一数据处理平台，团队成功解决了多模态数据处理的规模化难题，实现了8.7倍的性价比提升。该平台通过强类型数据框架API、端到端流式执行、双重执行路径等创新设计，在保持研究人员生产效率的同时实现了极致的扩展性。未来将重点投入数据血缘追踪、增量处理和自服务调试工具，以进一步提升平台的可靠性和易用性。

关键点：

1. [00:02:35](https://youtu.be/w4N0JmJo6jQ&t=155s) **平台设计核心需求与架构选择**

    - 多模态数据支持：框架必须将图像、张量、音频等复杂数据类型视为一等公民，而不仅仅是不透明的数据块。这要求系统能够理解不同数据类型的语义，从而进行针对性的优化。
    - 极致扩展性：系统需要能够管理PB级数据集，扩展到至少10万个CPU核心，并能有效利用GPU资源。这涉及到底层分布式计算引擎的选择和资源调度策略的设计。
    - 敏捷迭代能力：研发周期必须足够快速，从本地测试到生产级运行的过渡应该在数天内完成，不能因为规模化需求而牺牲迭代速度。
    - Python优先：需要与现有ML生态系统（如PyTorch、NumPy）无缝集成，提供熟悉的API，避免研究人员放弃现有工具学习新的领域特定语言。
2. [00:07:27](https://youtu.be/w4N0JmJo6jQ&t=447s) **双重执行路径设计**

    - 实验路径：提供对临时、低优先级且可抢占资源的即时访问，优化访问速度和成本，促进在中等规模数据集上的快速迭代，由热计算节点池支持。
    - 生产路径：要求GCP探测用户脚本和查询计划以评估计算复杂性和资源需求，然后基于特定作业配置专用的高优先级Ray集群，确保稳定性。
    - 解耦优势：这种设计允许平台智能管理基础设施，而无需向用户暴露复杂性，特别适合大多数不熟悉分布式系统和基础设施的机器学习工程师和数据科学家使用。
3. [00:10:42](https://youtu.be/w4N0JmJo6jQ&t=642s) **性能优化策略**

    - 强类型操作符：为常见的多模态任务（如音频解码、视频解码、图像调整大小等）使用内置的高度优化操作符。由于系统是强类型的，查询优化器了解数据的语义，能够进行算子融合、谓词下推等激进优化。
    - 类型感知混洗：多模态数据通常涉及在同步屏障上移动大量大型二进制对象，经常超出内存容量。由于引擎理解被混洗的类型，可以采用更高效的序列化和传输机制。
    - 灵活扩展：研究人员仍可定义复杂的UDF或优化操作符，平台将管理所有任意依赖关系，这种混合方法既提供了专用引擎的性能，又保持了Python生态系统的灵活性。
4. [00:13:26](https://youtu.be/w4N0JmJo6jQ&t=806s) **端到端流式执行架构**

    - 批处理级流水线：一旦批次被处理，结果立即推送到下一阶段，直到达到必要的同步屏障。这与传统的批处理同步系统形成对比，后者通常尝试在下一阶段开始前物化整个中间阶段或分区。
    - 解决数据爆炸问题：多模态数据管道经常具有巨大的扇出效应，例如100字节的URL数据可能变成100GB的视频，最终生成数千个剪辑。真正的流式处理从根本上改变了这种情况，不再需要物化整个中间阶段。
    - 性能收益：通过批处理级粒度的流式处理，可以重叠IO密集型下载、CPU密集型解码和GPU密集型推理，直接在内存中处理批次而无需暂存到磁盘，实现了约2倍的性能提升。
5. [00:16:57](https://youtu.be/w4N0JmJo6jQ&t=1017s) **容错与扩展机制**

    - 幂等性和检查点：在分区级别实现幂等写入和检查点，确保发生故障时从最后一个成功分区恢复，而不是重新启动作业。这对于在35,000个CPU上运行超过一周的作业至关重要。
    - 模块化扩展API：除了提供内置操作符外，还暴露插件API，允许专业团队表达自己的优化和编译操作符内核，根据他们的需求进一步推动性能。
6. [00:19:26](https://youtu.be/w4N0JmJo6jQ&t=1166s) **大规模运营的实际挑战**

    - 全栈可扩展性问题：在PB级规模下，在整个技术栈中都会遇到瓶颈，包括数据处理的并发控制、认证授权、远程存储访问、日志量和数千个并发工作器产生的网络流量。
    - Ray核心运行时瓶颈：在35,000个CPU、数百个节点扩展到数千个节点的Kubernetes集群中，开始看到磁盘IOPS饱和和Ray GCS进程中的线程IO争用。
    - 基础设施压力：遇到Kubernetes CNI插件限制和KubeRay健康检查开销等问题，这些问题导致长时间运行的大型计算作业不可靠，需要逐一排查和修复。
7. [00:21:21](https://youtu.be/w4N0JmJo6jQ&t=1281s) **集群生命周期管理优化**

    - 自定义设置脚本问题：用户提供的自定义设置脚本（如pip安装或数据预加载）在规模上难以维护，这些脚本可能不稳定、未经充分测试，可能因网络超时而挂起，导致初始化失败难以调试。
    - 内部编排架构：通过构建围绕KubeRay的编排层，将环境设置提升为平台的一等公民，能够定制和推广内部概念，如标准化环境配置、安全补丁和工作负载探测，而无需维护硬分叉。
    - 收益：保持与上游KubeRay操作符同步的同时，根据特定生产需求定制集群生命周期管理，显著改善了设置脚本的可观测性和故障诊断能力。

## **RDMA P2P Deep Dive: KvCache Transfer, Weight Updates &amp; MoE Routing at Perplexity | Ray Summit 2025**

[https://www.youtube.com/watch?v=Nl8iqYuEHhc](https://www.youtube.com/watch?v=Nl8iqYuEHhc)

视频介绍：本视频由Propensity AI的Lein分享，深入探讨了在大语言模型系统中使用RDMA点对点通信的技术，重点讲解了权重传输应用，并简要介绍了KV缓存传输和混合专家模型的通信优化。

结论：RDMA点对点通信技术为大语言模型系统中的动态通信模式提供了高效解决方案。相比传统的集合通信，RDMA点对点通信具有成员关系灵活、初始化简单、支持非均匀张量形状和无需强顺序保证等优势。通过权重传输、KV缓存传输和混合专家模型三个具体应用案例的实践，证明了该技术能显著提升通信效率，实现1.3秒内传输超大模型权重，并在AWS等云平台上实现万亿参数模型的运行。未来这一技术将继续推动大语言模型系统在多云环境下的可扩展性和性能优化。

关键点：

1. [00:01:04](https://youtu.be/Nl8iqYuEHhc&t=64s) **点对点通信模式的兴起与集合通信的局限性**

    - 新兴的点对点通信模式包括解耦推理中的KV缓存传输、强化学习后训练中的权重传输以及混合专家模型中的动态专家选择。这些场景下，传统的集合通信API显得不够灵活。
    - 集合通信的四个主要局限性：

      - 固定成员关系：通信世界的初始化需要所有节点同时参与，难以应对动态变化的集群规模。
      - 初始化成本高：创建点对点子组需要n²次通信轮次，初始化开销巨大。
      - 张量形状限制：要求发送方和接收方使用相同的张量形状和数据类型，对于动态负载场景会造成内存和带宽浪费。
      - 强顺序保证：集合通信强制要求操作顺序，而许多应用（如权重更新）并不需要严格的顺序保证，这会带来额外的缓冲和同步开销。
2. [00:06:33](https://youtu.be/Nl8iqYuEHhc&t=393s) **RDMA技术基础与核心原语**

    - RDMA技术优势：

      - 高性能网络：当前代支持400Gbps带宽，下一代将更快。
      - 内核旁路：用户空间应用直接与网卡通信，无需内核缓冲。
      - 本地PCIe交换：GPU到网卡流量保持在本地PCIe交换机内，避免系统总线竞争。
    - 核心RDMA操作：

      - 双边操作：send/receive，远程CPU会被唤醒，适用于控制消息传输。
      - 单边操作：write/write with immediate，远程节点无需感知内存修改，适用于大数据量传输。
      - 其他操作：read操作性能较差，原子操作效率不高。
    - 应用构建优势：无需构建通信世界、支持零拷贝传输、不限制张量形状和类型。
3. [00:12:05](https://youtu.be/Nl8iqYuEHhc&t=725s) **权重传输的具体实现与优化**

    - 应用场景：强化学习微调中的训练工作器到推理工作器的权重同步。
    - 性能提升：从传统方法的10-100秒提升到1.3秒完成超大模型权重传输。
    - 实现步骤：

      - 元数据收集：收集训练和推理工作器的参数元数据。
      - 调度计算：通过简单贪心算法计算权重传输调度。
      - 流水线执行：将权重传输构建为多阶段流水线。
    - 流水线阶段：

      - GPU内存加载：从CPU内存加载模型权重到GPU内存。
      - 全张量收集：通过all-gather操作收集分片参数。
      - 数据处理：进行投影融合和FP8量化等GPU操作。
      - RDMA传输：通过单边write操作直接传输到目标GPU。
    - 性能优势来源：

      - 全集群带宽利用：避免rank-zero瓶颈，充分利用所有网卡带宽。
      - 无协调开销：推理引擎无需参与传输过程。
      - 流水线重叠：不同硬件资源上的操作充分重叠。
4. [00:18:55](https://youtu.be/Nl8iqYuEHhc&t=1135s) **KV缓存传输与混合专家模型的通信优化**

    - 解耦推理中的KV缓存传输：

      - 动态扩展：支持随时添加新的预填充副本，无需重新构建通信世界。
      - 层间传输重叠：支持逐层传输，重叠计算时间和网络传输时间。
      - CUDA图支持：与预填充阶段的CUDA图兼容。
    - 控制消息设计：

      - 使用双边send/receive传输控制消息（开始预填充、取消等）。
      - 通过write with immediate的立即数计数替代完成消息，减少控制开销。
    - 混合专家模型通信：

      - 跨平台兼容：在AWS EFA等非Nvidia平台上实现高性能通信。
      - 性能对比：在CX7网卡上甚至超过基于IBGDA的DP实现。
      - 延迟优化：CPU到GPU的PCIe延迟仅2微秒，对整体性能影响可接受。
5. [00:24:59](https://youtu.be/Nl8iqYuEHhc&t=1499s) **性能成果与实际应用价值**

    - 解码性能：在128 tokens/rank的配置下，DeepSeek-V3架构上表现优异。

      - 在CX7网卡上超过DP实现，即使不使用IBGDA技术。
      - 在EFA上实现合理的延迟性能。
    - 预填充性能：目前略慢于DP，但通过优化数据传输量可进一步提升。
    - 实际应用价值：

      - 支持在AWS等云平台上运行万亿参数大模型。
      - 多节点配置可同时降低延迟和成本。
      - 具体案例：DeepSeek-V3模型在32GPU配置下实现最优延迟-成本权衡；KimK2万亿参数模型必须依赖多节点通信才能运行。
    - 开源贡献：相关代码已开源，研究论文已在arXiv发布，推动社区技术发展。

## **How Latitude AI Trains Perception Models at Massive Scale | Ray Summit 2025**

[https://www.youtube.com/watch?v=YoO5t7Lpl74](https://www.youtube.com/watch?v=YoO5t7Lpl74)

视频介绍: 本视频由福特汽车旗下自动驾驶技术公司Latitude AI的工程师团队在Ray Summit上分享，详细介绍了他们如何利用Ray框架优化感知机器学习训练数据流水线，实现了9倍于原始PyTorch数据加载器的性能提升。

结论: Latitude AI通过系统性地优化基于Ray的数据流水线，成功解决了自动驾驶感知模型训练中的数据瓶颈问题。团队从初始的PyTorch数据加载器迁移到Ray Data，通过采样策略重构、流水线阶段合并、序列化优化、预取机制改进等多维度优化，最终实现了训练速度的大幅提升和资源使用效率的显著改善。这些优化不仅解决了内存溢出、训练不稳定等实际问题，还为团队提供了更好的可观测性和调优灵活性，展现了Ray在复杂机器学习工作负载中的强大能力。

关键点:

1. [00:02:11](https://youtu.be/YoO5t7Lpl74&t=131s) **训练数据瓶颈的挑战与优化成果**

    - 自动驾驶感知模型的特殊性：多传感器同步数据（激光雷达、摄像头、雷达）导致每个样本数据量达到GB级别，同时需要丰富的时空元数据（立方体、姿态、校准等）
    - 数据瓶颈的本质：在使用H200等大型GPU训练时，数据加载和预处理成为主要瓶颈，而非模型的前向/反向传播计算
    - 优化成果展示：相比初始PyTorch数据加载器实现9倍加速，相比初始Ray Data实现实现3倍加速且额外CPU需求减少4倍
    - 初始问题诊断：GPU利用率低下，训练节点CPU完全饱和，增加PyTorch数据加载器工作线程数无法提升吞吐量
2. [00:05:33](https://youtu.be/YoO5t7Lpl74&t=333s) **数据流水线架构与瓶颈转移现象**

    - 完整流水线流程：从Parquet元数据文件开始 → 加载传感器数据 → 数据增强和转换 → 预取到GPU节点 → 批处理和整理 → 传输到GPU内存 → 训练
    - 流水线划分：前半部分为"数据流水线"，后半部分从预取开始为"训练循环"
    - 瓶颈动态转移：优化数据流水线后，训练循环成为新瓶颈，需要交替优化两个部分
    - 初始困境：团队通过增加GPU数量和并行度来解决问题，导致GPU集群利用率低下和长队列等待时间
3. [00:07:41](https://youtu.be/YoO5t7Lpl74&t=461s) **Ray Data初始实现与采样策略演进**

    - 初始实现的局限性：采用预采样方式，需要为不同权重配置写出TB级的预采样文件，不符合ML开发者的工作流程
    - 采样策略创新：在训练开始时计算一次采样统计信息，生成行索引的Ray数据集来指示数据加载顺序
    - 流水线重构：添加额外的map_batches步骤来从索引数据集加载行，大幅提升团队使用的友好性
    - 持续存在的问题：内存溢出、磁盘溢出、训练速度不一致等稳定性问题
4. [00:11:45](https://youtu.be/YoO5t7Lpl74&t=705s) **流水线阶段合并与内存优化突破**

    - 问题根源分析：发现基于DuckDB的采样器将所有Parquet文件保存在内存中且存在内存泄漏
    - 关键技术改进：切换到纯Arrow-based采样器并显式清理生成的表，解决了内存问题
    - 阶段合并效益：将两个map_batch阶段合并为一个，减少50%的actor内存使用，显著降低对象存储内存占用
    - 性能提升：允许大幅增加并发度，设置更低的对象存储限制，释放更多集群资源，流水线稳定性和吞吐量一致性显著改善
5. [00:15:13](https://youtu.be/YoO5t7Lpl74&t=913s) **训练步骤优化与预取机制改进**

    - 性能分析发现：40%的CPU时间花费在预取和整理操作上
    - 架构改进：将预取和整理移动到专用的预取actor，通过对象存储在预取actor和主训练actor之间传输批次
    - 异步优化：在单独线程中执行非阻塞的主机到设备拷贝，避免影响模型前向/反向传播
    - 效果验证：CPU利用率从60%提升到85%，瓶颈重新回到数据流水线
6. [00:16:21](https://youtu.be/YoO5t7Lpl74&t=981s) **传感器数据加载与文件IO优化**

    - IO瓶颈分析：数据流水线严重受限于文件IO，传感器数据加载本身密集，多个工作线程通过元数据存储路由获取单个传感器数据工件地址
    - 创新解决方案：将所有单个传感器数据工件压缩到单个存档中，并缓存存档地址
    - 性能影响：正常情况实现2倍加速，元数据存储拥塞时实现高达5倍加速
    - 数据减少策略：将确定性转换上游化，将这些转换的输出具体化为版本化工件，减少66%的文件IO和actor内存使用
7. [00:19:34](https://youtu.be/YoO5t7Lpl74&t=1174s) **序列化优化与开发者体验提升**

    - 根本矛盾：开发者偏好使用数据类，而Ray Data期望使用numpy数组以实现零拷贝传输
    - 多模态数据挑战：每个样本的形状不一致会破坏Ray Data的批处理要求
    - 序列化解决方案：将数组类属性提取到numpy数组中，为形状设置防护栏，将剩余数据类字段序列化为字节
    - 训练端处理：在训练节点将数组字典解包到批次数据类中并转发到模型
    - 开发者收益：能够无问题地使用数据类，添加非数组元数据类型而无需担心序列化问题
8. [00:21:48](https://youtu.be/YoO5t7Lpl74&t=1308s) **高级优化技巧与最佳实践总结**

    - 数据类型设置：在数据流水线中设置正确的数据类型对减少内存利用率和网络传输成本至关重要
    - Ray调度调优：Ray默认为每个actor调度4个任务，但不均匀分配块，设置适当的分区数确保工作负载均匀分布
    - 可观测性实践：记录Ray Data进度条并在多个实验中锁定，相比Grafana和Prometheus更易于跟踪优化进展
    - 最终比较：优化版本相比Torch数据加载器减少一半GPU使用，速度提升9倍；相比初始Ray Data版本减少一半GPU使用和4倍CPU使用，速度提升3倍

## **Scaling User-Focused Foundation Models at Grab with Ray | Ray Summit 2025**

[https://www.youtube.com/watch?v=DRtha-uVgpY](https://www.youtube.com/watch?v=DRtha-uVgpY)

视频介绍：本视频由Grab AI与实验平台团队的Chong Yu和Nick分享，详细介绍了他们在构建用户中心化基础模型方面的实践经验。作为东南亚领先的超级应用，Grab拥有涵盖出行、外卖、购物、金融等多元业务的丰富数据，这为构建能够深入理解用户行为的基础模型提供了独特机会。

结论：Grab通过创新的适配器中心化架构成功构建了用户中心化基础模型，该模型能够从表格数据和时序数据中联合学习用户表征。关键技术突破包括多模态令牌化、稀疏优化训练和异构集群部署，使得模型在多个下游任务中展现出优于传统方法的性能。未来发展方向包括改进预训练目标、扩展适配器能力和优化非均匀时序编码，为实现更精准的用户理解和个性化服务奠定基础。

关键点：

1. [00:04:55](https://youtu.be/DRtha-uVgpY?t=295) ​**构建基础模型的动机与传统方法的局限性**：

    - 传统方法包括人工创建用户属性、构建专用模型和知识图谱，这些方法虽然有效但存在明显局限。人工属性创建依赖分析团队总结用户特征，过程耗时且难以规模化；专用模型针对单个用例定制，缺乏通用性；知识图谱虽能表示信息关联，但构建和维护成本高昂。
    - 团队提出的核心问题是：能否构建一个直接从数据学习的基础模型，替代持续改进用户属性和专用模型的传统路径？这一思考促使他们探索基础模型的新范式，目标是提取市场所有实体的嵌入表示，包括用户、司机、商家和市场环境，这些嵌入可广泛应用于各种下游任务。
    - 基础模型的第二个目标是直接为下游任务提供模型服务，下游团队可以基于具体用例数据进行微调，这有助于解决数据稀疏性和冷启动问题。目前项目处于第一阶段，主要聚焦嵌入用例，但已在不同垂直领域的多个用例中展现出 promising 的结果。
2. [00:07:19](https://youtu.be/DRtha-uVgpY?t=439) ​**多模态令牌化与适配器设计的创新**：

    - 令牌化的核心挑战是如何定义"用户语言"，包括如何捕捉用户属性、用户交互等复杂信息。初始尝试采用朴素令牌化方法，将表格中的每个单元格（列名+值）视为一个令牌，时序数据中的日期和动作类型也作为独立令牌。
    - 朴素方法面临两个主要问题：词汇表爆炸式增长，因为每个唯一ID都成为独立令牌；信息利用不充分，相同实体在不同上下文中的关联性无法保证。例如"订购商家ID45"和"浏览商家ID45"应该共享商家信息，但传统令牌化无法实现这一点。
    - 创新解决方案是将令牌视为键值对，其中键提供事件上下文，值是事件的具体数值。这种设计能同时适用于表格数据（列名-值组合）和时序数据（动作类型-值组合）。为进一步解决词汇表问题，团队引入模态视角，将不同类型数据视为不同模态，包括文本模态、分类值模态、数值模态等。
    - 适配器设计的精髓在于其可替换性。每个模态都有专门的适配器，部分利用预训练基础模型，其他采用自定义设计。这种架构的优越性在于当Grab出现新的数据类型时，可以简单引入新适配器来扩展基础模型能力。以商家适配器为例，当前版本结合预训练文本嵌入模型和嵌入层，未来版本计划整合商家更多属性信息，有效应对冷启动问题。
3. [00:12:56](https://youtu.be/DRtha-uVgpY?t=776) ​**模型架构设计与训练优化策略**：

    - 模型需要同时处理表格数据和时序数据，这两类数据有截然不同的要求。表格数据需要列顺序不变性，改变列顺序不应影响预测结果；而时序数据中动作的顺序至关重要。为解决这一矛盾，模型对表格数据使用可学习令牌，对时序数据使用包含绝对时间和相对时间的位置嵌入。
    - 预训练目标采用掩码语言建模和下一令牌预测的组合。由于输入数据是多模态的，需要为不同模态设计不同的输出头：用户ID使用分类或相似度损失，数值数据使用均方误差损失等。这种设计避免了监督学习可能导致的模型偏向特定任务的问题。
    - 大规模嵌入层的内存优化是关键挑战。Grab每月有约4000万活跃用户，仅嵌入层就需要数百GB的GPU内存。团队采用两种优化策略：使用稀疏原子优化器近似完整原子，重复利用内存；将嵌入层和优化器状态卸载到CPU，完全释放GPU容量用于主要模型训练。
    - 分类层优化经历了两个阶段：初始使用分层分类减少计算量，但仍需大量内存；改进版本使用BYOL风格的目标网络，通过指数移动平均更新，显著减少目标网络规模和计算量，同时支持CPU卸载进一步释放GPU内存。
4. [00:17:14](https://youtu.be/DRtha-uVgpY?t=1034) ​**嵌入提取与实验迭代的重要性**：

    - 模型价值提取通过两种类型的嵌入实现：长期用户嵌入来自ID层，模型通过历史数据训练学习用户的长期行为模式；短期用户交互嵌入通过最近时序数据经模型处理后的序列聚合得到。这种双嵌入体系为下游任务提供了全面的用户表征。
    - 成功并非一蹴而就，团队经历了大量的实验迭代。初期效果不佳，通过持续优化数据集构建、事件重要性分析、去重策略、数据表示方法和适配器设计逐步提升性能。模型设计方面不断改进头部设计、分层分类、目标网络、时间编码和主要Transformer架构。
    - 快速实验迭代依赖于两个关键因素：训练速度和实现速度。在使用Ray之前，团队面临多重迭代周期瓶颈，包括需要创建Spark作业、定义预处理逻辑、管理存储路径等大量手动工作。训练本身也受限于单节点训练，只能垂直扩展，迭代周期非常缓慢。
    - 性能瓶颈分析发现GPU利用率仅19%，主要原因是CPU资源竞争。由于将大量优化操作卸载到CPU，同时数据加载也是CPU密集型，导致模型训练和数据加载在同一个节点上争夺有限CPU资源。通过构建异构集群，将最后阶段数据预处理分离，训练时间减少了6倍，GPU利用率提升到85%。
5. [00:22:43](https://youtu.be/DRtha-uVgpY?t=1363) ​**未来发展方向与技术挑战**：

    - 典型的基础模型发展路径是规模扩展，团队在模型和数据缩放方面都看到了良好的实验结果。另一个重要方向是改进预训练目标本身，当前使用的掩码语言建模和下一令牌预测主要关注单个令牌，但嵌入应用需要总结整个用户旅程，存在目标不匹配问题。
    - 商家适配器V2的开发代表了对丰富上下文信息的深入利用，包括整合商品信息、营业时间、位置数据等多参数事件。这种改进能更好地理解商家特征，为新商家或低活跃度商家提供更准确的表征。
    - 非均匀时序编码是另一个重要挑战。用户行为的时间间隔变化很大，有些动作间隔数天，有些在几秒内发生。团队正在研究如何更好地处理这种时间异质性，使模型能更精确地捕捉用户行为的时间模式。
    - 团队还致力于提升嵌入的可解释性分析，虽然当前主要通过下游任务的性能提升来证明嵌入的有效性，但深入理解嵌入内部表征的工作也在进行中。这有助于更好地向用户证明模型价值，并指导后续的模型优化方向。
6. [00:25:36](https://youtu.be/DRtha-uVgpY?t=1536) ​**实际应用与业务价值体现**：

    - 当前主要应用模式是作为传统推荐模型的补充输入，下游团队可以将基础模型生成的嵌入作为额外特征集成到现有架构中。这种方式的优势在于侵入性较小，团队可以继续使用他们熟悉的模型架构，同时获得用户理解的提升。
    - 团队同时探索用基础模型完全替代传统架构的可能性，认为未来的趋势在于时序学习而非传统的宽深架构。这种转变需要下游团队的接受和配合，目前已有一些探索性工作在进行中。
    - 训练数据主要基于Grab内部业务数据，目的是学习特定的业务上下文。对于通用知识部分，如菜品描述、菜系分类等，团队使用公开可用的文本嵌入来编码这些通用信息，但不进行反向传播更新这些预训练模型。
    - 从实验失败到成功的核心经验表明，最大的性能提升来自V0到V1版本的嵌入提取方法改进。其他优化包括预训练过程的损失函数收敛性、数据事件的完备性等，都需要通过持续的假设验证和迭代实验来完善。

## **How Autodesk Built a Next-Gen Deep Learning Platform with Ray | Ray Summit 2025**

[https://www.youtube.com/watch?v=akI12Sr82jM](https://www.youtube.com/watch?v=akI12Sr82jM)

视频介绍：本视频由Autodesk Research的机器学习工程经理Guo Del Castillo和首席机器学习工程师Kamal Rahim Malik Shang共同分享，详细介绍了他们如何基于Ray框架构建内部深度学习平台Ray Lab，以解决大规模模型训练、数据流水线和计算瓶颈等挑战。

结论：Autodesk通过构建基于Kubernetes的Ray Lab平台，成功将分散的GPU作业转变为统一的生产级研究平台，支持跨团队和跨区域的大规模深度学习工作负载。该平台通过简化的集群管理、统一的训练API、智能作业调度和全面的监控体系，在保持研究敏捷性的同时显著提升了资源利用效率和团队协作能力。未来的工作重点包括多租户公平性、自动扩缩容和更高效的资源管理策略，为工业级AI研究提供了可扩展的基础设施解决方案。

关键点：

1. [00:01:20](https://youtu.be/akI12Sr82jM?t=80s) ​**大规模深度学习的核心挑战**：演讲者详细阐述了在构建Ray Lab平台前Autodesk面临的主要技术瓶颈。

    - **数据规模挑战**：处理海量非结构化数据是首要难题。Autodesk的一个典型项目需要处理300TB的原始数据，经过去重和格式转换后生成100TB的训练数据，涵盖3D对象、文本和图像等多种格式。这种数据规模对存储、传输和处理都提出了极高要求。
    - **计算资源瓶颈**：深度学习工作负载需要大量的并行计算和专用硬件支持。团队不仅需要跨多个实例进行并行处理，还必须使用GPU等特殊硬件，这导致了复杂的作业编排需求和基础设施专业知识门槛。
    - **迭代效率低下**：现有解决方案存在作业启动时间长、部署周期慢的问题，严重拖慢了实验迭代速度。同时缺乏中心化的可见性使得调试变得困难，昂贵的GPU资源经常处于空闲状态，造成资源浪费。
2. [00:05:14](https://youtu.be/akI12Sr82jM?t=314s) ​**Ray框架的核心价值与局限性**：深入分析了选择Ray作为基础框架的原因及其在Autodesk环境中的适用性问题。

    - **Ray的技术优势**：Ray作为一个通用的Python分布式框架，提供了从数据处理到训练推理的端到端机器学习支持。其交互式开发环境、实时日志和实时仪表板使得快速迭代和调试变得简单。Ray还提供对计算资源扩展和任务调度的细粒度控制，并且是云无关的，可以部署在任何云基础设施上。
    - **实际应用中的局限性**：虽然Ray功能强大，但对于大多数ML研究人员和科学家来说，配置和管理集群仍然过于复杂，需要基础设施和云专业知识。同时，Ray本身缺乏实验跟踪和检查点等关键功能，且难以满足Autodesk内部的安全要求，这些因素促使团队在Ray之上构建Ray Lab平台。
3. [00:08:34](https://youtu.be/akI12Sr82jM?t=514s) ​**Ray Lab平台架构设计**：详细介绍了Ray Lab的四大核心组件及其功能实现。

    - **集群管理组件**：提供简化的接口来创建和管理Ray集群，通过抽象底层基础设施复杂性，使研究人员能够专注于算法开发而非基础设施配置。
    - **统一训练和处理API**：在Ray原生API基础上增加了检查点和实验跟踪等功能，为分布式训练和数据处理提供一致的编程接口。
    - **智能作业调度系统**：支持基于优先级的作业调度，设置不同的优先级层级，并为不同团队和资源类型分配配额，确保资源的公平使用。
    - **资源效率和监控组件**：提供实时仪表板、主动监控和Slack通知功能，全面监控所有实验的运行状态和资源使用情况。
4. [00:11:11](https://youtu.be/akI12Sr82jM?t=671s) ​**多租户架构与用户接口设计**：展示了Ray Lab如何支持多团队协作和提供多样化的用户交互方式。

    - **多租户隔离机制**：使用Kubernetes命名空间将不同团队隔离，通过基于角色的访问控制精细管理团队对数据和命名空间的访问权限，确保安全性和隔离性。
    - **CLI命令行接口**：提供预定义默认值的快速开发接口，用户可以通过单一命令创建销毁集群、提交训练作业或查看日志，无需接触Kubernetes YAML文件或云控制台。
    - **Python客户端**：允许研究人员直接从notebook和流水线中以编程方式启动集群，特别适合运行实验、超参数扫描和模型刷新作业的自动化。
    - **Web用户界面**：为不习惯使用终端的科学家、研究人员、工程师甚至管理层提供集群活动的实时可视化，包括GPU使用情况、日志和指标，支持配额管理和空闲集群关闭。
5. [00:13:42](https://youtu.be/akI12Sr82jM?t=822s) ​**分布式训练简化与性能优化**：重点介绍了如何降低分布式训练的技术门槛并提升训练效率。

    - **统一训练接口**：通过整合Ray Train和PyTorch Lightning创建了Ray Lightning Experiment类，使得标准的Lightning作业能够无缝扩展到多节点Ray集群，同时保持相同的代码结构和训练逻辑。
    - **自动化基础设施**：自动集成检查点保存和日志记录，直接保存到S3存储桶并向Comet或TensorBoard报告指标，确保实验的可重现性和可追溯性。
    - **高性能网络优化**：通过集成AWS EFA（弹性光纤适配器）功能，将节点间通信速度从默认的10GB/s提升到最高3TB/s，解决了大规模训练中的通信瓶颈问题。
    - **内置监控与调试**：平台自动收集时间日志、作业状态、GPU利用率和训练指标，通过Web仪表板和CLI暴露所有信息，使工程师能够早期检测数据加载器瓶颈或不均匀的GPU利用率等问题。
6. [00:18:38](https://youtu.be/akI12Sr82jM?t=1118s) ​**资源分配与公平调度策略**：详细阐述了如何在多团队环境中实现资源的高效利用和公平分配。

    - **配额分配机制**：为不同团队分配预留GPU实例的固定份额，通过Kubernetes配额功能强制执行，确保各团队都能获得所需的计算资源。
    - **优先级调度系统**：在配额基础上引入基于优先级的作业调度，允许团队在正常配额作业之外提交低优先级的后台作业，充分利用其他团队的闲置资源。
    - **智能抢占与恢复**：当拥有配额的团队需要资源时，低优先级后台作业会被立即抢占，但系统会自动从最后一个检查点恢复作业，无需人工干预，既保证了资源利用率又维护了作业连续性。
    - **主动资源监控**：通过实时监控所有作业的资源使用效率，对低效作业自动发送优化建议，对严重低效的昂贵作业自动回收资源，直到问题解决，确保集群整体运行效率。
7. [00:23:20](https://youtu.be/akI12Sr82jM?t=1400s) ​**平台成效与未来展望**：总结了Ray Lab平台的实际效果和持续发展方向。

    - **业务影响**：Ray Lab已成为Autodesk可扩展AI研究的骨干平台，支持生成式AI、CAD和3D等多个项目的研发工作，显著提升了研究效率和资源利用率。
    - **技术成就**：通过抽象基础设施复杂性同时保留高级用户所需的定制功能，成功平衡了易用性和灵活性。基于优先级的调度和配额系统使预留GPU资源的利用率得到大幅提升。
    - **用户采纳**：Ray Lab的CLI、UI和SDK大大降低了新用户的上手门槛，使得研究人员能够快速在Ray集群上进行实验，促进了跨团队的技术协作和知识共享。
    - **持续演进**：团队继续在Ray和Ray Lab的基础上进行创新，重点关注多租户公平性、自动扩缩容和更精细的资源管理策略，为未来的AI研究需求做好准备。

## **How Geotab Scales Video AI Efficiently with Anyscale | Ray Summit 2025**

[https://www.youtube.com/watch?v=XM0PijtXEoU](https://www.youtube.com/watch?v=XM0PijtXEoU)

视频介绍：本视频由Geotab公司的工程师Mike分享，详细介绍了该公司如何利用Anyscale Ray框架优化视频AI推理和运营的完整历程。内容涵盖了从车队管理摄像头系统的演进、技术挑战识别，到基于Ray的架构设计决策和最终实现的模型管理平台。

结论：Geotab通过构建基于Anyscale Ray的集中式机器学习平台，成功解决了视频AI处理中的多项技术挑战。该平台实现了从模型训练、测试到部署的全流程自动化，显著提升了团队协作效率和系统性能。具体成果包括视频处理QPS提升43倍、GPU利用率从10%提升至50%、GPU资源需求减少40%，并为未来扩展到非视频机器学习领域奠定了坚实基础。这一案例展示了Ray在企业级AI系统中的强大应用价值。

关键点：

1. [00:02:31](https://youtu.be/XM0PijtXEoU?t=151s) ​**Geotab摄像头系统的演进与价值**：

    - **从车辆遥测到视频分析的跨越**：Geotab作为全球领先的车队远程信息管理公司，拥有超过500万台设备和1000亿数据点。传统遥测数据只能反映车辆内部状态，但无法解释外部环境因素。例如，司机急刹车的原因可能是前方有行人，也可能是司机分心使用手机，这些关键信息只有通过视频分析才能获得。
    - **两代AI摄像头的发展**：

      - 第一代Go Focus摄像头（2024年2月发布）具备边缘计算能力，能够在摄像头本地运行机器学习模型，同时将视频记录上传到云服务进行云端推理。
      - 第二代Go Focus Plus摄像头（2024年9月发布）性能更强大，配备前后双摄像头，不仅能监控车外环境，还能监测司机行为。当检测到司机使用手机时，系统能够实时进行语音提醒，这种即时反馈机制显著提升了驾驶安全性。
    - **业务价值实现**：视频分析系统为车队管理者提供了识别优秀司机和改进不良驾驶行为的能力，有效降低了碰撞风险，同时开启了更多AI应用可能性，如维护预测等高级功能。
2. [00:05:09](https://youtu.be/XM0PijtXEoU?t=309s) ​**与Ray合作的技术演进历程**：

    - **探索阶段**：2024年10月前，团队在Kubernetes上进行内部POC研究，使用开源的KubeRay进行初步实验。
    - **技术转折点**：参加2024年10月的Ray峰会后，团队被各行业案例深深震撼，开始深入理解Ray的潜力。同年12月邀请Anyscale团队到Geotab办公室进行培训和技术讨论。
    - **实践里程碑**：

      - 2025年3月构建了第一个POC就绪的视频后处理应用，实现了解码、转码、人脸模糊、车牌处理等云端功能。
      - 2025年5月该应用投入生产环境，连续6个月无生产问题，证明了系统的稳定性。
      - 2025年9月实现了首个完整的端到端计算机视觉模型，从模型实验、训练、调优、测试到部署全部在Anyscale Ray平台上完成。
3. [00:06:52](https://youtu.be/XM0PijtXEoU?t=412s) ​**视频AI面临的核心技术挑战**：

    - **代码重构难题**：现有模型主要使用C++编写以获得边缘设备的最佳性能，但如何将这些代码有效重构以在Ray上高效运行成为首要挑战。同时需要确保Python代码能在摄像头设备上原生运行。
    - **环境一致性验证**：需要在云环境和边缘环境之间确保结果的一致性，不能仅依赖真实摄像头进行测试，必须建立有效的模拟器测试体系。
    - **手动流程瓶颈**：整个模型端到端工作流中存在大量手动任务，包括训练、回归测试、模型注册、部署等，数据科学家花费大量时间在这些重复性工作上。
    - **团队协作障碍**：多个团队（Go Focus团队、Go Focus Plus团队、LLM团队、MLOps团队等）各自为战，缺乏高效的协作机制，团队间依赖关系导致项目进度受阻。
    - **测试验证复杂性**：

      - 性能测试需要确保模型能以特定帧率处理视频
      - 回归测试需要覆盖雪天、夜晚、雨天等各种场景的大数据集
      - 一致性测试需要验证边缘设备和云端产生相同结果
    - **数据隐私与合规性**：视频数据包含大量个人信息（司机面部、行人面部、车牌等），必须确保符合GDPR和其他AI法规要求，防止数据泄露。
4. [00:09:30](https://youtu.be/XM0PijtXEoU?t=570s) ​**关键架构设计决策**：

    - **Kubernetes vs VM选择**：

      - 选择Kubernetes的原因：Geotab已有大量数据平台和AI平台服务基于Kubernetes构建（Kafka、MLflow、Jupyter Hub、Airflow等），与现有基础设施集成更便捷。
      - VM的优势：作业启动时间更快（1分钟 vs Kubernetes的5-7分钟），但通过后续优化显著改善了Kubernetes的性能。
    - **C++代码集成方案**：

      - 在子进程和PyBind两种方案间权衡
      - 子进程方案迁移现有代码更容易，Python和C++代码松耦合，便于独立修改
      - PyBind方案开销更低，调试更简单
      - 最终因时间限制选择子进程方案，性能表现良好，未来新模型会重新评估
    - **代码仓库结构**：

      - 在单一仓库和多仓库模式间选择
      - 多仓库给予团队更多自主权，减少跨团队依赖
      - 单一仓库提高代码复用性，提供集中式CI/CD流水线，便于安全治理和控制
      - 最终采用混合方案：按团队或产品划分模型仓库（如Go Focus有自己的模型仓库，Go Focus Plus也有独立的模型仓库）
5. [00:13:05](https://youtu.be/XM0PijtXEoU?t=785s) ​**Geotab模型管理中心(MMC)的设计理念**：

    - **平台定位思考**：市场上现有的机器学习平台分为两类——开源但专业化的平台（如MLflow、Airflow、Ray）功能深入但需要集成，封闭但端到端的平台（如Vertex AI、SageMaker、Databricks）集成度高但定制性差。由于不同团队、公司的基础设施偏好、业务需求、安全控制等差异巨大，通用的"一体适用"解决方案不可行。
    - **MMC设计原则**：构建轻量级服务，连接所有现有组件，实现即插即用，最重要的是连接所有团队在统一环境中协作，并能根据自身需求进行定制。
    - **核心功能**：

      - 自动化工作流：从训练到测试、验证、部署、监控的全流程自动化，消除手动任务
      - 定制化功能：自有模型卡、回归测试、边缘设备与云端模型文件转换、A/B测试等
      - 系统集成：将Ray与MLflow、Airflow等系统集成
      - 统一界面：所有团队成员可在同一平台协作、验证结果、批准生产部署、审查模型代码和配置
6. [00:18:07](https://youtu.be/XM0PijtXEoU?t=1087s) ​**新旧架构对比与优化成果**：

    - **旧架构痛点**：

      - 数据科学家在个人笔记本或Jupyter Notebook上运行训练
      - 模型注册到MLflow后缺乏后续自动化流程
      - 测试数据集手动下载到本地进行文件转换和回归测试
      - 使用Airflow部署，但未针对机器学习工作流优化，难以配置GPU，无法最大化CPU/GPU利用率
    - **新架构优势**：

      - 引入MMC和Anyscale Ray作为核心组件
      - 模型训练、测试、文件转换、回归测试全部在Ray上运行
      - 服务从Airflow迁移到Ray服务或作业
      - 使用静态持久化磁盘存储完整视频数据集，相比GCS存储性能显著提升
      - 建立集中式可复用的回归测试流水线
      - 实施Git代码所有者清晰分离，各团队责任明确
      - 建立视频存储监控告警，保护数据安全合规
    - **性能提升**：

      - 视频处理QPS在高峰时段提升43倍
      - GPU利用率从约10%提升至50%
      - 支持Ray服务所需的GPU总量减少40%
      - 节点和作业启动时间从20分钟优化至5分钟以内
7. [00:24:02](https://youtu.be/XM0PijtXEoU?t=1442s) ​**未来发展方向**：

    - **平台扩展**：将Ray作为集中式模型训练和推理平台扩展到其他非视频机器学习领域，包括深度学习模型、传统机器学习模型、开源LLM微调、自托管等。
    - **视频推理创新**：探索使用LLM进行视频推理的新方法。
    - **监控系统完善**：目前使用Apache Superset作为临时解决方案，未来将构建更适合机器学习工作负载的专业监控系统。
    - **自动化重训练**：建立自动检测数据漂移或性能下降的机制，实现自动触发重训练。
    - **本地开发优化**：让数据科学家能在本地机器上更高效地进行故障排查，而不仅限于Anyscale工作空间。
    - **持续性能优化**：进一步优化节点和作业启动时间，寻求更多性能提升机会。

## **How xAI Scales Image &amp; Video Processing with Ray | Ray Summit 2025**

[https://www.youtube.com/watch?v=z9RHaQZnWM4](https://www.youtube.com/watch?v=z9RHaQZnWM4)

视频介绍: 本视频由XAI技术团队成员Jay和Kashin Chan分享，详细介绍了他们如何利用Ray框架构建大规模图像视频数据处理基础设施，以支持Grok多模态模型的训练需求。

结论: XAI通过构建基于Ray和Redis Queue的视频数据管理流水线，成功解决了大规模视频处理中的扩展性和容错性挑战。该系统采用简洁的三层架构，通过幂等性Actor设计、可靠队列机制和KubeRay自动扩展功能，实现了高效的视频数据处理能力。关键创新包括将状态信息完全存储在Redis中、实现任务的至少一次处理语义，以及通过KubeRay降低运维成本。这些技术方案为大规模多模态AI训练提供了可靠的数据处理基础设施，展现了Ray在工业级AI系统中的强大应用潜力。

关键点:

1. [00:03:11](https://youtu.be/z9RHaQZnWM4?t=191s) ​**视频数据处理的三大关键要素**：

    - **质量要求**：视频生成模型需要具备良好的美学品味，数据质量直接影响生成效果。团队通过美学评分、动态运动检测等算法确保输入数据的质量。
    - **多样性覆盖**：视频数据需要涵盖丰富的世界知识，包括各种活动场景如打篮球、踢足球等，确保模型能够学习到多样化的视觉模式。
    - **规模需求**：预训练级别的模型需要海量数据支持，基础设施必须能够处理从数千到数百万甚至更多视频的大规模数据处理任务。
2. [00:04:31](https://youtu.be/z9RHaQZnWM4?t=271s) ​**基础设施的核心要求**：

    - **可扩展性**：系统需要能够水平扩展以满足业务需求，实现资源投入与处理能力的线性增长。团队面临的关键挑战是如何高效利用大量GPU资源，确保基础设施能够处理大规模作业。
    - **低运维成本**：作为小型团队，系统必须具有极低的运维成本。设计目标是构建一个不需要大量时间维护的基础设施，避免将时间花费在重启作业和调试问题上。
3. [00:06:01](https://youtu.be/z9RHaQZnWM4?t=361s) ​**视频数据管理流水线的完整流程**：

    - **预处理阶段**：包括短片检测、场景检测，将视频切割成多个片段，然后运行视频编码算法以特定格式保存。
    - **去重处理**：在流程早期进行视频去重，通过获取视频嵌入向量并在数据库中搜索，确保只有独特视频进入下一阶段。
    - **标注算法**：运行美学评分、暗度检测、运动分析、OCR等多种标注算法，这些信息用于数据过滤和平衡。
    - **字幕生成**：最后阶段生成视频的文字描述，用于最终的视频生成模型训练。
4. [00:08:34](https://youtu.be/z9RHaQZnWM4?t=514s) ​**系统架构设计的简洁性原则**：

    - **三层架构**：采用Redis Queue存储元数据，Ray集群消费队列任务，底层基于Kubernetes集群的简洁设计。
    - **状态管理**：只在Redis Q中存储状态信息，Actor不保存任何状态，这种设计简化了系统的复杂度和调试难度。
    - **故障隔离**：减少组件数量以降低单点故障风险，确保系统易于理解和维护。
5. [00:10:37](https://youtu.be/z9RHaQZnWM4?t=637s) ​**容错性设计的核心思想**：

    - **幂等性保证**：确保每个Actor的操作都是幂等的，无论执行多少次都能产生相同结果，这是分布式系统设计的关键基础。
    - **Actor池管理**：维护Actor池，当某个Actor因内存不足或资源丢失而崩溃时，会自动被踢出池并启动新的Actor替代。
    - **可靠队列机制**：使用Redis可靠队列，包含待处理队列和处理队列，通过原子操作确保任务不会丢失，实现至少一次的处理语义。
6. [00:13:37](https://youtu.be/z9RHaQZnWM4?t=817s) ​**KubeRay迁移的价值与优势**：

    - **运维成本降低**：通过Ray集群CRD自动检查控制平面组件状态，避免手动监控Ray状态。
    - **可观测性提升**：提供SLI指标、Kubernetes事件和结构化日志，便于故障排查和系统监控。
    - **自动扩展支持**：支持应用层和集群层的自动扩展，能够动态创建任务和Actor，并根据需求启动新的工作节点。
7. [00:25:00](https://youtu.be/z9RHaQZnWM4?t=1500s) ​**水平扩展的实际实现**：

    - **两层扩展机制**：在Ray集群内部通过添加新Actor进行扩展，在Kubernetes集群层面通过在不同数据中心启动多个Ray集群实现扩展。
    - **性能表现**：单个Ray集群能够舒适地处理1万到2万个Actor，当需要更大规模时，通过启动新的Ray集群来分担负载。
    - **瓶颈识别**：系统性能不仅受Ray本身限制，还可能受网络带宽等其他因素制约，需要持续识别和解决瓶颈问题。
8. [00:27:30](https://youtu.be/z9RHaQZnWM4?t=1650s) ​**技术选型的深层考量**：

    - **Ray的优势**：作为连接集群层和应用层的粘合剂，Ray能够基于任务需求触发扩展，大大减少了应用与集群集成的开发工作量。
    - **资源灵活性**：相比直接使用Kubernetes Pod，Ray Actor可以更灵活地指定CPU、GPU等资源需求，支持小到0.1 GPU的细粒度资源分配。
    - **任务分配机制**：每个Actor通过参数指定消费的队列，实现任务的自动分配和处理。

## **Motional’s Blueprint for High-Performance ML Systems in Autonomous Driving | Ray Summit 2025**

[https://www.youtube.com/watch?v=BLWXwFjyiHA](https://www.youtube.com/watch?v=BLWXwFjyiHA)

视频介绍：本视频由Motional公司的工程师Dhanj和Avi分享，重点介绍了他们在自动驾驶技术领域使用Ray框架构建机器学习数据管道的实践经验。演讲详细阐述了如何利用Ray处理PB级别的驾驶日志数据，以及通过三次迭代优化实现数据处理效率的显著提升。

结论：Motional通过采用Ray框架成功构建了高效可靠的机器学习数据管道，将数据处理规模从TB级扩展到PB级，实现了10倍的速度提升和100倍的成本降低。关键成功因素包括：基于Ray Core的任务编排、Ray Data API的高效数据处理、多层次容错机制、静态Actor池优化以及创新的节点单例初始化器模式。这些实践不仅大幅提升了模型训练效率，还赋能ML工程师自主管理数据管道，将数据发布周期从数周缩短至数天，为自动驾驶技术的快速发展提供了坚实的数据基础。

关键点：

1. [00:01:47](https://youtu.be/BLWXwFjyiHA&t=107s) **Ray在Motional数据栈中的核心价值与应用背景**

    - **数据处理挑战**：Motional面临处理海量驾驶日志数据的核心挑战，每个驾驶日志是从自动驾驶汽车单次行程中收集的数据集合，包含来自激光雷达、雷达、摄像头等多模态传感器数据，总数据量达到PB级别
    - **传统流程瓶颈**：在采用Ray之前，数据处理流程复杂冗长，驾驶日志需要经过数据仓库和训练平台处理后才能到达ML工程师手中，整个流程效率低下
    - **Ray带来的变革**：通过引入Ray框架，显著缩短了原始驾驶日志与ML工程师之间的距离，数据处理吞吐量提升了约10倍，同时平台团队接收的请求数量也大幅增加
    - **业务影响**：这一改进使得公司能够更充分地利用海量驾驶数据，为自动驾驶模型的训练和优化提供了更丰富的数据支持
2. [00:03:02](https://youtu.be/BLWXwFjyiHA&t=182s) **ML数据准备管道的核心架构设计**

    - **基础架构组成**：基于两个主要组件构建 - Ray Core用于驾驶日志处理的基础编排，以及Ray Data API用于单个驾驶日志的具体数据处理
    - **任务编排机制**：使用ray.wait模式限制并发处理的驾驶日志数量，实现类似滑动窗口的控制机制，避免集群资源过载
    - **资源智能调度**：针对驾驶日志大小高度可变的特点（从10分钟到3-4小时不等），为每个任务提供内存估计参数，使调度器能够更智能地分配资源
    - **数据处理流程**：每个驾驶日志的处理封装在独立的process_drive_log任务中，利用Ray Data API进行读取、映射、连接和写入等操作，支持分布式连接等高级功能
3. [00:07:25](https://youtu.be/BLWXwFjyiHA&t=445s) **管道优化策略与性能调优实践**

    - **处理顺序优化**：采用"大任务优先"策略，优先处理规模较大的驾驶日志，通过增加计算重叠度来节省总体处理时间
    - **调度策略选择**：使用spread调度策略分散任务执行，降低节点故障的影响，避免资源热点问题
    - **容错机制设计**：构建多层次重试机制，在数据读取、处理和任务执行各个层面都启用重试功能，确保长时间运行作业的稳定性
    - **状态检查点**：实现作业状态检查点机制，将已处理的驾驶日志状态持久化存储，在任务重启时能够跳过已完成的处理，避免重复工作
4. [00:11:21](https://youtu.be/BLWXwFjyiHA&t=681s) **下游特征生成管道的三次迭代演进**

    - **迭代一：快速原型**：基于Ray Data API快速构建可工作的管道，专注于转换函数的实现，通过Hydra构建编排层，使ML工程师能够轻松定义和替换转换逻辑
    - **迭代二：问题解决**：面对工作内存溢出和管道随机挂起问题，引入ray.wait背压模式和状态检查点机制，解决了基本稳定性问题但出现内存泄漏挑战
    - **迭代三：性能突破**：放弃弹性集群需求，创建静态Actor池，将样本映射到固定的执行器，实现内存泄漏问题的根本解决，性能相比迭代二提升2倍
    - **最终成果**：相比原有的Spark EMR基线，新管道速度提升10倍，成本降低100倍（从10万美元降至1千美元），数据处理周期从数周缩短至数天
5. [00:19:27](https://youtu.be/BLWXwFjyiHA&t=1167s) **节点单例初始化器与数据提供者Actor模式**

    - **问题背景**：处理远程数据源时面临并发访问限制，传统解决方案如共享文件系统性能不佳，网络优化实例成本高昂
    - **模式设计**：在每个节点上部署单例Actor协调数据预加载，所有工作器通过该Actor访问数据，实现节点级数据缓存和高效访问
    - **实现细节**：数据提供者Actor负责初始化节点数据预加载，并提供受控的数据访问方式；通过节点亲和调度策略确保每个节点只有一个实例；消费者通过获取节点ID和Actor引用来访问本地缓存数据
    - **核心优势**：提供快速本地数据访问，避免对象存储内存占用，特别适合具有并发限制的API数据访问场景，为大规模数据处理提供了可靠的底层支持
6. [00:25:34](https://youtu.be/BLWXwFjyiHA&t=1534s) **团队协作与工程实践的最佳实践**

    - **ML工程师赋能策略**：通过清晰的代码所有权划分，ML工程师负责业务逻辑层，基础设施团队负责框架层，在保证灵活性的同时维持系统性能
    - **桥梁工程师角色**：在ML团队中设置既懂机器学习又精通Ray的工程师，作为基础设施团队与ML工程师之间的技术桥梁，确保管道代码的质量和性能
    - **代码审查机制**：要求ML工程师在引入新方法时证明其性能不低于原有方案，通过严格的代码审查流程保证整体系统的稳定性和效率
    - **持续改进文化**：鼓励团队不断优化和迭代管道设计，通过实际数据驱动决策，确保技术方案始终服务于业务目标

## **SkyRL: A Scalable and Flexible Post-Training Framework | Ray Summit 2025**

[https://www.youtube.com/watch?v=5ZLGPRmlekg](https://www.youtube.com/watch?v=5ZLGPRmlekg)

视频介绍：本视频由SkyRL团队核心成员分享，详细介绍了他们开发的开源强化学习框架SkyRL的设计理念、架构特点以及实际应用案例。演讲从强化学习在语言模型领域的发展历程切入，深入解析了SkyRL如何通过模块化设计解决传统RL框架在应对复杂任务时的局限性。

结论：SkyRL通过将训练器、生成器和控制器三大核心组件解耦，实现了前所未有的灵活性和可扩展性。该框架不仅支持多样化的训练后端和推理引擎，还能让研究人员专注于算法创新而无需担忧底层系统复杂性。从简单的数学推理到复杂的多轮工具调用任务，SkyRL都展现了出色的适应能力。未来的发展方向包括更好的异步训练支持和内存管理优化，为大规模智能体训练提供更强大的基础设施支持。

关键点：

1. [00:01:07](https://youtu.be/5ZLGPRmlekg&t=67s) ​**强化学习框架的演进历程与设计动机**：

    - 演讲者将开源RL框架的发展划分为三个时代：RLHF时代、推理训练时代和智能体训练时代，每个时代都对系统架构提出了不同的要求。
    - 在RLHF时代，工作负载主要特征是单轮生成、纯文本输出和较短的上下文，这导致训练和推理紧密耦合的设计成为主流。
    - 随着DeepSeek R1等模型的出现，推理步骤开始主导训练时间，研究人员首次感受到需要将推理工作负载独立扩展的压力。
    - 智能体训练时代的到来彻底改变了工作负载特性：多轮交互、工具调用、更长上下文和环境服务器的引入，使得传统紧密耦合架构不再适用。
    - 这种演进促使伯克利实验室团队重新思考整个RL栈设计，最终催生了SkyRL项目的诞生。
2. [00:04:39](https://youtu.be/5ZLGPRmlekg&t=279s) ​**SkyRL框架的核心设计原则**：

    - 支持多样化后端：允许用户根据需求灵活选择或自定义训练栈、推理栈和环境目标，打破技术限制。
    - 最佳硬件利用：能够为每个组件选择最适合的硬件配置，无论是追求性能、可用性还是成本效益。
    - 明确定义的公共API：通过良好的软件工程实践，提供清晰的组件接口规范，使用户能够轻松集成自定义后端。
    - 灵活控制流：支持研究人员表达各种训练循环执行流程，包括GRPO、PPO、多智能体系统等复杂场景。
    - 这些原则共同确保了框架的扩展性和适应性，能够满足不同规模和复杂度的研究需求。
3. [00:07:06](https://youtu.be/5ZLGPRmlekg&t=426s) ​**SkyRL架构的三大核心组件详解**：

    - 训练器：专注于接收轨迹和奖励数据并更新策略模型参数，完全无需关心数据来源。支持FSDP和Megatron等先进训练技术，同时提供训练工作者API供用户集成其他后端。
    - 生成器：负责产生轨迹和奖励数据供训练器使用，完全独立于训练器存在。该组件封装了推理引擎和环境交互，是最常被用户定制的部分。
    - 控制器：管理组件间的控制流，为用户提供高级抽象来表达训练控制逻辑。负责资源分配、调度和算法调整，而无需关心底层基础设施细节。
    - 这种清晰的职责分离使得每个组件都能独立优化和发展，同时保持整体的协调运作。
4. [00:10:56](https://youtu.be/5ZLGPRmlekg&t=656s) ​**环境集成与自定义的灵活性**：

    - Sky Gym组件提供了低门槛的入门方式，支持快速迭代简单任务，如搜索工具、Python代码执行器和文本转SQL等。
    - 框架支持与现有环境栈和智能体栈的集成，包括Primex Environments Hub、Terminal Bench和MiniAgent等。
    - 集成过程通常只需要30-50行代码，主要涉及数据格式转换和OpenAI API兼容端点的配置。
    - 这种设计使得研究人员能够专注于各自领域的研究，而无需被框架的其他部分所困扰。
5. [00:14:41](https://youtu.be/5ZLGPRmlekg&t=881s) ​**高级功能集成案例：Flash RL和策略蒸馏**：

    - Flash RL集成允许在8位精度下运行推理，同时保持训练的半精度，通过在线量化和修正损失函数实现性能提升。
    - 该方法在32B模型上实现了最高1.7倍的加速，同时保持原始性能，展示了框架对新技术的快速适配能力。
    - 策略蒸馏的实现通过自定义训练器实例修改奖励后处理，结合自定义优势估计和损失函数，实现了比传统RL更快的收敛速度。
    - 这些案例证明了在SkyRL中进行算法修改的相对简便性，为研究人员提供了强大的实验平台。
6. [00:17:17](https://youtu.be/5ZLGPRmlekg&t=1037s) ​**真实世界智能体任务的系统复杂性**：

    - 深度研究任务：智能体使用Google搜索和网页抓取工具回答复杂用户查询，涉及API交互和智能缓存策略。
    - 计算机使用任务：智能体通过鼠标键盘工具在计算机屏幕上操作，需要虚拟机实例管理和资源复用。
    - SWE任务：智能体使用bash和文件编辑工具解决GitHub仓库问题，涉及容器化环境和测试套件执行。
    - 不同任务在工具可用性、环境扩展、调度策略和智能体框架方面存在显著差异，需要针对性的优化方案。
7. [00:21:12](https://youtu.be/5ZLGPRmlekg&t=1272s) ​**SkyRL Agent层的高级抽象设计**：

    - Agent Runner类负责整个轨迹批次的生成管理，使用用户指定的调度逻辑来分发生成任务。
    - 支持用户自定义工具实现、智能体框架和生成上下文，为不同任务提供灵活的配置选项。
    - 标准化OpenAI API格式确保了组件间的互操作性，使得不同部分能够无缝协作。
    - 这种设计使得研究人员能够专注于各自专业领域，而无需深入了解整个系统架构。
8. [00:23:09](https://youtu.be/5ZLGPRmlekg&t=1389s) ​**优化调度策略的性能影响**：

    - 计算机使用任务采用异步固定池策略，实现虚拟机实例的有效复用。
    - 简单任务如数学、编码和深度研究采用完全并行化策略，充分利用廉价的计算资源。
    - SWE任务采用三阶段流水线策略：容器初始化、代码生成循环和奖励计算，通过重叠执行实现优化。
    - 实验结果显示，在训练32B模型、512条轨迹、2个节点的配置下，异步流水线方法相比异步批处理方法实现了1.5倍的加速。
    - 这种优化在更大规模的训练运行中效果更加显著，证明了定制调度策略的重要性。
9. [00:25:50](https://youtu.be/5ZLGPRmlekg&t=1550s) ​**路线图与未来发展计划**：

    - Sky LTX新库支持在自有硬件上运行Tinker兼容API，扩展了框架的应用范围。
    - LoRA支持实现了高效训练的同时保持准确性，为资源有限的研究者提供了更多选择。
    - 短期重点包括改进异步训练支持飞行中权重更新，以及内存管理支持在训练过程中集成摘要同时保持策略行为。
    - 社区建设方面建立了Slack工作区，方便用户交流使用经验和贡献代码。
    - 这些发展计划体现了团队对框架持续改进和社区生态建设的承诺。

## **KubeRay + vLLM at DatalogyAI: Engineering Trillion-Scale Synthetic Data Systems | Ray Summit 2025**

[https://www.youtube.com/watch?v=DNpWaWb2kLI](https://www.youtube.com/watch?v=DNpWaWb2kLI)

视频介绍：本视频由Dtology公司的联合创始人Bogdan和技术团队成员Fan共同分享，详细介绍了他们在万亿级别token规模的预训练中扩展合成数据所获得的工程经验教训。Dtology是一家数据策展产品公司，专注于帮助客户识别最佳数据子集，通过过滤、去重、转换、增强和生成合成数据来优化模型训练效果。

结论：Dtology通过构建统一的合成数据生成流水线，成功解决了在万亿token规模下合成数据生成的核心工程挑战。他们通过优化元数据获取机制、实现可恢复的执行架构、构建跨云调度系统以及快速搜索最优推理参数，实现了合成数据生成效率的质的飞跃。该解决方案使客户能够获得训练速度提升10-20倍、模型性能提升8-12个百分点、参数量减少1-2个数量级但性能相当的显著收益。未来发展方向包括解决资源打包问题、提升数据局部性和实现智能调度，展现了在大规模AI训练中合成数据技术的巨大潜力。

关键点：

1. [00:02:10](https://youtu.be/DNpWaWb2kLI?t=130) ​**合成数据的必要性与价值**：

    - **数据墙挑战**：随着扩展定律的要求，需要指数级更多的数据和计算资源来获得更好的模型，但互联网数据终将耗尽，形成所谓的"数据墙"。
    - **合成数据解决方案**：通过生成合成数据可以获得所需的数据量，突破数据墙的限制。Dtology的"超越网络"方法通过重新表述文档或在不同位置重构文档来增加数据多样性。
    - **实际效果验证**：实验结果显示，使用合成数据可以训练出更好的模型，例如他们的3B参数模型在其他数据集上能够匹配8B参数模型的性能，同时训练速度提升2.7到7.7倍。
2. [00:05:20](https://youtu.be/DNpWaWb2kLI?t=320) ​**从研究到生产的演进历程**：

    - **初始架构**：最初采用双轨制架构，在研究端的Slurm集群中生成合成数据，然后转移到基于Kubernetes的生产集群，这个过程缓慢且容易出错。
    - **技术栈构成**：生产环境基于Kubernetes部署，数据存储在对象存储中，使用Flight作为工作流编排器，Spark作业实现策展算法。
    - **统一流水线**：现在将所有组件整合到一个大型流水线中，利用Kubernetes和Ray实现大规模推理，通过自定义调度器能够调度Spark、Ray和任何标准的Kubernetes作业。
3. [00:08:02](https://youtu.be/DNpWaWb2kLI?t=482) ​**万亿token规模的元数据瓶颈**：

    - **问题规模**：在预训练数据规模下，需要处理超过30万亿token的数据集，单个作业的数据集足迹可达600TB以上，存储规模达到PB级别。
    - **元数据获取瓶颈**：数百万个数据分区存储在云端，运行作业时会出现S3请求限制，导致GPU长时间空闲。以500万个分区为例，单线程元数据获取需要7-11天。
    - **创新解决方案**：通过最小化元数据需求，只存储文件名和大小信息，将元数据获取时间从11天缩短到2小时，元数据存储从12GB减少到0.5GB。
4. [00:13:15](https://youtu.be/DNpWaWb2kLI?t=795) ​**长运行推理作业的容错机制**：

    - **GPU基础设施不稳定性**：长达数天甚至数周的推理作业面临GPU节点故障或更换的风险，研究人员经常遇到作业在最后1%失败的情况。
    - **一对一分区映射**：建立Spark作业与Ray作业之间的一对一分区映射，实现数据流的可追溯性，失败时知道从哪里恢复。
    - **检查点与重试机制**：通过检查点机制跟踪作业进度，失败时只重新处理失败的1%进度，将多日作业失败转变为可恢复的重试。
5. [00:16:29](https://youtu.be/DNpWaWb2kLI?t=989) ​**跨云资源调度优化**：

    - **资源限制挑战**：万亿token规模的工作负载已经达到GPU容量限制，需要在不同提供商或云之间协调工作负载。
    - **异构作业调度问题**：调度异构作业会导致边缘情况、碎片化和GPU空闲，造成成本浪费。展示了CPU作业阻塞GPU作业的具体案例。
    - **作业队列分割**：通过构建作业队列概念，将Spark作业和Ray作业分别路由到不同的资源池，避免资源争用。调度器配置允许用户定义作业路由策略和排序策略。
6. [00:21:34](https://youtu.be/DNpWaWb2kLI?t=1294) ​**推理参数优化与性能提升**：

    - **vLLM推理后端**：使用vLLM作为推理后端，但发现不同模型和硬件配置下的性能差异很大。
    - **关键优化参数**：识别出批处理大小、连续批处理、推测解码等关键优化杠杆。特别发现ngram技术在缺乏草稿模型时特别有效。
    - **参数搜索方法**：通过快速搜索最优vLLM参数，在L40s上相比默认参数获得40%的吞吐量提升，而不需要过度工程化。
7. [00:24:20](https://youtu.be/DNpWaWb2kLI?t=1460) ​**成果展示与未来方向**：

    - **规模演进**：从去年底的300亿token，到今年春季的1万亿token，再到当前正在处理的7万亿token，成为世界上最大的文本合成数据运行之一。
    - **领域扩展**：正在将这种方法扩展到网络、多语言、数学和代码、多模态以及特定领域（如法律数据）数据集。
    - **未来研究方向**：包括解决资源打包问题、在相同硬件上协同定位CPU和GPU作业、提升数据局部性、实现智能调度以优化资源分配。

## **Accelerating vLLM with LMCache | Ray Summit 2025**

[https://www.youtube.com/watch?v=TwLd15HE6AM](https://www.youtube.com/watch?v=TwLd15HE6AM)

视频介绍：本视频详细介绍了IronCache项目，这是一个专为大语言模型推理设计的KV缓存解决方案。演讲者深入探讨了构建该项目的动机、系统架构设计、性能优化技术以及未来的机器学习优化方向，旨在打造连接推理引擎与存储硬件的最快开源KV缓存系统。

结论：IronCache通过解决KV缓存规模快速增长和存储需求变化的挑战，成功构建了一个高性能的KV缓存层。该项目采用创新的系统架构设计，包括与推理引擎协同演化的API接口、PCIe带宽饱和技术、请求级异步处理和层级流水线等优化手段，实现了显著的性能提升。同时，团队正在探索KV缓存压缩、稀疏注意力优化和跨模型缓存重用等机器学习优化技术，为大规模AI推理应用提供了可靠的缓存基础设施。该项目已获得广泛社区支持和企业采用，展现了在AI推理基础设施领域的重要价值。

关键点：

1. [00:01:57](https://youtu.be/TwLd15HE6AM?t=117) ​**构建IronCache的动机与趋势分析**：

    - KV缓存规模快速增长：随着大语言模型的发展，KV缓存的大小正在迅速超过GPU容量。从2024年初开始，用户开始需要CPU内存来存储KV缓存，到第二季度已经出现使用本地SSD的用例，现在甚至需要对象存储和跨云解决方案。这种增长趋势使得传统存储方案无法满足需求。
    - 每令牌KV缓存大小减小：由于注意力机制的演进，包括从传统MHA到分组查询注意力(GQA)再到MLA等稀疏注意力机制，每个令牌所需的KV缓存大小正在显著减小。这种变化使得从存储加载KV缓存的速度提升了10倍，让基于存储的解决方案变得可行。
    - 基础设施需求：KV缓存是大型张量数据，与传统网页或文本数据不同，需要专门的基础设施来处理。随着KV缓存越来越多地存储在外部存储中，需要一个标准化的解决方案来管理这些数据。
2. [00:06:40](https://youtu.be/TwLd15HE6AM?t=400) ​**系统架构与采用情况**：

    - 分层架构设计：IronCache位于分布式服务框架（如vLLM、SGLang）和推理引擎之下，作为一个专门的KV缓存层，连接推理引擎与存储硬件。
    - 广泛采用证明：项目已处理超过300TB的KV缓存数据和每周12.8亿个命中令牌，获得了100多名全球贡献者的支持，并加入了PyTorch生态系统项目。
    - 企业级采用：包括设计合作伙伴和早期采用者在内的众多大型公司都在使用IronCache，证明了该解决方案的实用性和可靠性。
3. [00:09:54](https://youtu.be/TwLd15HE6AM?t=594) ​**性能优势与基准测试**：

    - 显著性能提升：在使用Llama 3.1模型的测试中，相比商业替代方案和基础vLLM，IronCache实现了5-10倍的首令牌时间(TTFT)加速和2倍的令牌间延迟提升，同时支持更高的请求率。
    - 社区驱动发展：项目的成功离不开开源社区的贡献，包括可观测性、SGLang集成、存储和硬件支持等多个方面的改进。
    - 部署便利性：提供简单的API接口和Helm图表，支持单节点和分布式部署，同时通过SaaS产品降低使用门槛。
4. [00:13:38](https://youtu.be/TwLd15HE6AM?t=818) ​**系统优化技术深度解析**：

    - 与推理引擎协同演化：通过在设计vLLM时构建公共API集，确保与快速演进的大语言模型生态保持兼容。这种设计考虑了vLLM的调度器-工作者分离架构和前缀缓存抽象。
    - PCIe带宽饱和技术：编写自定义CUDA内核来加速分页KV缓存的加载和卸载，避免大量小内核启动的开销。通过请求级异步和层级流水线实现计算与数据传输的重叠。
    - 存储带宽优化：采用批处理操作减少RPC开销，实现零CPU拷贝，通过与存储供应商的紧密合作直接将数据复制到注册内存空间。
5. [00:20:59](https://youtu.be/TwLd15HE6AM?t=1259) ​**工程实现细节**：

    - 请求级异步处理：通过允许vLLM在KV缓存未加载完成时跳过请求调度，实现更高的异步性。测试显示这种优化能将执行时间从86秒减少到79秒。
    - 层级流水线技术：通过精细的工程实现，将CPU卸载的开销从10%降低到2-3%，几乎消除了对推理性能的影响。性能分析显示能够同时充分利用计算和内存带宽。
    - API设计哲学：提供调度器和工作者两套API，通过前缀缓存抽象实现功能兼容性，利用分段计算图支持在不破坏CUDA图支持的情况下注入GPU操作。
6. [00:26:29](https://youtu.be/TwLd15HE6AM?t=1589) ​**机器学习优化与未来方向**：

    - KV缓存压缩：利用KV缓存在令牌维度上的连续性，采用空间编码和量化技术。发现深层对量化更不敏感，该技术已在Bloomberg的生产环境中部署。
    - 稀疏注意力优化：针对使用稀疏注意力的模型，只加载推理计算所需的KV缓存，相比现有框架可实现4倍的长上下文性能提升。
    - 跨位置和跨模型缓存：开发Cache Blend技术支持在任何位置进行缓存重用，通过稀疏交叉注意力校正实现非前缀部分的加速。Joy Speak技术允许在不同模型间选择性重用KV缓存，为模型间通信开辟了新可能性。

## **How Red Hat Scales Large-Scale Serving with vLLM | Ray Summit 2025**

[https://www.youtube.com/watch?v=WQnKLvbBDcU](https://www.youtube.com/watch?v=WQnKLvbBDcU)

视频介绍：本视频由Red Hat工程师Rob分享，深入探讨了在vLLM平台上优化混合专家模型推理性能的技术方案，重点分析了传统张量并行方法的局限性以及数据并行注意力与专家并行相结合的新范式。

结论：通过采用数据并行注意力与专家并行的混合架构，结合高效的dispatch-combine操作、双批次重叠技术和预填充-解码分离策略，vLLM平台成功解决了现代大模型在多节点部署中的性能瓶颈。这些优化使得在96个GPU的集群上能够实现每秒2000个token的高吞吐量，同时显著提升了KV缓存的利用效率。未来还将继续完善弹性专家并行和容错机制，为大模型推理提供更稳定可靠的分布式解决方案。

关键点：

1. [00:02:57](https://youtu.be/WQnKLvbBDcU&t=177s) ​**传统张量并行方法的局限性**：

    - 传统方法采用张量并行在单个节点内分割权重矩阵，通过列并行、行并行和all-reduce操作实现GPU间同步。这种方法对于标准的Llama类稠密模型效果良好，但在处理现代新型模型时面临严峻挑战。
    - KV缓存分片问题：传统模型如Llama 3设计有8个KV头，可以自然地分配到8个GPU上。但现代模型如Quen 3E只有4个KV头，而DeepSeek R1使用的MLA注意力机制将所有KV头压缩到单个潜在向量中，无法在多GPU间分片。
    - 多节点部署瓶颈：从单节点扩展到多节点时，通信从高速NVLink切换到较慢的Infiniband网络，传统张量并行方法的通信开销急剧增加，性能显著下降。特别是对于MLA这类压缩KV缓存模型，需要在所有TP rank间复制KV缓存，导致实际可用缓存大小大幅缩减。
2. [00:06:18](https://youtu.be/WQnKLvbBDcU&t=378s) ​**数据并行注意力与专家并行新范式**：

    - 新架构采用数据并行处理注意力层，专家并行处理MLP层，避免了传统all-reduce操作，转而使用稀疏集体操作，在多节点环境中具有更好的扩展性。
    - 核心优势：数据并行注意力允许请求路由到N个rank中的一个，实现完整的KV缓存利用率，无需在不同rank间复制MLA注意力机制的KV状态。专家并行确保在多节点环境中只有一份专家权重副本，同时提供超大的KV缓存容量。
    - 工作流程：注意力层在各DP rank上独立运行，之后通过dispatch操作将token发送到相应的专家rank，执行标准group gem操作，最后通过combine操作将结果返回原始DP rank。这种稀疏通信模式大幅减少了节点间数据传输量。
3. [00:09:47](https://youtu.be/WQnKLvbBDcU&t=587s) ​**高效dispatch-combine实现**：

    - DeepGEMM和DeepEP协同设计：DeepSeek开源的高性能内核，专门优化dispatch-combine操作。提供两种变体：针对预填充优化的高吞吐版本和针对解码优化的低延迟版本。
    - 预填充案例：采用动态缓冲区分配，处理不同形状的输入。流程包括dispatch、permute、gem计算、激活函数、unpermute等步骤，类似于标准group gem操作。
    - 解码案例优化：为兼容CUDA Graphs使用静态预分配缓冲区，通过batched变体直接处理接收到的token，跳过无效token的计算，避免permute操作，实现极低延迟。这种设计消除了CPU-GPU同步，完全兼容CUDA Graphs。
4. [00:16:48](https://youtu.be/WQnKLvbBDcU&t=1008s) ​**双批次重叠技术**：

    - 性能瓶颈分析：即使在优化后，dispatch-combine操作仍占据层运行时间的近一半，特别是在使用Infiniband网络的多节点环境中。
    - 微批次划分：将请求分成更小的微批次，通过精细调度实现通信与计算的重叠。传统方法使用多个CUDA流，但DPP低延迟作为RDMA内核支持更细粒度的控制。
    - 单流优化：在单个流中将dispatch-combine操作分解为send和receive两个步骤，实现attention计算与RDMA通信的完美重叠。具体流程包括：微批次0的attention→dispatch send→微批次1的attention→dispatch receive等交替执行模式。
    - 实际效果：在EP72的大规模部署中（9个节点，256并发请求），成功隐藏了大部分集体通信操作时间，在H100上实现每秒2000个token的解码性能，且随着GPU数量增加保持良好扩展性。
5. [00:22:51](https://youtu.be/WQnKLvbBDcU&t=1371s) ​**预填充-解码分离架构**：

    - 分离必要性：确保DP rank间的负载均衡和一致性，避免因单个rank执行预填充工作而拖慢整个系统。同时允许分别使用针对预填充和解码优化的不同DPP变体。
    - 架构组成：需要编排框架协调请求和高效的KV传输机制。LLMD项目提供完整的编排解决方案，包括网关、端点选择器、sidecar等组件。
    - KV传输流程：预填充工作器处理提示后返回KV传输元数据，解码工作器通过Nixl库执行RDMA事务从远程实例拉取KV数据。Nixl支持多种传输后端（UCX、Moonake等），并支持零拷贝GPU Direct。
    - 异步集成：在vLLM中KV传输完全异步执行，不阻塞引擎执行，不占用SM资源。即使在单节点环境中，预填充-解码分离也能在中等并发范围内提供显著的性能提升。
6. [00:34:10](https://youtu.be/WQnKLvbBDcU&t=2050s) ​**生产就绪状态与未来发展**：

    - 功能成熟度：预填充-解码分离功能已在vLLM中存在约6个月，经过充分硬化，已有大规模用户在生产环境中使用。YDP功能在过去2-3个月达到成熟，在强化学习等需要高批次大小的场景中表现优异。
    - 编排生态：LLMD专注于Kubernetes环境的生产部署，NVIDIA Dynamo提供类似功能但更少依赖Kubernetes。整个编排层生态仍在发展中，包含KV缓存路由、磁盘卸载、智能负载均衡等丰富功能。
    - 弹性专家并行挑战：当前主要难点在于底层集体操作需要新的API来支持故障恢复。当96个GPU中的任何一个出现故障时，整个系统都可能受到影响。解决方案需要Nixl团队和社区在底层基础设施上的持续改进，目前LLMD采取快速检测和重启策略来保证系统可用性。

## **Embedded LLM’s Guide to vLLM Architecture &amp; High-Performance Serving | Ray Summit 2025**

[https://www.youtube.com/watch?v=2Xpovbw9M78](https://www.youtube.com/watch?v=2Xpovbw9M78)

视频介绍：本视频由Embedded LM的工程师TJ分享，详细介绍了如何在vLLM推理框架中集成自定义内核（kernel）来加速大语言模型推理，特别是针对AMD GPU平台的ROCm后端集成实践。

结论：通过将AMD优化的ROCm内核集成到vLLM框架中，成功实现了对DeepSeek等模型推理性能的显著提升。集成过程涉及自定义算子实现、注意力后端开发、CUDA图支持等多个关键技术环节。vLLM通过分层架构设计、torch compile兼容性处理和统一的集成评估流程，为不同硬件平台的内核集成提供了标准化方案。未来随着更多优化内核的加入，vLLM在多硬件平台上的性能表现将进一步提升。

关键点：

1. [00:01:10](https://youtu.be/2Xpovbw9M78&t=70s) ​**vLLM架构概述与内核集成定位**：

    - vLLM采用分层架构设计，从上至下包括API层、异步引擎层、LLM引擎层和模型实现层。
    - API层提供OpenAI兼容的接口，便于下游应用集成。
    - LLM引擎层包含调度器、KV缓存管理器和模型执行器，是核心推理逻辑所在。
    - 内核集成主要发生在硬件抽象层，即模型实现层之下，负责将特定硬件的优化内核集成到框架中。
    - 这种分层设计使得vLLM能够保持架构清晰的同时，支持多种硬件平台的内核集成。
2. [00:04:00](https://youtu.be/2Xpovbw9M78&t=240s) ​**ROCm内核集成实践与性能提升**：

    - 通过集成AMD优化的ROCm内核，在DeepSeek模型上实现了2倍的吞吐量提升。
    - AMD提供了多种内核实现方式：Composable Kernel（CK）、汇编内核（HIP/ASM）和Triton内核。
    - 针对DeepSeek模型架构，识别出需要优化的关键算子，包括RMSNorm、Rotary Positional Embedding等。
    - 内核集成分为两类抽象：非注意力相关的自定义算子和注意力后端实现。
    - 实际集成过程中，需要根据算子特性选择合适的集成方式，确保与现有框架的兼容性。
3. [00:06:02](https://youtu.be/2Xpovbw9M78&t=362s) ​**自定义算子集成技术细节**：

    - vLLM提供custom_ops扩展点，用于注册和调用自定义算子。
    - 由于vLLM默认启用torch compile，需要确保自定义内核与torch inductor兼容。
    - 使用direct_register_custom_op函数注册不兼容的内核，使其对torch compile透明。
    - 注册过程需要定义真实实现函数和fake tensor函数，后者用于向torch inductor描述算子签名。
    - 通过torch.ops.vLLM命名空间调用注册的自定义算子，确保框架能够正确路由到特定硬件实现。
4. [00:10:50](https://youtu.be/2Xpovbw9M78&t=650s) ​**多平台支持与编译配置**：

    - vLLM支持通过不同的forward路径（cuda、xpu、hip）实现多平台支持。
    - 在模型类中实现多个forward方法：forward_native（PyTorch原生）、forward_cuda、forward_hip等。
    - 通过compilation_config配置指定使用哪个平台的实现，框架根据配置自动选择对应的forward路径。
    - 这种设计使得同一模型代码可以支持多种硬件平台，只需实现相应的内核和forward方法。
    - 当前已集成多个ROCm内核，不仅优化了DeepSeek模型，也惠及Llama等其他模型架构。
5. [00:12:10](https://youtu.be/2Xpovbw9M78&t=730s) ​**注意力后端集成架构**：

    - 注意力后端是另一类重要的内核抽象，用于实现MLA（Multi-Head Attention）和线性注意力。
    - 集成注意力后端需要修改attention_ops、attention_backend等目录下的代码。
    - 对于ROCm MLA后端，需要实现三个核心组件：元数据管理类、prefill路径和decode路径。
    - 通用逻辑集中在common.py中，包含共享的元数据类和基础实现，平台特定逻辑在子类中实现。
    - DeepSeek模型的特殊性在于其解码注意力部分，需要实现自定义的decode元数据和内核。
6. [00:16:25](https://youtu.be/2Xpovbw9M78&t=985s) ​**CUDA图支持与性能优化**：

    - CUDA图支持对高性能推理至关重要，特别是在高算力GPU上能显著减少CPU开销。
    - 实现CUDA图支持需要初始化可重用的缓冲区，并在decode路径中正确填充数据。
    - 通过cuda_graph_support枚举告诉调度器后端支持的CUDA图捕获能力。
    - 支持级别包括：从不支持、单token解码、统一批次解码和全阶段支持。
    - 默认启用piecewise模式，在prefill和混合阶段使用部分图，decode阶段使用完整图。
    - 在ROCm后端中，仅支持纯decode阶段的CUDA图捕获就带来了31%的吞吐量提升。
7. [00:21:06](https://youtu.be/2Xpovbw9M78&t=1266s) ​**性能优化技巧与陷阱避免**：

    - 避免在forward路径中频繁检查环境变量，这种开销在深层模型中会累积成显著性能损失。
    - 实际案例显示，一个简单的if分支标志就导致了2倍的性能下降。
    - 优化方法：在初始化阶段评估环境变量并存储为类属性，在forward中直接复用。
    - 这种优化对生成数千个token的推理任务尤为重要，能避免毫秒级的时间累积。
    - 体现了在框架级别进行性能优化时，需要关注看似微小但频繁执行的操作。
8. [00:22:30](https://youtu.be/2Xpovbw9M78&t=1350s) ​**集成评估与测试流程**：

    - 准确性验证是首要任务，即使性能提升明显，精度不达标也无法使用。
    - 使用vLLM的性能分析工具确认内核确实被调用，避免框架代码修改未生效的情况。
    - 针对不同批次大小进行测试，特别是注意力内核对批次大小较为敏感。
    - 使用vLLM文档推荐的基准数据集进行回归测试，收集性能数据作为参考。
    - 建议在代码修改前收集性能trace，用于检测是否引入了新的开销。
    - 强调端到端评估的重要性，因为torch compile会对整个计算图进行优化。
9. [00:25:50](https://youtu.be/2Xpovbw9M78&t=1550s) ​**端到端评估的重要性**：

    - 由于vLLM使用torch compile进行端到端优化，孤立的内核性能测试可能不具有代表性。
    - torch inductor越来越智能，有时能生成比手写内核更优的代码。
    - 需要对比自定义内核与torch compile生成的原生操作器的性能。
    - 这种对比有助于确保集成的内核确实带来了实际的性能提升。
    - 在提交PR时提供详细的基准测试数据，有助于决定是否默认启用该内核。
10. [00:26:42](https://youtu.be/2Xpovbw9M78&t=1602s) ​**完整贡献流程总结**：

     - 首先确定内核的存放位置：在vLLM内部还是外部库中。
     - 实现相应的单元测试，确保功能正确性。
     - 检查与torch compile的兼容性，不兼容时进行direct_register_custom注册。
     - 实现自定义算子类或注意力后端类。
     - 编写模块级或模型级的集成测试。
     - 进行严格的集成评估，包括准确性、性能和回归测试。
     - 所有测试通过后提交PR，等待代码审查和合并。
     - 这一标准化流程确保了vLLM生态中内核集成的质量和一致性。

## **vLLM TPU: A new unified-backend supporting Pytorch and JAX natively on TPU | Ray Summit 2025**

[https://www.youtube.com/watch?v=Ln5l7jEX2Zk](https://www.youtube.com/watch?v=Ln5l7jEX2Zk)

视频介绍：本视频由Google工程师团队分享，重点介绍了他们在vLLM（开源推理引擎）中最新推出的统一TPU后端，该后端在一周半前正式发布，原生支持JAX和PyTorch框架，旨在将TPU的竞争优势带给更广泛的用户群体。

结论：通过构建基于硬件插件架构的统一TPU后端，Google成功实现了在vLLM生态中深度集成TPU加速能力。该系统通过完全重写的XLA降低路径、优化的核心内核和SPMD编程范式，在保持与上游vLLM相同用户体验的同时，实现了5倍的性能提升。未来将重点支持Ironwood新一代TPU芯片、强化学习集成和LMD生产部署，展现了TPU在大规模语言模型推理领域的巨大潜力。

关键点：

1. [00:01:08](https://youtu.be/Ln5l7jEX2Zk&t=68s) ​**大语言模型推理的挑战与TPU优势**：

    - **模型规模与性能需求**：当前大语言模型规模持续增大，需要生成更多token，同时用户对超低延迟的要求日益严格，强化学习等复杂场景还需要训练和推理在同一循环中完成
    - **TPU硬件特性**：Google定制开发的TPU芯片经过10年迭代，具备专门针对AI工作负载优化的硬件组件

      - MXU单元：专门针对大规模矩阵乘法提供显著加速
      - ICI互连：专用网络技术，可在单个pod内连接多达9,000个TPU芯片
    - **广泛应用场景**：TPU在Google内部用于Gemini等模型的训练和推理，同时被金融服务、顶级AI实验室等云客户作为核心芯片使用
2. [00:03:10](https://youtu.be/Ln5l7jEX2Zk&t=190s)​**vLLM生态系统与TPU集成的战略意义**：

    - **vLLM的核心价值**：作为开源推理引擎，vLLM致力于构建最快、最易用的开源语言模型推理和服务引擎
    - **社区飞轮效应**：vLLM拥有强大的社区生态，模型创作者将其视为重要分发渠道，新模型发布当天即可在vLLM上获得支持
    - **硬件互操作性**：vLLM采用统一的顶层API接口，底层支持多种硬件加速器，用户可以在不同硬件间自由切换而无需更改代码
    - **TPU集成目标**：通过vLLM TPU项目将TPU的性价比优势引入vLLM生态，提高各种加速器的可获得性，让用户能够根据工作负载选择最适合的硬件
3. [00:06:44](https://youtu.be/Ln5l7jEX2Zk&t=404s) ​**新一代TPU后端的技术突破**：

    - **架构演进历程**：从2月份的原型版本到4月份的改进版本，最终在一周半前发布高性能生产就绪版本
    - **保留的核心特性**：

      - 用户级API完全兼容，无需修改任何代码即可运行上游vLLM模型定义
      - 保持硬件无关的遥测系统，确保部署的可移植性
      - 基础调度器保持不变，继续使用Python实现
    - **重大技术改进**：

      - 完全重新定义XLA降低路径，重写运行器和工作者组件
      - 优化基础内核，特别是Ragged Page Attention内核
      - 采用硬件插件设计，实现更快的迭代开发
      - 新增JAX和PyTorch原生支持
4. [00:09:31](https://youtu.be/Ln5l7jEX2Zk&t=571s) ​**性能优化与核心技术实现**：

    - **统一后端设计**：为PyTorch和JAX模型提供统一的后端支持，通过tox库将PyTorch模型转换为JAX格式，利用JAX后端获得最优性能
    - **高性能内核开发**：

      - Ragged Page Attention v3内核：相比v1版本性能提升5倍，具有更强的部署灵活性，头维度不再固定，还额外获得10%的性能提升
      - 融合内核：针对MoE模型开发，通过激进流水线技术完全重叠通信和计算，在GMMA计算期间完全重叠通信
    - **SPMD编程范式**：采用单程序多数据编程模式作为TPU的一等公民，与GPU通常使用的MPMD方法形成对比，显著提升性能
5. [00:19:17](https://youtu.be/Ln5l7jEX2Zk&t=1157s) ​**Ironwood新一代TPU与未来发展路线**：

    - **Ironwood技术规格**：

      - HBM内存带宽比前代提升4.5倍
      - 每芯片HBM容量增加6倍
      - TFLOPS提升2.5倍
      - 单个pod规模达到9,216芯片
    - **芯片定位差异**：

      - Trillium适合计算密集型工作负载和小型模型，提供市场上最具性价比的FLOPS
      - Ironwood适合大规模模型和超低延迟场景，特别擅长生成大量token的任务
    - **未来重点方向**：

      - 离线推理和超低延迟优化
      - 强化学习集成，支持JAX和PyTorch模型的分布式RL
      - LMD集成，支持更复杂的部署模式如分离式服务和前缀感知路由
      - 异构部署，支持训练器和采样器使用不同框架的混合配置
6. [00:24:33](https://youtu.be/Ln5l7jEX2Zk&t=1473s) ​**开发者资源与社区参与**：

    - **开源可用性**：所有代码已在GitHub上开源，用户可以通过vLLM项目内的TPU推理仓库开始使用
    - **多种访问途径**：除了Google Cloud，还通过TPU研究信用计划向学术界提供免费TPU资源，Google Colab也支持TPU使用
    - **社区支持**：提供Slack频道和适合初学者的入门问题，鼓励社区参与和贡献
    - **硬件限制说明**：目前TPU仅通过云服务提供，不提供本地部署选项，这突显了vLLM在提供硬件可移植性方面的重要性

## **Elastic Expert Parallelism for vLLM | Ray Summit 2025**

[https://www.youtube.com/watch?v=qE1jQSG4He0](https://www.youtube.com/watch?v=qE1jQSG4He0)

视频介绍：本视频由AnyScale公司的Ray和UC Berkeley的Yung G共同分享，详细介绍了针对视觉语言模型(vLLM)的弹性专家并行技术(Elastic Expert Parallelism)。该技术旨在解决大规模混合专家模型在在线服务中面临的扩展性和容错性挑战。

结论：弹性专家并行技术通过实现细粒度的专家组级别扩展，显著提升了大规模混合专家模型在动态负载下的成本效益和服务连续性。该架构结合了Ray集群编排、专家权重管理和渐进式CUDA图更新等优化技术，将扩展过程中的服务中断时间从数分钟降低到几乎无感知的水平。未来发展方向包括增强容错机制、自动化扩展调度以及跨硬件平台支持，为工业级大规模AI推理服务提供了可靠的技术基础。

关键点：

1. [00:01:06](https://youtu.be/qE1jQSG4He0&t=66s) **混合专家架构基础与专家并行概念**

    - **混合专家架构核心组成**：在标准Transformer层中，用一组专门的FFN网络（专家）和路由器替代原有的FFN层。每个专家都是包含训练参数子集的神经网络，而路由器是带有训练参数的门控网络，负责决定将哪些令牌路由到哪些专家进行处理。
    - **专家并行技术本质**：作为一种模型并行策略，专家并行将专家分布到多个GPU上。在没有专家并行的情况下，所有专家位于单个GPU；而采用专家并行后，每个专家可以部署在独立的GPU上，实现令牌的并行处理。
    - **现代混合专家模型特点**：通常包含数百个专家和数百亿甚至万亿参数，其中大部分参数集中在专家层。这种大规模特性使得专家并行成为必要的分布式策略。
2. [00:03:32](https://youtu.be/qE1jQSG4He0&t=212s) **大规模宽专家并行的挑战与局限**

    - **扩展单元单一化问题**：传统方法需要在模型级别进行部署或扩展，而不是在专家组级别。当在线工作负载增加时，要么部署完整的模型副本（成本效率低），要么重新部署具有新并行配置的相同模型副本（服务中断时间长）。
    - **服务中断时间分析**：重新部署过程涉及重新加载模型权重、捕获CUDA图等多个步骤，通常需要数分钟完成，严重影响用户体验。
    - **容错性挑战**：随着GPU和节点数量的增加，故障概率相应提高。单个GPU的故障可能导致整个模型服务中断，缺乏细粒度的故障隔离机制。
3. [00:05:54](https://youtu.be/qE1jQSG4He0&t=354s) **弹性专家并行架构设计原理**

    - **核心创新**：提供专家组级别的弹性和细粒度扩展能力，实现更好的成本效益来适应动态在线负载，同时显著加快扩展速度。
    - **容错增强机制**：引入检测、限制和从单个GPU故障中恢复的机制，可选择在故障容错后扩展到原始规模。
    - **架构工作流程**：通过API服务器接收扩展命令，引擎客户端触发现有引擎核心的重新初始化，同时引擎核心管理器与Ray集群交互启动新引擎核心，协调器负责请求波协调和所有引擎核心信息更新。
4. [00:08:01](https://youtu.be/qE1jQSG4He0&t=481s) **引擎编排与状态管理关键技术**

    - **引擎编排机制**：在扩展时启动额外的引擎核心，在缩减时拆除部分引擎核心，采用基于Ray的编排策略，每个引擎核心本质上是一个反应器。
    - **分布式状态管理**：状态分布在vLLM的不同组件中，包括协调器、具有数据并行通信器的引擎核心和GPU工作器。这些状态包含分布式环境、通信器、模型权重和CUDA图等关键信息。
    - **专家权重更新策略**：使用专家并行负载平衡(EPLB)机制重新平衡GPU上的专家分布，在扩展过程中使用APB进行权重传输，比从磁盘或远程存储加载更快。
5. [00:10:13](https://youtu.be/qE1jQSG4He0&t=613s) **弹性扩展过程的详细实现**

    - **扩展流程**：当vLLM引擎接收到请求时，转发给所有现有引擎核心，触发GPU工作器的重新初始化。GPU工作器执行环境重置，包括销毁旧的分布式环境和通信器、更新并行配置、重新初始化分布式环境和通信器、重新配置并运行EPLB重排。
    - **新引擎核心启动**：vLLM引擎与Ray集群交互创建新引擎核心，启动过程与正常启动类似，但关键区别在于GPU模型运行器需要先从现有引擎核心接收动态EPB信息，然后才能正确加载和重新配置模型。
    - **缩减流程**：vLLM引擎确定要保留和终止的引擎核心，对保留的核心调用重新配置，对要关闭的核心先运行APB重排，然后清理分布式通信器并自行关闭。
6. [00:13:13](https://youtu.be/qE1jQSG4He0&t=793s) **弹性扩展优化的核心技术挑战**

    - **最小化现有工作器停机时间**：需要支持通信器的异步设置、权重到新工作器的传输，以及支持包含新内核集的更新配置的CUDA图，同时分配新的通信缓冲区。
    - **新工作器快速启动**：需要新工作器快速完成启动并加入现有推理组，以便处理突发负载。
    - **当前工作流问题**：在发出扩展或缩减命令后，需要重新初始化EP通信器，在扩展情况下将注意力层权重发送到新工作器，运行EPLB重排，然后重新捕获所有CUDA图，导致现有工作器停机1-2分钟。
7. [00:15:27](https://youtu.be/qE1jQSG4He0&t=927s) **扩展成本分析与CUDA图捕获优化**

    - **成本分布分析**：从2个GPU扩展到4个GPU的30B模型实验中，重新初始化通信器和传输权重的成本相对较小，主要开销发生在重新捕获所有CUDA图时。
    - **CUDA图捕获机制**：在图形捕获期间，vLLM将模型前向包装在PyTorch CUDA图管理器上下文中，调用CUDA API执行开始捕获和结束捕获，实质上是在不执行任何实际内核的情况下执行整个模型前向逻辑。
    - **CPU操作瓶颈**：图形捕获缓慢的主要原因是CPU操作本身缓慢，在所有CUDA API之间存在大量气泡，这正是需要使用CUDA图来避免解码时间成本的原因。
8. [00:17:43](https://youtu.be/qE1jQSG4He0&t=1063s) **增量图更新与渐进式重新配置优化**

    - **增量图更新机制**：将重新捕获或更新不同批次的CUDA图的成本分摊到多个模型前向迭代中。当遇到批次大小B需要服务时，如果该批次大小的图形尚未更新，仍可使用具有原始NGPU集的旧图形计算模型前向。
    - **渐进式重新配置优势**：将权重发送和图更新等所有成本分解到多个模型前向迭代中。没有渐进式重新配置时，需要支付高昂的前期停机时间，延迟所有进行中批次的完成时间；而采用渐进式重新配置后，每个模型前向步骤的执行时间只会略微延迟，但进行中批次的平均作业完成时间可以显著改善。
    - **子图优化**：只有MO块的子图在扩展和缩减时发生变化，注意力部分是纯数据并行或可以与小型张量并行结合，注意力计算不需要重新捕获，也不需要为注意力模块分配计算缓冲区。
9. [00:20:38](https://youtu.be/qE1jQSG4He0&t=1238s) **新工作器快速启动与权重传输优化**

    - **权重获取优化**：让新工作器从对等GPU获取注意力权重和专家权重，而不是从远程存储获取。即使需要通过RDMA传输GPU内存的整个内容，传输时间仍然非常快（约2-3秒），远快于从远程存储获取。
    - **预编译内核缓存**：可以缓存Torch编译二进制文件和所有JIT内核，用于所有可能的EP大小，避免从头开始进行JIT编译。
    - **材料化CUDA图**：可以材料化所有CUDA图，从磁盘加载材料化的图形，解决新工作器上的重新配置问题。
10. [00:21:40](https://youtu.be/qE1jQSG4He0&t=1300s) **技术路线图与未来发展方向**

     - **当前进展**：已经实现弹性扩展的基本功能，目前正在优化先前讨论的技术。
     - **容错扩展**：研究如何在部分GPU禁用的情况下继续服务请求，以及当故障GPU恢复可用时如何替换。
     - **自动化调度**：构建调度能力，基于当前在线负载自动触发扩展或缩减，在给定当前性能目标和负载的情况下优化服务成本。
     - **跨平台支持**：虽然当前优化主要针对CUDA图，但类似概念也适用于其他GPU架构，如AMD GPU或TPU，这些平台都有类似的图形优化对应物。

## **Secure &amp; Scalable AI on Ray + Kubernetes: Google’s Decoupled Agent Pattern | Ray Summit 2025**

[https://www.youtube.com/watch?v=LU43Q_JktMs](https://www.youtube.com/watch?v=LU43Q_JktMs)

视频介绍：本视频由Google Cloud团队的产品经理Brandon Royal和工程师Alex Blenco在Ray峰会上的演讲，深入探讨了如何结合Ray和Kubernetes构建生产级的智能体AI系统，重点解决了分布式系统中的弹性、安全隔离和智能调度等核心挑战。

结论：通过将Ray的应用层编排能力与Kubernetes的基础设施编排及Agent Sandbox的安全隔离能力相结合，可以构建出能够处理长时间运行任务、支持大规模并行工具执行且具备故障恢复能力的生产级智能体系统。这种解耦架构不仅提供了卓越的性能和扩展性，还通过沙箱隔离确保了代码执行的安全性，为复杂智能体工作负载的工业化部署提供了完整解决方案。

关键点：

1. [00:01:09](https://youtu.be/LU43Q_JktMs?t=69) ​**智能体系统的分布式特性**：

    - 智能体系统本质上是分布式系统问题，包含三个具有不同扩展特性的子系统：

      - LLM推理服务器：专注于模型推理的扩展优化
      - 智能体编排层：管理会话、用户上下文和历史记录的有状态应用
      - 工具调用层：处理并行工具执行的无状态任务
    - 这种分布式架构需要各自独立的扩展策略，传统的单体架构无法满足生产级需求
    - Ray的自然契合性体现在其Actor模型非常适合管理有状态组件，而Task机制则完美匹配工具执行的并行需求
2. [00:03:21](https://youtu.be/LU43Q_JktMs?t=201) ​**生产级智能体的核心挑战**：

    - 持久性和状态管理：在长时间运行任务中维护任务状态和执行的持久性是一个重大挑战，智能体可能需要运行数分钟到数小时
    - 安全隔离：特别是在代码执行和计算机使用场景中，传统的沙箱方案在扩展性和性能方面存在不足
    - 独立扩展：每个子系统需要根据其特性进行独立扩展，不能采用一刀切的扩展策略
    - 计算效率：底层计算资源的高效利用对于强化学习等计算密集型任务至关重要
3. [00:06:02](https://youtu.be/LU43Q_JktMs?t=362) ​**Agent Sandbox的安全隔离解决方案**：

    - Agent Sandbox是Kubernetes开源项目，专门为智能体工作负载设计的安全隔离层
    - 核心技术特性：

      - 亚秒级延迟执行：通过预热池系统实现快速启动
      - 快照和恢复功能：在GKE环境中支持Pod状态的快照和恢复，提高计算效率
      - 开发者友好的SDK和API：为机器学习工程师提供简洁的抽象层
      - 开源架构：确保安全原则透明，支持多云环境集成
    - 支持多种隔离技术，包括gVisor和正在集成的microVM方案如Firecracker和Kata Containers
4. [00:09:24](https://youtu.be/LU43Q_JktMs?t=564) ​**解耦架构的三支柱解决方案**：

    - 弹性：通过将有状态的智能体"大脑"与无状态的临时工具解耦，避免单点故障
    - 安全：使用内核级隔离但不牺牲性能，避免为每个任务配置完整VM带来的不可接受延迟
    - 智能调度：超越简单的for循环，在集群中高效调度数千个依赖任务，管理数据局部性，避免序列化开销
    - 三层架构实现：

      - Kubernetes作为基础设施编排器，通过Agent Sandbox CRD声明式定义安全环境
      - Ray作为应用运行时，通过KubeRay Operator部署，解决调度问题
      - ADK作为客户端抽象，定义批量感知工具，隐藏复杂分布式后端
5. [00:11:27](https://youtu.be/LU43Q_JktMs?t=687) ​**股票波动性分析演示的四个阶段**：

    - 批量获取阶段：并行启动50个Ray远程网页抓取任务，每个任务处理一只股票，在7秒内完成所有数据获取
    - 批量生成阶段：基于第一阶段抓取的数据引用，并行执行代码生成任务，在15秒内完成
    - 安全执行阶段：50个Ray工作器将执行任务分发到50个由Agent Sandbox管理的gVisor沙箱中
    - 批量分析阶段：再次运行50个任务聚合每只股票的指标，最后进行整体汇总
    - 数据传递模型：使用云存储传递数据键而非数据本身，虽然引入延迟但展示了链式操作的解耦能力
6. [00:19:38](https://youtu.be/LU43Q_JktMs?t=1178) ​**扩展和可靠性考虑**：

    - 智能体扩展：需要根据使用模式确定扩展策略，类似于有状态微服务的扩展问题，但传统的内存和CPU指标可能不直接适用
    - 可靠性保障：

      - 智能体编排层通过将状态卸载到内存存储和会话存储来实现高可用性
      - 工具调用层通过Ray的重试逻辑和Ray Data的容错机制确保执行可靠性
      - ADK提供会话持久化存储和检查点机制，支持长时间工作流的恢复
    - 监控和可观测性：基于OpenTelemetry标准实现跨计算环境的完整追踪，确保在分布式架构中的端到端可见性

## **Ray Data &amp; vLLM for Scalable Image Captioning at Zoox | Ray Summit 2025**

[https://www.youtube.com/watch?v=QJeRP8kS1CM](https://www.youtube.com/watch?v=QJeRP8kS1CM)

视频介绍：本视频由Zuks机器学习平台的分布式训练负责人分享，重点介绍了如何利用Ray框架构建大规模LLM和VLM模型的后训练评估系统。演讲者从业务需求出发，详细阐述了评估工作流的技术架构选择、实现方案优化过程以及实际应用中的经验总结。

结论：通过采用Ray框架构建异构计算评估系统，Zuks成功实现了训练与评估工作流的解耦，大幅降低了模型评估对昂贵训练资源的占用。系统通过数据分区、批处理优化、状态管理和监控集成等策略，显著提升了多模态模型评估的效率和可扩展性。该方案不仅支持灵活的评估触发模式，还能无缝集成现有的自定义工具库，为大规模AI模型的工业化评估提供了可靠的技术基础。

关键点：

1. [00:01:14](https://youtu.be/QJeRP8kS1CM&t=74s) ​**后训练评估的业务需求与技术挑战**：

    - **业务背景**：作为机器人出租车公司，Zuks需要处理多种模态的输入数据，包括文本、图像、传感器数据等，并训练大量的LLM和VLM模型。
    - **核心问题**：传统的训练循环中包含验证和测试步骤，这会显著增加训练时间成本。由于使用P5N 45 48X Large和P6s 200等昂贵机器，评估步骤会挤占宝贵的训练资源。
    - **解决方案方向**：将评估工作流从训练循环中分离出来，利用成本更低的节点进行异步评估，实现训练资源的最优化配置。
    - **技术目标**：构建支持异构计算和独立扩展的评估系统，能够灵活处理不同规模的评估任务。
2. [00:03:17](https://youtu.be/QJeRP8kS1CM&t=197s) ​**评估工作流的技术架构选择与设计原则**：

    - **评估触发模式**：

      - 同步模式：训练完成后启动后处理作业，对所有检查点进行全面评估
      - 选择性评估：基于训练指标筛选部分优质检查点进行重点评估
      - 异步模式：训练过程中检查点生成时立即触发评估，不等待训练结束
    - **架构设计原则**：

      - 异构计算：CPU负责数据读写和后处理，GPU专注模型推理
      - 独立扩展：各处理阶段可根据需求独立扩展计算资源
      - 成本优化：使用廉价节点进行评估，保留昂贵机器用于核心训练任务
    - **技术选型依据**：Ray框架提供了完善的生态系统，能够统一管理不同类型的计算任务，支持灵活的资源配置和扩展。
3. [00:05:22](https://youtu.be/QJeRP8kS1CM&t=342s) ​**基于Ray Data LLM的技术实现方案**：

    - **处理流程架构**：

      - 输入层：处理图像、视频等多模态数据
      - 批处理层：利用Ray Data的批处理机制优化数据处理效率
      - 模型推理层：VLM模型推理，支持多种模型接口
      - 输出层：结果存储到S3或进行后续处理
    - **技术优势**：

      - 统一的异构计算环境，支持CPU和GPU任务协同工作
      - 与现有工具链的无缝集成，包括自定义LLM评估工具包和VLM评估套件
      - 支持OpenAI兼容端点，满足不同工程师的使用习惯
      - 灵活的扩展能力，各处理阶段可独立调整资源配比
    - **平台兼容性**：作为平台工程师，能够在不要求用户重写代码的前提下，为不同工作流程提供统一的扩展方案。
4. [00:12:00](https://youtu.be/QJeRP8kS1CM&t=720s) ​**实际演示与监控优化实践**：

    - **基础流水线构建**：

      - 四步处理流程：加载数据集、创建LLM处理器、数据标注、后处理
      - 标注任务示例：基于机器人出租车采集的场景数据，识别物体、行人、交通决策等关键信息
    - **监控系统集成**：

      - 配置Prometheus和Grafana进行实时监控
      - Ray Dashboard提供任务执行的可视化洞察
      - 需要解决VLM指标与Ray指标的统一展示问题
    - **指标集成挑战**：

      - VLM和Ray使用不同的端口输出指标
      - 需要通过Ray Prometheus Stats Logger进行指标格式转换和统一
      - 关键监控指标包括token吞吐量、缓存利用率、token延迟等
5. [00:19:41](https://youtu.be/QJeRP8kS1CM&t=1181s) ​**性能优化策略与流水线演进**：

    - **第一代流水线优化**：

      - 数据重分区：根据处理需求将数据集分配到多个工作节点
      - 批处理优化：使用map_batches替代简单的map操作
      - 资源配置：为每个工作节点分配特定数量的CPU资源
    - **第二代流水线改进**：

      - 处理阶段分离：将预处理和后处理与核心推理任务完全分离
      - 并发控制：通过num_cpus和concurrency参数精细控制资源使用
      - 完全控制：绕过默认的Ray构造器，直接指定处理逻辑
    - **性能提升效果**：通过优化批大小、VLM端的chunk prefill等技术，显著提升了处理效率。
6. [00:24:52](https://youtu.be/QJeRP8kS1CM&t=1492s) ​**高级优化与自定义集成方案**：

    - **状态管理优化**：

      - 使用带状态管理的Actor类，避免重复初始化处理器
      - 对于Qwen 2.5 VL等特定模型，集成专用的处理器库
      - 通过类初始化确保处理器只加载一次，提升效率
    - **自定义处理集成**：

      - 实现chat_template和process_vision等关键方法
      - 完全控制提示词构建和多模态输入处理
      - 绕过默认的tokenization和图像处理流程
    - **技术深度集成**：需要深入理解VLM Stage和VLM Processor的内部机制，以及多模态输入的文档规范。
7. [00:27:22](https://youtu.be/QJeRP8kS1CM&t=1642s) ​**适用场景总结与最佳实践**：

    - **适用场景**：

      - 离线批量处理任务，关注吞吐量而非单次请求延迟
      - 大规模数据处理（超过千张图像），能够抵消框架开销
      - 不需要自定义CUDA内核的预处理任务
    - **不适用场景**：

      - 对单次请求延迟敏感的应用场景
      - 小规模任务（千张图像以下），框架开销可能超过收益
      - 需要在GPU上运行自定义CUDA内核的预处理任务
    - **实施建议**：

      - 从小的基线开始构建，逐步优化
      - 充分利用Ray Dashboard和Grafana进行监控分析
      - 分离关注点：预处理、后处理、GPU工作负载应该独立管理
      - 通过状态管理避免频繁的Hugging Face API调用，防止速率限制错误

## **How BMW Scales Automotive AI Workloads with the Ray Framework | Ray Summit 2025**

[https://www.youtube.com/watch?v=s3x3STd5vuU](https://www.youtube.com/watch?v=s3x3STd5vuU)

视频介绍：本视频由BMW集团的Thomas分享，详细介绍了BMW如何利用Ray框架在汽车行业中扩展AI工作负载。重点阐述了BMW Connected AI平台的架构演进历程，从最初的Kubeflow解决方案到基于Ray的现代化平台转型，并深入分析了在模型训练和模型服务两个关键领域的具体实践案例。

结论：BMW通过采用Ray框架成功解决了原有Kubeflow平台在基础设施复杂性、生成式AI支持和大规模训练扩展方面的局限性。平台转型的核心在于实现集中化管理的基础设施、提供可扩展的构建模块，并通过工作流API平衡用户自主权与平台治理。在语音助手训练案例中实现了从过夜训练到分钟级训练的显著加速，在大语言模型自托管方面成功部署了6850亿参数的模型。未来平台将扩展到大数据处理、多模型部署和强化学习等方向，展现了Ray框架在工业级AI系统中的强大应用价值。

关键点：

1. [00:01:25](https://youtu.be/s3x3STd5vuU&t=85s) ​**Connected AI平台概述与目标**：

    - **平台定位**：Connected AI平台是BMW的中央机器学习和AI平台，支持超过60个不同的机器学习用例，包括视频中展示的智能充电盖板等创新功能
    - **部署模式**：平台支持车载部署和云端部署两种模式，覆盖完整的端到端机器学习生命周期，从数据访问到最终应用监控
    - **核心目标**：通过自动化实现加速，缩短机器学习模型上市时间，快速创造业务价值。平台在全球主要市场（美国、欧洲、亚洲）进行扩展，目前服务于550多名开发者和数据科学家
    - **技术演进**：平台最初于2022年上线，专注于经典机器学习用例，采用Kubeflow解决方案部署在EKS上，但随着需求变化和技术发展，正在向基于Ray的新架构转型
2. [00:03:59](https://youtu.be/s3x3STd5vuU&t=239s) ​**原有平台的挑战与转型动因**：

    - **基础设施复杂性**：分散式部署模式导致数据科学团队需要自行维护Kubernetes集群，许多团队缺乏足够的基础设施技能，难以遵循最佳实践，支持维护成本高昂
    - **生成式AI需求**：2023年生成式AI热潮兴起，团队要求扩展到更多GPU和节点进行训练，而原有Kubeflow堆栈在生产环境中无法有效满足这些需求
    - **技术局限性**：Kubeflow工具链对生成式AI的支持不足，促使团队寻求新的技术解决方案。这些因素共同推动了平台的全面重构和重新架构决策
    - **转型时机**：转型工作于2024年底启动，定义了集中化管理基础设施、提供可扩展构建模块和保持统一生态系统等基本范式
3. [00:08:21](https://youtu.be/s3x3STd5vuU&t=501s) ​**Ray在模型训练中的实践应用**：

    - **试点用例**：BMW语音助手项目作为Ray应用的试点案例，团队需要通过监督微调训练transformer模型，支持德语和英语等多种语言数据集
    - **性能挑战**：原有训练过程耗时数天，严重拖慢开发进度。核心需求是将训练分布到多个GPU上加速，同时构建完整的端到端部署流水线
    - **技术架构**：在EKS上部署Kubernetes集群，使用KubeRay操作符和Karpenter进行高级节点配置和自动扩缩容。数据科学团队通过GitHub仓库管理训练代码，能够按需部署具有头节点和多个GPU工作节点的Ray集群
    - **集成生态**：实现Ray作业与MLflow实验跟踪的集成，支持模型指标和损失功能监控。训练好的模型存储在S3中并在MLflow中注册，最终通过JFrog Artifactory进行产品化打包，通过空中下载更新部署到车辆中
4. [00:12:04](https://youtu.be/s3x3STd5vuU&t=724s) ​**试点项目成果与经验总结**：

    - **性能提升**：通过数据并行技术在8个GPU上分布训练，实现了高效的资源利用，训练时间从过夜大幅缩短到几分钟或几小时，具体取决于模型复杂度
    - **配置管理**：在Ray集群配置中实现了平台治理与用户自主权的平衡。平台设置的参数包括命名空间隔离、强制缩容以节省成本、服务账户配置等，用户可配置参数包括Karpenter注解以配置合适的GPU节点
    - **成本优化**：通过强制缩容机制防止客户启动大量未使用的GPU资源产生闲置成本，体现了平台在资源管理和成本控制方面的重要考量
    - **扩展规划**：基于试点经验，平台正在构建工作流API作为生产级产品，计划在2026年初迁移首批用例
5. [00:14:14](https://youtu.be/s3x3STd5vuU&t=854s) ​**生产级工作流API设计要点**：

    - **抽象复杂性**：通过中央API抽象所有基础设施复杂性，数据科学家无需关心底层基础设施维护。API规范具有灵活性，提供大量默认值，同时支持初级和高级用户
    - **存储优化**：解决EBS卷被日志填满导致训练崩溃的问题，考虑在生产环境中使用S3和EFS挂载。在Ray集群与Ray作业之间做出权衡，生产环境更适合使用Ray作业，能够创建临时集群并在批处理工作负载完成后清理
    - **模板化支持**：针对重复性作业提供预定义模板选择，客户可以直接使用。同时支持构建多步骤复杂流水线，满足不同用例的多样化需求
    - **多租户隔离**：通过Kubernetes命名空间实现多租户隔离，确保每个用例只能访问正确的数据。架构包括中央API、工作流状态数据库和模板库，后端通过Argo工作流操作符编排工作流执行
6. [00:18:53](https://youtu.be/s3x3STd5vuU&t=1133s) ​**基于Ray Serve的模型服务实践**：

    - **大语言模型自托管**：通过黑客松活动探索大语言模型自托管，使用DeepSeek R1 T2 Heimera模型（6850亿参数），在16位浮点精度下需要超过1TB的GPU显存
    - **并行化策略**：使用16个H100 GPU分布在两个p5dn.48xlarge节点上，采用流水线并行（大小为2）在节点间分布模型，并在每个节点上使用张量并行在8个GPU间分布张量操作
    - **性能表现**：推理时间约28秒处理1500个token，达到约50 token/秒的吞吐量，满足大多数应用需求。部署时间包括从Hugging Face下载在内不到40分钟，使用AWS竞价实例的运营成本约为每小时60美元
    - **集成能力**：Ray Serve与OpenAI API规范兼容，自托管模型可以无缝集成到现有系统中，如GitHub Copilot代码生成工作流。还尝试了多模型服务，成功部署了小型视觉语言模型
7. [00:24:06](https://youtu.be/s3x3STd5vuU&t=1446s) ​**平台未来发展展望**：

    - **功能扩展**：大语言模型自托管将在2026年成为客户功能，模型微调预计会成为机器学习团队的常见需求。多模型服务和新的Ray优化技术将进一步提升效率
    - **成本考量**：考虑到BMW有来自车辆的数百万请求，自托管模型可能比按token付费的AWS Bedrock更经济。对于敏感客户数据用例，自托管也可能是必要选择
    - **技术方向**：将采用完全Python优先的方法使用Ray Data进行大数据处理，建立中央特征存储用于批处理和流式数据源。探索更复杂的多模型部署和模型组合功能
    - **创新应用**：计划集成自定义视觉语言模型和语音模型，探索强化学习应用，利用客户的人类反馈数据进行后训练，实现已部署模型的重新训练和最终客户反馈闭环

## **Inside NVIDIA Dynamo: Faster, Scalable AI Deployment | Ray Summit 2025**

[https://www.youtube.com/watch?v=74MUe65_P1g](https://www.youtube.com/watch?v=74MUe65_P1g)

视频介绍：本视频由NVIDIA推理服务团队的Harry分享，重点介绍了他们最新开发的推理服务堆栈Dynamo。作为对传统Triton方案的革新，Dynamo专门针对大语言模型推理的独特挑战而设计，通过系统级优化和模块化架构，为生产级AI推理提供高性能解决方案。

结论：Dynamo通过三个核心基础创新——智能调度、内存管理和数据传输，为大语言模型推理带来了革命性的性能提升。其独特的分离式服务技术能够将预填充和解码阶段分配到不同GPU，在扩展时实现超线性性能增长。结合KV路由、KV块管理和Nixl数据传输等系统级优化，Dynamo在Llama 70B等大型模型上展示了30%到2倍以上的性能提升。模块化设计使得开发者可以灵活选用各个组件，而生产级服务功能如AI配置器、动态规划器和Grove调度器，则确保了从配置到生产的无缝过渡。Dynamo代表了下一代AI推理服务的发展方向，为大规模模型部署提供了完整的解决方案。

关键点：

1. [00:01:15](https://youtu.be/74MUe65_P1g&t=75s) ​**Dynamo的诞生背景与设计理念**：演讲者详细阐述了从Triton转向Dynamo的根本原因和全新设计思路。

    - **范式转变的需求**：随着DeepSeek R1等大型模型的出现，传统推理服务在计算、内存和扩展方面面临严峻挑战。这些模型对计算资源和内存带宽提出了更高要求，同时在大规模部署时遇到了路由和调度的瓶颈。
    - **系统性方法的重要性**：Dynamo采用了更加系统化的设计理念，专注于构建坚实的基础设施，包括调度、内存管理和数据传输三个核心支柱。这种基础优先的方法确保了系统能够适应未来可能出现的新技术和新需求。
    - **AI操作系统的定位**：正如Jensen在GTC主题演讲中提到的，Dynamo被定位为AI操作系统，这反映了其在AI推理基础设施中的核心地位和全面性。
2. [00:04:56](https://youtu.be/74MUe65_P1g&t=296s) ​**分离式服务的核心技术价值**：深入分析了分离式服务的工作原理及其带来的性能突破。

    - **预填充与解码的差异性**：大语言模型推理包含两个截然不同的阶段——预填充阶段（计算密集型）和解码阶段（内存密集型）。这种差异使得将它们分离到不同GPU成为可能。
    - **性能解锁机制**：通过分离这两个阶段，系统可以独立扩展预填充和解码资源，避免了相互干扰，同时能够为每个阶段应用最适合的并行策略。
    - **扩展优势的放大效应**：在单节点部署中，分离式服务带来约30%的性能提升，而在双节点部署中，每GPU吞吐量提升超过2倍。这种超线性扩展打破了传统的线性扩展定律。
3. [00:10:19](https://youtu.be/74MUe65_P1g&t=619s) ​**KV路由的智能调度创新**：详细介绍了KV路由算法的工作原理和实际效果。

    - **双因素决策机制**：KV路由算法综合考虑KV缓存命中率和KV缓存负载两个关键因素，智能地将请求路由到最能提供最佳性能的工作节点。
    - **实际性能表现**：客户案例显示，在使用Qwen 3 480B编码器模型时，仅通过KV路由就实现了TTFT（首令牌时间）2倍加速和TPS（每秒令牌数）1.6倍提升。
    - **持续扩展方向**：团队正在将KV路由与分离式服务结合，并扩展支持多级内存层次结构，进一步提升系统的灵活性和性能。
4. [00:12:27](https://youtu.be/74MUe65_P1g&t=747s) ​**KV块管理的多层次内存优化**：阐述了KV缓存管理的技术实现和业务价值。

    - **内存层次结构的充分利用**：KV块管理器支持从GPU内存（HBM）到主机内存、本地SSD再到网络存储的完整内存层次结构，最大化存储资源利用率。
    - **命中率提升效果**：实际客户案例显示，从仅使用HBM扩展到包含主机内存后，KV缓存命中率从30%提升到70%，显著改善了系统效率。
    - **应用场景的广泛性**：该技术适用于多轮对话、代码生成、智能体调用等多种场景，通过智能的卸载和预取策略优化整体性能。
5. [00:15:52](https://youtu.be/74MUe65_P1g&t=952s) ​**Nixl数据传输的技术优势**：介绍了Nixl在数据传输方面的独特价值。

    - **统一API设计**：Nixl提供了统一的API，既支持GPU间的对等传输，也支持跨不同内存层次结构的数据传输，简化了开发复杂度。
    - **动态扩展能力**：与传统的通信库不同，Nixl不设固定的世界大小，支持在运行时动态添加或移除工作节点，为弹性扩展提供了基础。
    - **容错性支持**：在专家混合模型部署中，Nixl支持故障节点的动态移除和替换，确保系统的稳定运行。
6. [00:19:43](https://youtu.be/74MUe65_P1g&t=1183s) ​**生产级服务的完整工具链**：展示了从配置到生产的全流程工具支持。

    - **AI配置器的自动化优化**：通过分析内核执行时间等数据，AI配置器能在1分钟内在笔记本电脑上找出最佳部署配置，大幅减少了传统手动调优所需的时间和资源。
    - **动态规划器的实时调整**：基于实时流量模式，动态规划器能够自动调整预填充和解码资源的比例，在阿里云ACK的测试中实现了80%的SLA违规减少和5%的GPU使用优化。
    - **Grove调度器的拓扑感知**：克服了Kubernetes领导工作节点集的限制，支持预填充和解码资源的独立动态扩展，同时考虑拓扑位置优化性能。
7. [00:26:38](https://youtu.be/74MUe65_P1g&t=1598s) ​**细粒度容错机制**：详细介绍了针对大语言模型推理的特殊容错需求设计的解决方案。

    - **路由器状态共享**：通过在多路由器间共享状态，确保单个路由器故障不会影响系统整体运行。
    - **请求取消支持**：支持在推理过程中取消请求，避免在工具调用等场景中产生不必要的计算浪费。
    - **请求迁移能力**：当工作节点出现故障时，系统能够自动将请求迁移到健康节点，保证服务连续性。
    - **过载保护机制**：通过请求削减等机制防止系统过载，确保在高负载情况下的稳定运行。

## **Inside Adobe Firefly: JIT-Embedding with Ray Serve for Faster GenAI Training | Ray Summit 2025**

[https://www.youtube.com/watch?v=RA5GRNJOW2w](https://www.youtube.com/watch?v=RA5GRNJOW2w)

视频介绍：本视频由Adobe Firefly AI平台团队的工程师Han和Bachelo分享，详细介绍了他们如何利用Ray Serve框架构建"即时嵌入服务"(Just Embedding Service)，以支持Adobe Firefly大规模多模态基础模型的训练工作。

结论：Adobe通过构建基于Ray Serve的即时嵌入服务，成功解决了大规模基础模型训练中嵌入计算的效率与灵活性平衡问题。该系统通过专门的嵌入服务集群、优化的媒体编码传输、客户端负载均衡和预取机制，实现了训练GPU资源的充分利用，显著提升了迭代速度并降低了成本。该方案已成功支撑Adobe Firefly产品线中图像、视频、音频等多种模态的基础模型训练，展现了在工业级AI系统中专业化服务架构的重要价值。

关键点：

1. [00:03:02](https://youtu.be/RA5GRNJOW2w?t=182) ​**嵌入服务的技术背景与挑战**：

    - **嵌入计算的核心地位**：从潜在扩散模型到扩散变换器(DiT)，嵌入已成为多模态生成模型的通用接口，在训练工作流中高效运行嵌入计算至关重要。
    - **两种嵌入生成方式的权衡**：实时生成嵌入提供最大灵活性，但会消耗宝贵的GPU资源（包括显存和训练时间）；离线预计算嵌入在训练时最有效，但无法应对VE（变分自编码器）在训练期间频繁变更的问题。
    - **实际训练中的动态需求**：在大规模基础模型训练阶段，VE通常会变更多次，每次变更都需要重新计算所有训练数据的嵌入，使得完全的预计算方案不可行。
    - **平衡点的寻找**：在快速发展的生成式AI领域，需要在灵活性、速度和成本之间找到最佳平衡，这正是即时嵌入服务要解决的核心问题。
2. [00:05:39](https://youtu.be/RA5GRNJOW2w?t=339) ​**即时嵌入服务的架构设计**：

    - **专业化服务理念**：借鉴特征服务的概念，将嵌入计算委托给专门的服务，训练机器可以异步检索嵌入，避免昂贵的训练GPU运行小型模型。
    - **框架选择依据**：选择Ray Serve主要基于其在服务器端的灵活性、无状态计算能力以及易于部署的特性。
    - **客户端集成方案**：与PyTorch数据加载器深度集成，利用预取机制来分摊服务调用成本。
    - **数据传输设计**：客户端直接发送媒体数据到服务端，服务端运行VE或其他模型后直接返回张量给加载器组件。
3. [00:09:44](https://youtu.be/RA5GRNJOW2w?t=584) ​**媒体编码与序列化的深度优化**：

    - **编码方案选择**：基于Adobe在媒体处理领域的专业经验，为不同模态选择最优编解码器——图像使用JPEG、视频使用H.264、音频使用MP3或WAV格式。
    - **Rust技术栈的优势**：采用基于Rust的序列化框架，利用当时最快的JPEG解码库，实现零拷贝反序列化和向量化优化。
    - **同构编解码要求**：客户端和服务端必须使用相同的编解码器，这要求双方的技术栈保持一致。
    - **成本优化导向**：所有优化都围绕降低开销展开，避免训练机器在嵌入计算上花费过多时间。
4. [00:12:08](https://youtu.be/RA5GRNJOW2w?t=728) ​**高延迟挑战与预取机制**：

    - **VE模型的高延迟特性**：为追求重建质量而使用的高质量变分自编码器具有较高延迟，在服务器拥堵时请求可能延迟数秒。
    - **GPU资源浪费风险**：如果训练过程需要等待嵌入计算，昂贵的GPU将处于空闲状态，造成巨大的资源浪费。
    - **顺序确定性优势**：利用内部基础设施提供的顺序确定性，可以预先知道数据点的加载顺序，实现主预取机制。
    - **大规模并发隐藏延迟**：通过服务数百个GPU的大规模并发来分摊和隐藏延迟，确保训练过程不会因嵌入计算而停滞。
5. [00:15:11](https://youtu.be/RA5GRNJOW2w?t=911) ​**异构环境下的客户端负载均衡**：

    - **异构性挑战**：训练负载运行在H100/H200等高端GPU上，而嵌入服务运行在获取到的各种GPU上，形成异构硬件环境。
    - **客户端负载均衡设计**：从头节点获取服务器清单，客户端根据自身需求直接选择服务器，避免通过头节点路由大型请求。
    - **设计优势**：

      - 避免单点瓶颈：不通过头节点中继大型媒体请求
      - 客户端智能决策：客户端最了解自己的需求状态和媒体类型
      - 策略灵活性：支持后续实现各种负载均衡策略
    - **业务价值**：在保证为客户提供最佳产品的同时，实现成本节约的目标。
6. [00:17:56](https://youtu.be/RA5GRNJOW2w?t=1076) ​**序列化框架的数学抽象与优化**：

    - **组合函数的技术债务**：传统的序列化反序列化配对在频繁变更编解码器时容易出错，需要同步修改客户端和服务端代码。
    - **范畴论启发**：受Haskell范畴理论和同构概念启发，将编解码操作抽象为同构关系。
    - **同构组合模式**：LZ4压缩是字节间的同构，请求打包是任意负载到请求格式的同构，通过直接组合这些同构获得双向函数。
    - **开发效率提升**：这种数学抽象实现了大量自动化，减少了手动组合的错误，提高了开发效率。
7. [00:20:11](https://youtu.be/RA5GRNJOW2w?t=1211) ​**实际成效与未来发展方向**：

    - **业务成果**：该方案已成功支撑投入生产的Firefly基础模型，实现了更快的迭代速度、成本节约和高效的服务部署。
    - **高分辨率工作负载支持**：特别优化了那些VE计算变得昂贵的高分辨率工作负载。
    - **技术演进方向**：

      - RDMA支持：利用Ray新提供的RDMA功能直接发送请求，避免HTTP开销
      - 服务扩展：将类似架构应用于VM（视觉模型）等其他服务场景
      - 计算图网络：未来可能直接发送计算图而非媒体数据
    - **团队协作价值**：强调这种大规模基础模型训练的变更需要跨团队的紧密协作和低层优化。

## **Scaling Ray on Kubernetes: Pragmatic Strategies for Every Team | Ray Summit 2025**

[https://www.youtube.com/watch?v=W65Vzqjidrg](https://www.youtube.com/watch?v=W65Vzqjidrg)

视频介绍：本视频由Google GKE产品经理工程师Roger和软件工程师Spencer共同分享，详细介绍了在Kubernetes上优化Ray集群规模与性能的最佳实践。内容涵盖集群生命周期管理、资源优化、性能调优和可观测性等关键主题。

结论：通过结合Kubernetes和Ray的优势，企业能够构建高性能、可扩展的AI/ML平台。视频展示了从集群配置、资源分配到故障排查的完整解决方案，特别强调了动态资源分配、自动扩缩容、节点启动优化和作业调度等关键技术。Google与Anyscale的合作进一步推动了这一生态系统的发展，为大规模AI工作负载提供了企业级的部署和管理方案。

关键点：

1. [00:02:42](https://youtu.be/W65Vzqjidrg&t=162s) ​**集群生命周期管理的三个阶段**：演讲者将Ray on Kubernetes的集群生命周期划分为三个关键阶段，每个阶段都有其独特的挑战和解决方案。

    - **启用和配置阶段**：重点在于为开发者提供无缝的体验，包括快速部署、资源声明和基础设施配置。这个阶段需要考虑如何让开发者能够轻松地声明他们需要的资源类型，包括GPU、TPU等加速器，并通过Kubernetes的原生机制来实现资源的动态分配。
    - **性能和扩展阶段**：当工作负载开始扩展时，性能和规模成为关键考量。这包括优化资源调度、处理大规模工作负载的挑战，以及确保系统在高负载下的稳定性。演讲者特别提到了高级调度功能如DRRA（动态资源分配）和RDMA技术的重要性。
    - **故障排查和可观测性阶段**：在日常运维中，能够快速诊断和解决问题至关重要。这需要将Kubernetes和Ray的监控数据关联起来，提供统一的视图来理解系统行为，识别性能瓶颈和故障根因。
2. [00:06:04](https://youtu.be/W65Vzqjidrg&t=364s) ​**临时集群与持久化集群的部署模式**：深入探讨了两种主要的Ray集群部署模式及其适用场景。

    - **临时集群的优势**：适用于批处理训练、CI/CD测试等短期任务，任务完成后集群自动销毁。这种模式避免了升级维护的复杂性，资源分配更加灵活，特别适合不需要长期运行的实验性工作负载。
    - **持久化集群的价值**：更适合模型服务和长期实验场景，能够保持环境稳定性，简化调试过程。但需要仔细考虑资源隔离、安全策略和邻居干扰等问题，确保不同工作负载之间的性能隔离。
    - **多集群管理策略**：通过命名空间隔离、资源配额管理和权限控制，可以在单个Kubernetes集群上运行多个Ray集群，实现资源的有效共享和隔离。
3. [00:11:48](https://youtu.be/W65Vzqjidrg&t=708s) ​**KubeRay的关键作用与优化价值**：KubeRay作为连接Ray和Kubernetes的关键组件，在平台优化中发挥着核心作用。

    - **资源调度优化**：通过标签调度、自动扩缩容和动态资源管理，KubeRay能够更有效地匹配工作负载需求与可用资源，减少资源碎片化和空闲成本。
    - **运维自动化**：简化了集群的清理流程、节点故障处理和资源回收，提高了系统的可靠性和运维效率。
    - **可观测性集成**：GKE的托管插件能够自动关联Kubernetes事件和Ray作业事件，提供统一的故障诊断视图，帮助快速定位问题根源。
4. [00:12:37](https://youtu.be/W65Vzqjidrg&t=757s) ​**性能优化的四个关键领域**：Spencer详细介绍了优化Ray on Kubernetes性能的具体技术方案。

    - **动态资源分配**：通过DRA技术实现GPU资源的细粒度共享和拓扑感知调度，显著提高了加速器的利用效率，同时确保最佳的性能表现。
    - **自动扩缩容改进**：Ray Autoscaler v2与KubeRay的深度集成，结合原地Pod调整和垂直Pod自动扩缩容，实现了更精准的资源分配和更快的响应时间。
    - **节点启动优化**：通过镜像流式传输、模型预加载和缓存策略，将节点启动时间从可能长达20分钟缩短到几分钟，显著提高了资源利用率。
    - **作业调度与配额管理**：使用Kueue进行批量作业调度，支持优先级调度、资源借用和抢占策略，确保关键工作负载能够及时获得所需资源。
5. [00:19:15](https://youtu.be/W65Vzqjidrg&t=1155s) ​**开发者体验的持续改进**：从集群访问到任务调度的全方位优化，提升开发者的工作效率。

    - **灵活的集群访问方式**：提供从简单的kubectl端口转发到负载均衡器暴露的多种访问方案，满足不同安全性和便利性需求。
    - **标签调度API**：将Kubernetes原生的标签选择器概念引入Ray调度，使开发者能够直接在工作代码中指定计算资源偏好和回退策略。
    - **统一的可观测性**：结合Ray Dashboard和Kubernetes监控栈，为开发者和平台管理员提供不同层次的系统视图，便于快速定位和解决问题。
6. [00:23:18](https://youtu.be/W65Vzqjidrg&t=1398s) ​**端到端工作流整合与未来展望**：展示了各项技术如何协同工作，形成完整的解决方案。

    - **工作流协调**：Kueue作业调度、标签调度和资源分配的深度集成，确保复杂工作负载能够高效地在共享基础设施上运行。
    - **故障诊断改进**：通过事件关联和历史数据追踪，解决了临时集群故障诊断的挑战，提供了更完善的运维支持。
    - **生态系统演进**：Ray历史服务器等新功能的开发，将进一步增强系统的可观测性和故障诊断能力，为未来的大规模部署提供更强支撑。

## **How AWS Scales Reinforcement Learning Across Thousands of GPUs | Ray Summit 2025**

[https://www.youtube.com/watch?v=hNEcpYy_pLw](https://www.youtube.com/watch?v=hNEcpYy_pLw)

视频介绍：本视频由AWS的AI专家Anup和Kunal共同分享，重点探讨了如何利用AWS SageMaker HyperPod平台来扩展后训练工作负载，特别是在数千甚至数万GPU规模上优化强化学习等后训练任务。演讲深入分析了当前后训练领域的技术挑战、HyperPod的核心架构设计，以及实际应用中的性能优化策略。

结论：AWS SageMaker HyperPod通过构建弹性的、异构的、高效的AI训练平台，成功解决了大规模后训练工作负载中的基础设施挑战。该平台通过深度健康检查、自动节点替换、分布式内存存储、快速检查点恢复等创新功能，将训练时间减少了40%以上，检查点保存时间提升了2-3倍，模型加载时间提升了4-5倍。结合Kubernetes控制平面和Ray计算引擎，HyperPod为企业在快速演进的AI领域提供了灵活、可靠的规模化训练解决方案，已被Amazon自身用于Nova模型的训练，展现了其在工业级AI系统中的重要价值。

关键点：

1. [00:01:34](https://youtu.be/hNEcpYy_pLw&t=94s) ​**后训练规模化的重要性和挑战**：演讲者阐述了为什么2025年被称为"能力之年"而非"智能体之年"，并深入分析了规模化后训练面临的核心挑战。

    - **能力之年的背景**：当前AI发展的重点已经从智能体转向模型能力的全面提升，特别是在数学、物理等科学领域的突破性表现，这些能力的提升主要依赖于后训练的规模化扩展。
    - **强化学习的关键作用**：大多数客户通过基于强化学习的后训练来实现模型能力的显著提升，这需要大规模的计算资源支持。
    - **技术栈的演进状态**：后训练技术栈仍在快速演进中，Ray已成为主流计算引擎，支持包括Axol、Skyline等在内的多个流行框架。
    - **异构集群管理挑战**：实际部署中通常混合使用不同代的GPU（如Hopper、Blackwell）和CPU实例，如何在这种异构环境中优化性能是一个重要挑战。
    - **熵管理复杂性**：包括基础设施熵、工作负载熵和部署策略熵在内的多重不确定性因素，对后训练的效果产生显著影响。
2. [00:06:47](https://youtu.be/hNEcpYy_pLw&t=407s) ​**SageMaker HyperPod的核心架构与设计理念**：详细介绍了HyperPod作为专门优化的AI训练平台的设计目标和核心组件。

    - **平台定位**：HyperPod是专门为模型训练和推理优化的基础设施AI平台，最初为预训练设计，但发现对后训练工作负载的优化效果更为显著。
    - **核心目标**：以最低成本获得最佳模型，通过最大化GPU利用率和集群"良品吞吐量"来实现这一目标。
    - **三大支柱设计**：

      - 弹性：通过主动健康检查、持续监控和自动节点替换机制，将GPU故障完全从用户考虑范围内移除。
      - 灵活性：采用非固化的平台设计，支持各种训练框架、服务框架和分布式训练方案。
      - 效率：通过优化的计算、存储和网络架构，显著提升训练效率。
    - **技术实现**：基于Kubernetes控制平面，通过KubeRay操作符管理Ray集群，支持创建具有不同头节点和工作节点的异构集群。
3. [00:12:46](https://youtu.be/hNEcpYy_pLw&t=766s) ​**分布式内存存储与检查点优化**：重点介绍了HyperPod在检查点管理方面的创新技术突破。

    - **检查点开销分析**：传统检查点操作占用约10%的训练开销，主要因为同步操作和故障恢复时的模型加载时间。
    - **分布式内存存储架构**：新推出的分离式内存存储将检查点从GPU内存卸载到CPU内存，并在不同节点间自动复制分片。
    - **性能提升效果**：

      - 检查点保存时间提升2-3倍
      - 模型加载时间提升4-5倍
      - 支持更频繁的检查点保存，减少故障时的进度损失
    - **容灾机制**：除了节点间复制外，还定期将检查点持久化到S3等持久存储，应对灾难性故障场景。
    - **实际收益**：客户可以更频繁地进行检查点保存，同时显著降低检查点相关的总体开销。
4. [00:15:30](https://youtu.be/hNEcpYy_pLw&t=930s) ​**进程级重启与集群效率优化**：探讨了在大规模集群中如何通过精细化的故障恢复机制提升整体效率。

    - **大规模集群的挑战**：在15,000+ GPU规模的集群中，日常运行中会频繁出现各种故障，影响训练连续性。
    - **进程级重启机制**：通过保持Pod或容器存活，仅重启故障进程的最小恢复单元，将进程级开销从7.5%显著降低。
    - **总体效率提升**：结合检查点优化和进程级重启，将总体开销从15-17%大幅降低，显著提升集群良品吞吐量。
    - **快速恢复能力**：进程级重启能够实现秒级的故障恢复，最大限度减少GPU资源的闲置时间。
5. [00:17:20](https://youtu.be/hNEcpYy_pLw&t=1040s) ​**自动扩展与基础设施管理**：介绍了HyperPod在集群管理和自动扩展方面的先进特性。

    - **快速资源供应**：平台自动处理基础设施供应，在资源不可用时持续重试，用户只需提交资源需求状态。
    - **集成自动扩展**：通过托管Karpenter提供节点自动扩展能力，支持扩展到零，完全由平台管理安装和运维。
    - **智能调度优化**：自动标记网络拓扑信息，调度器可以利用这些信息确保工作负载在优化的网络配置中运行。
    - **多场景支持**：支持包括Ultra集群和GB200在内的不同硬件配置，为不同工作负载提供最佳的网络和计算环境。
6. [00:20:41](https://youtu.be/hNEcpYy_pLw&t=1241s) ​**端到端后训练解决方案实践**：展示了基于HyperPod构建的完整后训练解决方案架构和最佳实践。

    - **参考架构设计**：提供完整的GitHub参考实现，支持一键创建优化的后训练集群。
    - **混合部署模式**：支持在HyperPod实例上进行训练，在Spot实例或EC2实例上进行推理的灵活部署方式。
    - **统一可观测性**：集成Prometheus和Grafana，提供基础设施和工作负载的全面监控，包括定制的Ray仪表板增强。
    - **生产就绪特性**：支持优先级调度、资源争用管理，以及网络和存储瓶颈的实时监控。
    - **实际应用验证**：该架构已被Amazon用于Nova模型的训练，证明了其在生产环境中的可靠性和性能表现。

## **Apple’s Approach to Scalable Machine Learning Infrastructure on Ray | Ray Summit 2025**

[https://www.youtube.com/watch?v=qnhtKGDhVbg](https://www.youtube.com/watch?v=qnhtKGDhVbg)

视频介绍：本视频由Apple的研究工程师Yiha和Ho Chong共同分享，详细介绍了他们如何基于Ray框架构建一个灵活且可扩展的机器学习框架ASA，旨在解决内部机器学习工作流中的基础设施碎片化问题，并支持从传统深度学习到大规模基础模型训练的全流程工作。

结论：Apple通过构建ASA统一机器学习框架，成功解决了内部数千名机器学习开发者面临的工具链碎片化、环境切换复杂和资源效率低下等核心痛点。该框架基于Ray构建，具备基础设施无感知、原生开发者体验保持、分布式训练简化等核心特性，支持从单GPU实验到千卡级基础模型预训练的全尺度应用。实际案例证明，在70亿参数模型预训练中实现了每GPU 1,400 tokens/秒的高效训练性能，展现了框架在大规模生产环境中的成熟度和可靠性。该设计既为普通开发者提供了开箱即用的简易接口，又为高级用户保留了充分的灵活性和可扩展性。

关键点：

1. [00:01:20](https://youtu.be/qnhtKGDhVbg&t=80s) ​**机器学习工作流的碎片化挑战与统一愿景**：

    - **问题根源**：Apple内部存在数千名机器学习开发者协作开发各类AI功能，用例涵盖传统排序模型、搜索嵌入、大语言模型预训练、后训练、强化学习和智能体系统。但现代机器学习工作流的基础设施高度碎片化，涉及多个云提供商、数据平台、计算系统和编排工具，每个层级都引入独立的环境、API和依赖，迫使研究人员在不同上下文间频繁切换以完成端到端工作流。
    - **具体痛点**：

      - 特征工程阶段使用Spark等JVM环境工具在CPU集群进行大规模数据处理
      - 模型训练阶段使用PyTorch/TensorFlow等C++环境工具在GPU/TPU集群运行
      - 模型部署评估阶段依赖独立的Java/Go系统，集成度有限
    - **负面影响**：这种碎片化导致迭代速度放缓、额外学习成本增加、资源效率降低（特别是GPU工作负载），并严重阻碍研究原型向生产系统的快速转化。
    - **解决方案愿景**：构建统一框架让开发者专注于建模创新而非基础设施，具体目标包括：

      - 基于开源框架构建生产就绪方案
      - 保持原生开发者体验，延续熟悉API
      - 简化分布式和大规模训练，抽象集群配置和模型并行等复杂性
      - 实现数据处理、模型训练、评估部署的端到端无缝衔接
2. [00:03:07](https://youtu.be/qnhtKGDhVbg&t=187s) ​**ASA框架架构设计与核心能力集成**：

    - **基础设施层**：框架底层支持内部云、第三方云、内部Kubernetes计算系统和Ray后端计算系统，实现真正的代码可移植性和基础设施无感知，用户无需修改代码即可在不同基础设施上运行相同任务。
    - **核心能力集成**：

      - 训练核心：选择PyTorch和Hugging Face作为基础
      - 大规模模型并行：集成DeepSpeed、FSDP和Torch Distributed
      - 强化学习：集成TH、Hard Hugging Face TL和Verifier
      - 语言模型评估：集成Light Eval
      - 内部工具链：集成实验追踪工具、Read Data（大规模数据处理）、Airflow 3（编排）和Open Lineage（治理和血缘追踪）
    - **双接口设计**：

      - 标准用户接口（占平台用户70%）：针对经典深度学习训练和LLM后训练（如监督微调），提供PyTorch和Accelerate接口，支持数据并行、FSDP数据并行、全分片数据并行和DeepSpeed扩展
      - 高级用户接口（占平台用户30%）：针对大规模预训练和强化学习，提供Pouch和RE接口，支持基于Torch的多维并行，包括张量并行、流水线并行、上下文并行、序列并行和损失并行
3. [00:05:21](https://youtu.be/qnhtKGDhVbg&t=321s) ​**标准用户工作流实例与设计哲学**：

    - **实例展示**：左侧为定义核心训练逻辑的Python脚本，右侧为指定训练参数、资源需求和分布式设置的配置文件，以训练BERT模型为例，使用原生PyTorch和Hugging Face接口。
    - **框架设计哲学**：

      - 保持接口极简：所有训练逻辑在基类接口函数中定义，无额外封装层，保持笔记本脚本般的简洁性
      - 仅引入两个关键函数：YouTube函数（从配置文件加载配置为Python字典）和数据加载器（从内部对象存储直接读取数据，带全局缓存）
    - **配置灵活性**：配置文件包含训练脚本路径、模型名称、学习率、批大小、训练数据等参数，以及计算平台设置（GPU数量）和扩缩容策略，本例使用DeepSpeed ZeRO-3实现内存高效的分布式训练。
    - **用户体验价值**：用户使用熟悉的PyTorch/Hugging Face API编写代码，可在内部Kubernetes系统或Ray后端系统上运行，相同代码在本地、内部云或第三方云无需修改，训练根据模型并行策略自动扩缩容，开发者专注于建模逻辑而框架处理底层基础设施复杂性。
4. [00:07:47](https://youtu.be/qnhtKGDhVbg&t=467s) ​**高级用户的多维并行与自动化优化**：

    - **核心差异**：针对大规模基础模型预训练，框架提供两个关键增强：

      - 开发基于Torch的多维并行框架，支持数据并行、张量并行、流水线并行等组合应用
      - 专门暴露RE接口，利用Ray灵活性支持高级操作，如强化学习期间托管推理模型或超参数搜索
    - **并行策略配置**：用户可配置张量并行维度（如设为8）、流水线并行维度（如设为4），在模型策略区域指定如何分片模型维度、如何跨并行维度分片每个模型层，FSDP设置为自动模式，框架根据可用GPU数量自动调优。
    - **端到端自动化**：基于大规模训练效率基准，框架根据模型大小、GPU数量和数据量自动选择最高效的并行组合并应用于训练，使高级用户无需手动配置或调优策略即可获得接近最优的扩展性能。
5. [00:11:12](https://youtu.be/qnhtKGDhVbg&t=672s) ​**70亿参数基础模型预训练案例研究**：

    - **实验设置**：

      - 模型架构：70亿参数解码器only基础模型
      - 计算资源：120个H200节点（近1,000个GPU），总计240 petaflops FP16计算能力
      - 数据规模：序列长度4,098 tokens，训练约1万亿tokens（对应7500亿单词）
      - 数据管理：通过内部对象存储确保分布式基础设施的最高吞吐数据访问
    - **并行策略**：实施三维并行策略

      - 数据维度：60路分割配合FSDP
      - 模型层内：4路模型分割配合张量并行
      - 跨模型：4路分割配合流水线并行
    - **基础设施抽象**：用户无需手动编排复杂并行与基础设施设置，ML框架与Ray共同处理分布式协调、容错和资源管理，让团队专注于模型架构和训练动态。
6. [00:14:15](https://youtu.be/qnhtKGDhVbg&t=855s) ​**内存优化与训练稳定性关键技术**：

    - **激活检查点**：关键内存优化技术，通过计算换内存策略，在反向传播期间按需重新计算中间激活而非全部存储在GPU内存中。在应用FSDP、TP或流水线并行后，激活内存常成为GPU内存使用的主导因素，此技术使训练更大模型或通过增加批大小提升训练效率成为可能。
    - **混合精度训练**：配置选项进一步提升计算和内存效率，特别适合现代GPU的FP16精度计算优势，同时减少内存占用，应用灵活性取决于具体需求。
    - **Torch Compile**：即时编译优化，在训练循环中自动融合操作并减少PyTorch开销。
    - **确定性数据加载器**：提供可重现和可调试训练的关键特性，在分布式系统设置中简化调试和复现，支持中期恢复，确保同时恢复模型状态和数据加载器状态。
    - **检查点与恢复机制**：自动保存模型状态和数据加载器状态基于步骤进度，高层编排层支持在硬件故障或低优先级层被抢占时自动恢复训练，对于持续数周的训练任务至关重要，配合底层基础设施实现作业自动重启和重试。
7. [00:18:49](https://youtu.be/qnhtKGDhVbg&t=1129s) ​**训练性能结果与开发者体验价值**：

    - **性能指标**：在70亿参数模型训练中实现每GPU约1,400 tokens/秒，在近1,000 GPU设置下整体达到130万tokens/秒的训练吞吐量。
    - **计算效率**：每个token估计需要42 teraflops，实现每GPU接近800 teraflops的计算效率，完整训练耗时约10天。
    - **开发者体验**：启动此训练任务只需相同API配合不同配置，通过库集成和Ray集成，框架在幕后抽象了大量复杂性，真正实现了让机器学习工程师和研究人员专注于核心创新而非基础设施管理的设计目标。

## **Ray Serve: Advancing scalability and flexibility | Ray Summit 2025**

[https://www.youtube.com/watch?v=S3CaJ36xl_U](https://www.youtube.com/watch?v=S3CaJ36xl_U)

视频介绍：本视频由Anyscale公司的Ray Serve团队工程师Abra Shik和Alex Yang分享，详细介绍了Ray Serve框架在过去一年中在可扩展性、性能和灵活性方面的重大进展。内容包括异步推理支持、自定义请求路由、自定义自动扩缩容、性能优化以及多云服务支持等核心功能。

结论：Ray Serve作为一个生产就绪的AI推理引擎，通过引入异步推理、自定义路由策略和跨部署自动扩缩容等高级功能，显著提升了框架的灵活性和表达能力。性能方面，Anyscale运行时相比开源版本实现了高达7倍的吞吐量提升，并保持了近线性的扩展能力。新推出的多云服务功能进一步解决了GPU资源稀缺和单点故障问题，为大规模AI工作负载提供了跨云厂商的高可用解决方案。这些改进使Ray Serve在简化AI模型部署复杂度的同时，兼顾了性能、可靠性和易用性，成为企业级AI推理的理想选择。

关键点：

1. [00:01:28](https://youtu.be/S3CaJ36xl_U&t=88s) ​**Ray Serve的核心价值与定位**：

    - **解决最后一英里部署难题**：许多团队能够训练模型，但将其部署到生产环境却充满挑战。Serve致力于解决这一"最后一英里"问题，让用户能够直接从同一生态系统中将模型部署到生产环境，无需将模型重新打包为微服务。
    - **简化复杂系统配置**：在生产环境中运行AI推理引擎需要配置多个复杂系统，如Kubernetes HPA实现自动扩缩容、FastAPI中间件实现请求批处理、服务发现设置等。Serve将这些复杂性抽象化，提供用户熟悉的简洁API。
    - **支持现代AI工作负载**：当今AI推理工作负载正从单API单模型范式转向单API需要多个模型或模型链的推理。Serve提供了直观的模型组合API，便于开发者理解和实现复杂推理流程。
    - **成熟的生产就绪特性**：Serve开箱即用地提供了成熟的并发模型、错误处理、背压、重试、错误代码处理等功能，并与Kubernetes和Anyscale无缝集成，支持受控部署和集群编排。
2. [00:04:55](https://youtu.be/S3CaJ36xl_U&t=295s) ​**异步推理支持的实现与应用**：

    - **实际用例背景**：以香港语言AI公司FanoAI为例，该公司处理大量呼叫中心音频数据，其典型工作负载涉及将大型音频文件通过语言模型生成分析结果。由于音频文件较大，推理请求可能持续数分钟，不适合阻塞用户请求。
    - **异步推理解决方案**：FanoAI需要将请求移交到后台工作器，客户端可以在将来拉取结果。同时需要能够查询任务状态、失败重试、暂停恢复任务，并确保任务最终执行不受集群健康状况影响。
    - **技术实现架构**：

      - 客户端首先将大型音频文件上传到S3
      - 向入口部署发送包含S3文件引用的请求
      - 入口部署将任务加入Redis队列后立即响应客户端
      - 客户端获得任务引用但不阻塞请求完成
      - Celery工作器池从Redis队列中取出任务，利用Serve应用中的现有部署完成用户请求
    - **核心优势**：Celery工作器由Serve内部作为部署完全管理，这些工作器可以访问现有的Ray部署，其生命周期由Serve部署管理，并基于Redis队列中未完成任务数量进行自动扩缩容。
    - **扩展灵活性**：虽然示例使用Celery和Redis作为任务处理器和结果后端，但这不是硬性要求，用户可以扩展自己的任务处理器并使用不同的消息代理。
    - **广泛应用场景**：异步推理的应用远不止音频处理，还包括视频处理解码任务、处理大量文本数据生成嵌入等场景。
3. [00:09:44](https://youtu.be/S3CaJ36xl_U&t=584s) ​**自定义请求路由的增强功能**：

    - **默认路由算法局限性**：Serve默认使用"二选一"调度器，随机选择两个副本并将请求发送到负载较轻的那个。这种算法优化了最大负载副本与平均负载副本之间的差异，适用于大多数路由需求。
    - **LLM推理的特殊需求**：在LLM推理中，对于类似聊天机器人的轮转式系统，最好将请求路由到之前处理过部分提示前缀的副本，这样可以利用该副本上预热的KV缓存，确保高GPU利用率和低延迟。
    - **自定义路由能力**：通过Ray Serve的自定义请求路由支持，用户可以表达如此复杂的路由策略。路由决策在两个地方做出：入口层或代理层，以及部署间通信层。
    - **实现方式**：

      - 用户需要实现一个扩展自RequestRouter的路由策略类
      - 实现choose_replicas函数，该函数接收Serve框架自动注入的候选副本集
      - 返回排名的副本集，Serve将按排名顺序尝试将请求发送到副本
      - 最后将路由策略与部署关联，指示框架覆盖默认策略
    - **辅助工具**：库中提供了一组mixin供用户使用，如LocalityMixin确保在尝试节点外或可用区外的副本之前，优先尝试同一节点上的副本。
4. [00:12:54](https://youtu.be/S3CaJ36xl_U&t=774s) ​**自定义自动扩缩容的创新实现**：

    - **华为用例背景**：华为使用Ray Serve处理LLM推理流水线，典型的Serve应用包含三个部署：预处理阶段、LLM推理阶段和后处理阶段。他们发现现有的自动扩缩容功能较为有限。
    - **现有机制局限性**：当前机制基于用户设定的目标请求数量，Serve内部计算正在进行的请求数量，然后通过比较这两个量的相对差异来决定每个部署的目标副本集。
    - **华为的复杂需求**：

      - 基于从Serve应用内部发出的自定义指标做出决策
      - 编写跨所有部署的策略，而不仅仅针对单个部署
    - **技术实现架构**：

      - 每个部署中的副本现在能够发出自定义指标
      - Serve确保这些指标传输到控制器，并在自动扩缩容策略的上下文中可用
      - 框架定期从副本拉取指标，用户无需特殊操作
      - Serve使用框架跟踪的自身指标（如排队请求数和进行中请求数）来丰富自定义指标
      - 现在能够在Serve控制器的控制循环中运行自定义策略函数
    - **策略范围扩展**：用户不仅可以在部署级别指定策略，还可以编写跨多个部署的应用级别策略。
    - **广泛应用价值**：自定义自动扩缩容的应用范围巨大，用户可以定义在特定时间扩展副本、在夜间缩容到零的策略，以及更复杂的基于预测性自动扩缩容和预测模型的策略，或SLA驱动的自动扩缩容。
5. [00:17:39](https://youtu.be/S3CaJ36xl_U&t=1059s) ​**性能优化与Anyscale运行时优势**：

    - **效率优化重要性**：在模型服务规模上，效率不仅对降低基础设施成本至关重要，也对最终用户体验和整体系统可靠性极为重要。
    - **基准测试设置**：通过一个简单的推荐系统应用基准测试，比较Ray Serve与Anyscale运行时的性能。测试使用具有1.66亿参数的DRM模型，在GPU上运行，启用Serve动态批处理以最大化GPU效率。
    - **性能对比结果**：

      - Ray开源版本在1、2、4个GPU副本时分别达到75、100、100 RPS的峰值
      - 从1个增加到2个GPU副本时，峰值吞吐量增加约50%，但从2个增加到4个时仅略有改善
      - Anyscale运行时在1个GPU副本时能够提供3倍于开源版本的吞吐量
      - 在4个GPU副本时，Anyscale运行时服务6300 RPS，是开源版本的7倍
      - Anyscale运行时在200、400和750用户时达到峰值RPS，呈现近线性扩展
    - **性能优化技术**：

      - **Serve参与者级别优化**：减少了非关键路径操作的开销，使服务框架能够将更多CPU周期用于实际处理请求
      - **线程和事件循环合并**：提供选项将Serve的多个线程和事件循环合并为单个，减少上下文切换和管理多个异步事件循环的开销
      - **IO优化**：更积极地缓冲日志记录和指标以减少IO开销
      - **跟踪优化**：优化跟踪功能，使可观测性带来最小的性能损失
      - **路由层优化**：优化路由层以减少引入的延迟
      - **通信层优化**：增加总对象传输带宽，减少Serve参与者之间的通信开销
6. [00:22:47](https://youtu.be/S3CaJ36xl_U&t=1367s) ​**多云服务支持的创新演示**：

    - **当前AI工作负载面临的挑战**：

      - GPU资源极其稀缺，特别是在单个区域和单一云提供商约束内运行
      - 在单个区域运行服务会使服务在区域故障时面临风险
      - 在多云环境中操作变得更加困难且容易出错
    - **Anyscale解决方案**：构建了一个无缝跨越多个云和区域的单一Anyscale服务，具备协调推出、回滚、跨所有云和区域管理负载均衡器等所有优势。
    - **演示流程概述**：

      - 在AWS和Google Cloud的两个独立区域启动多云Anyscale服务
      - 启动几个客户端向服务发送连续流量，流量根据云提供商区域的硬件可用性动态配置权重
      - 模拟AWS集群故障，观察流量开始从AWS集群转移到Google Cloud集群
      - Google Cloud中运行的集群将扩展以支持需求涌入
    - **实际演示结果**：

      - 服务在两个云提供商中各启动一个集群，具有动态权重
      - 随着流量增加，Serve应用在每个集群中扩展其副本，集群也扩展工作节点以支持流量
      - 当模拟AWS集群故障时，立即看到AWS的RPS开始下降，Google Cloud集群的RPS开始增加
      - Google Cloud集群开始扩展其节点以支持需求涌入
      - 在整个实验过程中，即使经历了集群故障事件，应用请求延迟也保持完全平稳
    - **业务价值**：多云服务提供了GPU资源优化、高可用性保障和简化的跨云管理，为企业级AI应用提供了可靠的部署方案。

## **Scaling Machine Learning at Tripadvisor: Our Journey with Ray and Anyscale | Ray Summit 2025**

[https://www.youtube.com/watch?v=IBx1A81CRb8](https://www.youtube.com/watch?v=IBx1A81CRb8)

视频介绍：本视频由TripAdvisor的机器学习工程师Jada Stories和机器学习平台团队成员Sam Jenkins共同分享，详细介绍了该公司如何通过采用Ray和Anyscale平台来优化其机器学习基础设施，以应对日益复杂的AI工作负载和业务需求。

结论：TripAdvisor通过全面采用Anyscale平台，成功实现了机器学习工作负载的现代化转型。该迁移带来了显著的效率提升，包括代码量减少超过80%、计算成本降低超过80%以及处理时间大幅缩短。平台提供的统一开发环境、弹性伸缩能力和完善的监控体系，使得团队能够更快速地部署和维护复杂的AI应用。未来，公司计划进一步扩展Ray在实时AI服务、LLM训练与微调等领域的应用，巩固其作为MLOps核心计算平台的地位。

关键点：

1. [00:01:48](https://youtu.be/IBx1A81CRb8&t=108s) ​**TripAdvisor的机器学习规模与挑战**：

    - 业务覆盖范围广泛：TripAdvisor拥有涵盖完整旅行旅程的多样化产品界面，从旅行灵感到详细行程规划，再到生成式AI体验和比价预订服务。
    - AI集成深度：在所有产品界面都集成了人工智能和机器学习技术，包括实时AI智能体、推荐系统、搜索和信息检索、计算机视觉模型等。
    - 处理规模庞大：系统需要处理数亿条用户评论、数亿张用户上传照片、数十亿用户和数百万个地理位置信息。
    - 基础设施复杂度：拥有数百个离线机器学习工作负载、数百个实时推理模型，以及快速增长的AI微服务数量。
2. [00:04:32](https://youtu.be/IBx1A81CRb8&t=272s) ​**向Ray平台迁移的战略转变**：

    - 架构演进：从去年仅在CubeFlow管道任务中单节点使用Ray，发展到建立自动伸缩的异构计算节点集群。
    - 解耦部署：将Ray从CubeFlow中解耦，支持独立的Ray作业运行，这一转变由Anyscale平台实现。
    - 应用范围扩展：不仅用于离线工作负载，还开始探索在实时AI应用中替代独立的FastAPI服务和Seldon ML服务器部署。
    - 概念性转变：Ray作业作为CubeFlow管道的替代方案，提供了更灵活的计算框架抽象。
3. [00:08:45](https://youtu.be/IBx1A81CRb8&t=525s) ​**概念优势与技术选型依据**：

    - 离线工作负载优势：

      - 灵活的资源控制：能够在Python代码中直接控制工作负载的水平和垂直扩展
      - 代码简洁性：相比CubeFlow减少了大量样板代码，提高了可维护性
      - 运行效率：通过更好的资源利用实现了更高的计算效率
    - 实时推理服务优势：

      - 统一的开发体验：Ray Serve支持部署包含业务流程逻辑和嵌入式推理的完整机器学习应用
      - 简化的工作流程：使机器学习从业者能够在一个框架内完成模型训练、部署和微服务逻辑实现
    - 技术选型决策：

      - 在开源KubeRay和托管Anyscale平台之间，最终选择Anyscale
      - 主要考虑因素：避免7-10个月的平台开发时间，快速获得开发效率和计算效率的显著提升
4. [00:10:23](https://youtu.be/IBx1A81CRb8&t=623s) ​**概念验证成果与性能提升**：

    - 代码量大幅减少：在批处理用例中实现超过80%的Python代码行数减少
    - 成本显著降低：图像嵌入和评论嵌入等任务的成本降低超过80%
    - 性能全面提升：

      - 实时用例延迟大幅降低
      - 相同硬件上的吞吐量显著提升
      - 与API服务相比，LLM批处理推理成本优势明显
    - 具体测试用例：

      - 实时AI服务：支持位置搜索的混合语义和词汇检索
      - 批处理嵌入管道：图像和评论的嵌入处理
      - AI集合管道：将实时服务逻辑批量推理数百万输入
      - LLM批处理推理基准测试
5. [00:15:43](https://youtu.be/IBx1A81CRb8&t=943s) ​**平台集成与基础设施架构**：

    - 多账户架构：使用沙盒和生产环境AWS账户分别支持离线和在线工作负载
    - 项目组织：按数据科学领域（如图像数据科学、文本数据科学）划分Anyscale项目
    - 权限管理：利用Anyscale的项目范围IAM角色实现最小权限原则
    - 开发者工作流：

      - 简单访问集群头节点进行快速迭代
      - CI/CD管道打包作业进行编排
      - 自定义镜像构建打包依赖
      - 基于Azure Active Directory的角色访问控制
    - 内部工具：基于Anyscale API构建的GitOps工具，用于项目管理、预算控制和计算配置
6. [00:18:39](https://youtu.be/IBx1A81CRb8&t=1119s) ​**Anyscale与CubeFlow的核心差异**：

    - 硬件灵活性和资源利用：

      - Ray调度支持CPU、GPU和内存密集型工作负载的混合部署
      - 显著减少Kubernetes资源的空闲时间
      - 现货实例中断处理机制有效管理成本和维护集群状态
    - 开发者体验简化：

      - 无需理解Kubernetes原语，只需编写Python代码
      - 从研究到生产的转换更加顺畅，调试更简单
    - 操作可观测性：

      - Ray仪表板便于优化
      - Anyscale专有数据仪表板提供深入洞察
    - 统一平台优势：允许开发者在同一集群和工作空间中开发测试批处理作业和在线推理服务
7. [00:20:07](https://youtu.be/IBx1A81CRb8&t=1207s) ​**生产批处理管道迁移实例**：

    - 图像处理管道：

      - 嵌入管道：为所有发布到TripAdvisor的图像计算和存储嵌入向量，每日处理约10万张图像，全量回填时处理4亿张图像
      - 质量评分管道：使用美学评分和技术质量评分两个模型评估图像质量
    - 性能改进数据：

      - 嵌入管道：时钟时间减少65%，成本相应下降，代码行数减少87%
      - 关键技术：Ray Data的流式工作负载解耦CPU基础的图像下载预处理和GPU加速的嵌入过程
    - 位置数据处理：

      - 位置嵌入管道：结合名称、描述、热门提及等生成嵌入向量，处理1200万个兴趣点
      - 地理位置嵌入管道：新开发的管道，结合地理和位置特征生成嵌入向量
8. [00:25:21](https://youtu.be/IBx1A81CRb8&t=1521s) ​**未来发展规划与迁移策略**：

    - 技术探索方向：

      - 基于Ray Serve的智能体应用
      - LLM训练和微调
      - LoRA适配器多任务托管
      - MCT服务器和知识蒸馏技术
    - 全面迁移计划：

      - 将现有离线特征存储库迁移到Ray Data
      - 将FastAPI微服务框架迁移到Anyscale服务
      - 迁移更多批处理管道和经典机器学习模型重训练管道
    - 分阶段采用策略：

      - 首先由熟悉平台架构的机器学习工程师参与
      - 建立工具链和CI/CD流程后引入数据科学家
      - 采用分阶段采用策略确保平稳过渡

## **SGLang: An Efficient Open-Source Framework for Large-Scale LLM Serving | Ray Summit 2025**

[https://www.youtube.com/watch?v=w9-AYqIhHRo](https://www.youtube.com/watch?v=w9-AYqIhHRo)

视频介绍: 本视频由SGLang项目的核心开发者分享，全面回顾了SGLang在2025年上半年的发展成果和战略重点，并详细介绍了在GB200和V72硬件上部署DeepSeek模型的案例研究。

结论: SGLang作为开源AI推理引擎，在2025年上半年取得了显著进展，特别是在大规模部署、训练框架集成、推测解码、长上下文优化和内核优化等五大战略重点领域。通过PD解耦和专家并行技术，在92到3000个GPU规模上实现了行业领先的性能表现，相比DeepSeek API定价实现了5倍成本降低。下半年将重点提升生产可靠性、用户体验和功能兼容性，同时继续推进硬件优化和生态系统建设。在GB200硬件上的部署案例展示了低精度内核优化带来的显著性能提升，为企业在下一代硬件上部署大模型提供了可靠解决方案。

关键点:

1. [00:00:48](https://youtu.be/w9-AYqIhHRo&t=48s) ​**2025年上半年五大战略重点与成果**：演讲者系统总结了SGLang在2025年上半年的核心发展方向和取得的突破性进展。

    - 大规模部署优化：专注于吞吐量导向的优化，实现企业级服务能力，特别通过PD解耦和EP技术实现规模效率最大化。在5月份发布了在92个GPU上部署DeepSeek的博客，随后扩展到300、400直至3000个GPU规模，在XAI的生产服务中已部署在数百个GPU上。
    - 训练框架集成：深度集成SGLang推理引擎与多个AI框架，包括Slime、Ariel和Vero等。与Slime和Ariel团队从第一天起就紧密合作代码开发，已在多家公司的真实生产实践中得到应用。
    - 推测解码优化：经过多次优化实现了低延迟推测解码实施方案，显著减少推理时间，并开发了Spec Forge工具帮助训练推测草稿模型，近期仍在重构以进一步优化。
    - 长上下文优化：启用先进的内存管理和处理技术，处理扩展上下文窗口时实现最小性能下降。今年宣布了分层缓存功能，支持在GPU、CPU和磁盘上缓存KV缓存，构建分布式存储内存体系。
    - 内核优化：通过与NVIDIA紧密合作，在硬件特定内核实现方面取得重大改进，最大化计算利用率和内存带宽以获得最优性能。
2. [00:03:36](https://youtu.be/w9-AYqIhHRo&t=216s) ​**大规模部署性能突破与验证**：详细展示了SGLang在大规模部署方面取得的行业领先性能指标和实际验证结果。

    - 行业首创性能：5月份发布的博客实现了开源系统首次近乎匹配DeepSeek官方性能的表现，采用PD解耦和EP技术，达到了每个节点52.3k输入tokens/秒的行业领先输入处理速度，以及每个节点22.3k输出tokens/秒的卓越生成吞吐量。
    - 成本效益显著：相比DeepSeek API定价实现了5倍成本降低，已有超过10个团队成功复现了这一性能突破，验证了架构决策的正确性。
    - 企业级验证：证明了SGLang作为需要企业级AI推理能力组织的ongoing解决方案的定位和价值。
3. [00:04:43](https://youtu.be/w9-AYqIhHRo&t=283s) ​**训练框架深度集成架构设计**：深入解析了SGLang与各训练框架的集成架构和设计理念。

    - 原生框架协作：与Ariel和Slime等SGLang原生框架紧密合作，这些框架专门为充分利用SGLang在RL训练工作流中的全部潜力而构建，具有优化的性能特性。
    - 通用集成方案：同时投入时间进行通用集成，与Vero等更全面的框架集成，支持多个推理后端，作为一个可选方案。
    - Slime架构亮点：采用数据缓冲区发送提示到自定义rollout生成的架构设计，包含自定义机器人引擎生成组件，支持定义任何自定义数据生成方案，通过SGLang路由器将请求分发到服务器，实现了rollout逻辑的灵活定义，并分离了SGLang开发逻辑和RL部分。
4. [00:06:13](https://youtu.be/w9-AYqIhHRo&t=373s) ​**社区生态与发展态势**：全面介绍了SGLang社区的增长情况和行业采用现状。

    - 社区规模：目前已有超过800名贡献者，活跃贡献者来自世界各地，贡献代码、文档和社区支持，约有30人非常活跃，每天或每周都在持续贡献。
    - 行业采用：超过50个机构全面使用SGLang，超过20家企业用户采用SGLang作为其默认的DeepSeek推理引擎，在发布第一个月就得到广泛采用。
    - 平台支持：目前在Ray Data中得到支持，对Reserve的支持正在开发中，社区持续增长壮大。
5. [00:07:04](https://youtu.be/w9-AYqIhHRo&t=424s) ​**2025年下半年战略重点与路线图**：系统阐述了SGLang在下半年的发展方向和具体计划。

    - 生产可靠性和用户体验：在保持优秀性能优化的同时，重点关注易用性和用户亲近度，完善分布式缓存功能，使其更加全面，支持更多高级特性。
    - 功能兼容性：致力于支持所有主要特性的高性能组合，解决PT解耦、推测解码、各种内存池、不同注意力机制和重叠调度器等高级特性之间的冲突问题，实现在最佳性能下的同时启用。
    - 生产级可靠性：改进CI覆盖率，相比一个月前已有显著提升，新增崩溃转储重放和报告等调试功能，未来三个月将推出更多改进。
    - 可用性增强：开发SGLang Cookbook实现一键部署，开发测试框架平台支持直接与引擎后端交互，OM集成提供企业级AM管理的Kubernetes operator，支持自动化扩展、监控和部署编排。
6. [00:10:23](https://youtu.be/w9-AYqIhHRo&t=623s) ​**下一代内核优化与技术演进**：详细介绍了SGLang在硬件优化方面的持续投入和技术进展。

    - CUDA内核集成：持续进行CUDA内核集成工作，通过与NVIDIA紧密合作实现注意力内核的深度集成，这些优化针对Transformer架构的内存带宽利用率和计算效率。
    - 内核优化进展：改进了多个优化注意力内核和多种注意力后端，使其默认设置达到最佳状态，提升了高效稀疏MO路由和通信内核计算的性能。
    - 通信优化：在多节点间实现更快的reduce操作，优化参数共享的输出内核，减少解耦架构中的通信开销。
7. [00:13:58](https://youtu.be/w9-AYqIhHRo&t=838s) ​**GB200和V72硬件部署案例研究**：深入分析了在下一代硬件上部署DeepSeek模型的技术方案和性能表现。

    - 技术背景：基于5月份发布的PD解耦博客，在6月和9月发布了GB200的后续博客，在Edge 100上的大规模部署取得了优异性能。
    - 架构选择：使用3个节点作为预填充工作器，9个节点作为解码工作器，采用深度和DP注意力最大化吞吐量，选择专家并行和DP注意力的原因是DP注意力避免KV缓存重复，专家权重通过专家并行在设备间分区消除内存瓶颈。
    - 关键技术：遵循由DP和双批次重叠驱动的分发专家组合模式，最小化延迟和开销。双批次重叠将大批量分成两个微批次，在第一个批次计算时第二个批次进行通信，实现计算和通信重叠。
    - PD解耦优势：预填充不会抢占正在进行的解码任务，确保更平衡的DP注意力，兼容DP注意力下的不同分发模式，KV缓存传输现在支持非阻塞和基于RDMA的传输。
8. [00:16:19](https://youtu.be/w9-AYqIhHRo&t=979s) ​**GB200硬件升级带来的技术优势**：系统分析了从H100升级到GB200和V72带来的性能提升机会。

    - 硬件优势：增强的内存带宽和计算能力支持加速内核执行，更大的内存容量支持更大的KV缓存、更大的批次大小和改进的内核效率，更大的CUDA图，更大的NVLink域支持纯NVLink解决方案显著降低通信延迟。
    - 内核优化方案：FP8生成使用NVIDIA Blackware Deep Jam，NVFP4 Jam，预填充使用Flash Info Blackware Catalas Jam，解码使用Flash Info Blackware QDS Jam。BF16注意力中，预填充使用Flash Attention in QDS，解码使用TensorRT Attention in Flash Infer。FP8注意力使用Flash Info Blackware Tensor RTM Attention。
    - 精度保障：低精度不会影响准确度分数，使用SGLang服务低精度模型不会损失准确性。
9. [00:17:51](https://youtu.be/w9-AYqIhHRo&t=1071s) ​**GB200基准测试性能成果**：展示了在GB200硬件上的具体性能测试结果和优化效果。

    - 低精度内核性能：从BF-16切换到FP8注意力时，延迟减少约一半，从FP8切换到NVFP4 Jam时，速度提升高达1.96倍。
    - 端到端性能：在低精度下每个NVIDIA GPU实现超过26k输入和13k输出tokens/秒的性能，远快于H100，在高精度下也实现了非常高的性能表现。
    - 成本效益：基于GPU租赁价格和吞吐量计算，相比DeepSeek API实现了显著的成本优势。
10. [00:19:12](https://youtu.be/w9-AYqIhHRo&t=1152s) ​**生态系统扩展与社区互动**：通过问答环节展示了SGLang生态系统的扩展计划和社区支持体系。

     - 新RL框架计划：基于Slime构建的新框架将包含容错、弹性训练、rollout算法和True Own Policy等额外改进，与现有框架保持良好关系，新功能将向上游提交。
     - 多硬件支持：除CUDA NVIDIA外，还支持MPU和AMD加速器，有来自这些公司的贡献者积极在相应芯片上工作SGLang，同时支持SGLang Jacks在TPU上运行。
     - 负载均衡机制：专家并行具有称为EPB的负载均衡器，支持动态负载均衡。
     - 部署方案：提供OM工具支持在Kubernetes上部署SGLang，社区通过公开Slack频道加强交流协作。

## **SkyRL tx: A unified training and inference engine | Ray Summit 2025**

[https://www.youtube.com/watch?v=_JLnESEu2gw](https://www.youtube.com/watch?v=_JLnESEu2gw)

视频介绍: 本视频由SkyRL团队分享，详细介绍了他们开发的SkyRL TX项目——一个开源的Tinker API实现。该项目旨在为研究人员和开发者提供可自定义的模型训练服务，支持多租户LoRA适配器训练，并探索统一训练与推理引擎的创新架构。

结论: SkyRL TX项目通过构建统一的训练与推理引擎，成功实现了Tinker API的开源实现，支持多租户LoRA训练、高性能优化和灵活的扩展架构。该项目在短短一个月内取得了显著进展，包括支持Quen3模型、完整训练循环、采样端点和检查点功能。未来发展方向包括更多模型支持、分布式训练优化和与现有生态系统的深度集成，展现了开源AI训练框架在标准化和可定制化方面的巨大潜力。

关键点:

1. [00:02:08](https://youtu.be/_JLnESEu2gw&t=128s) ​**Tinker API的核心设计与价值**：演讲者详细阐述了Tinker API的设计理念和核心优势。

    - **抽象层级提升**：Tinker API允许用户在本地CPU上编写简单的训练循环，而将繁重的训练和采样工作交由远程GPU执行，大幅降低了用户与基础设施交互的复杂度。
    - **统一接口设计**：API通过四个核心方法（前向传播、后向传播、优化器步进、采样）统一了训练和采样接口，支持SFT、DPO、RL等多种训练方法。
    - **灵活性支持**：不强制使用LoRA适配器，支持全参数微调，原始服务采用多LoRA架构，支持在共享基础模型参数的基础上为每个客户端训练独立的适配器。
    - **标准化潜力**：类似于OpenAI API在推理领域的标准化作用，Tinker API有望成为训练领域的标准接口，促进开源社区工具和库的共享与发展。
2. [00:06:01](https://youtu.be/_JLnESEu2gw&t=361s) ​**架构选择：统一引擎的创新探索**：团队详细解释了选择构建统一训练推理引擎的原因和优势。

    - **两种架构对比**：第一种方案是组合现有的成熟训练和推理引擎，第二种是构建统一的共同引擎。团队选择了更具探索性的第二种方案。
    - **简化实现**：统一引擎可以共享模型定义、多LoRA实现、检查点管理和分片策略等大量代码，显著降低了系统复杂度。
    - **资源协同**：使得在训练和采样之间共享GPU资源变得更加容易，确保了数值一致性，简化了权重更新后的同步过程。
    - **探索价值**：尽管成熟的独立堆栈有其合理性，但团队希望通过统一引擎探索后训练框架的新可能性，特别是在强化学习等需要训练与推理紧密配合的场景中。
3. [00:08:41](https://youtu.be/_JLnESEu2gw&t=521s) ​**SkyRL TX系统架构与工作流程**：深入介绍了项目的整体架构设计和请求处理流程。

    - **请求生命周期**：用户通过REST API发送请求，系统将请求持久化到数据库中进行队列管理，支持多客户端和LoRA适配器的元数据跟踪。
    - **执行引擎**：CPU端的引擎负责请求批处理和调度决策，利用多LoRA架构实现跨客户端的批处理前向和后向传播，最大化GPU资源利用率。
    - **工作节点**：GPU工作节点实际执行请求，包括前向后向传播、优化器状态更新和模型检查点保存，当前基于JAX实现，计划支持PyTorch。
    - **早期优势**：项目处于早期阶段，代码结构简单清晰，是理解和贡献Tinker类服务实现的绝佳时机。
4. [00:12:21](https://youtu.be/_JLnESEu2gw&t=741s) ​**性能优化策略与实现**：详细介绍了项目在性能优化方面的具体工作和未来规划。

    - **编译优化**：通过JIT编译消除Python开销，使用CUDA图进行解码优化，实现内核融合和动态优化。
    - **张量填充**：对序列长度进行二进制舍入处理，减少芯片上的形状数量，平衡计算效率和内存开销。
    - **内存管理**：实现KV缓存、微批处理、梯度检查点等技术，显著降低内存压力。
    - **未来规划**：计划实现页面注意力、前缀缓存、上下文并行等高级优化，进一步提升系统性能。
5. [00:14:09](https://youtu.be/_JLnESEu2gw&t=849s) ​**核心技术：不规则矩阵乘法与多LoRA实现**：深入解析了项目的核心计算原语。

    - **不规则矩阵乘法**：作为多LoRA和MoE实现的基础原语，通过排序-分组计算-反排序的流程高效处理不同长度的输入序列。
    - **简洁实现**：多LoRA的实现代码极其简洁，通过LoRA混合类和分组矩阵乘法即可实现复杂的多租户训练功能。
    - **扩展性**：相同的技术可以扩展到MoE架构，支持适配器和专家的联合路由，代码实现仅需少量行数。
    - **架构优势**：共享基础模型配合独立的LoRA适配器和优化器，实现了真正的多租户GPU资源共享。
6. [00:18:03](https://youtu.be/_JLnESEu2gw&t=1083s) ​**分片策略与未来发展**：介绍了项目的分片实现和未来技术路线图。

    - **当前支持**：已实现张量并行分片，基于JAX的注解系统使得分片配置极其简单，只需在模型图中添加分片注解。
    - **扩展计划**：计划支持FSDP分片，只需扩展网格维度即可实现，未来还将支持流水线并行、上下文并行和专家并行。
    - **开发理念**：坚持简单性原则，代码应该清晰反映设计思想，避免过度工程化，确保项目的可扩展性和可修改性。
    - **社区贡献**：项目已经吸引了众多外部贡献者，欢迎更多开发者参与模型实现、分片机制优化和API扩展等工作。
7. [00:35:45](https://youtu.be/_JLnESEu2gw&t=2145s) ​**项目定位与未来愿景**：阐述了项目在SkyRL生态系统中的定位和长期发展目标。

    - **生态集成**：SkyRL TX作为SkyRL仓库中的独立子项目，未来有望成为SkyRL核心训练循环的后端实现。
    - **标准化推动**：类似于vLLM对OpenAI API的价值，SkyRL TX旨在成为Tinker API的高质量开源实现，满足企业对数据隐私和定制化的需求。
    - **应用场景**：特别适合研究实验室和多用户环境，可以部署为内部服务，让研究人员专注于算法创新而非基础设施管理。
    - **社区建设**：希望通过持续开发和社区贡献，打造功能完善、性能优异的开源训练框架，推动AI训练技术的标准化和普及化。

## **Ray Direct Transport: RDMA Support in Ray Core | Ray Summit 2025**

[https://www.youtube.com/watch?v=NrTmPUGQtVU](https://www.youtube.com/watch?v=NrTmPUGQtVU)

视频介绍：本视频由Any Scale公司Ray核心团队的软件工程师Stephanie和Choling共同分享，详细介绍了Ray 2.51.1版本中新增的Ray Direct Transport（RDT）功能。这是一个处于alpha阶段的重要特性，旨在优化GPU间数据传输性能。

结论：Ray Direct Transport通过引入直接数据传输机制，成功解决了传统Ray对象存储在GPU间数据传输中的性能瓶颈。该功能支持多种高速传输技术（如NCCL、NIXL），能够实现高达1000倍的性能提升，特别适用于强化学习、大语言模型训练等需要大量GPU间数据传输的场景。RDT保持了Ray核心API的易用性，同时提供了灵活的数据传输选择，为分布式AI工作负载提供了更高效的数据传输解决方案。

关键点：

1. [00:00:47](https://youtu.be/NrTmPUGQtVU&t=47s) ​**RDT的开发背景与需求**：

    - **现有系统局限性**：传统Ray对象存储基于CPU内存，在GPU间数据传输时需要经过GPU→CPU→网络→CPU→GPU的多重拷贝过程，造成了显著的性能开销。这种架构在处理大规模GPU工作负载时成为主要瓶颈。
    - **新兴应用场景驱动**：特别是在大语言模型的强化学习（RL for LLMs）场景中，需要同时运行推理引擎和训练引擎，这两个系统共享相同的模型权重，并且需要频繁进行权重同步和回放数据传输。
    - **技术挑战**：现代AI集群通常配备高速网络（如InfiniBand），但传统Ray架构无法充分利用这些硬件优势，导致网络带宽利用率低下。
2. [00:03:42](https://youtu.be/NrTmPUGQtVU&t=222s) ​**RDT的核心设计理念**：

    - **直接数据传输**：RDT允许actor之间直接通信，绕过传统的对象存储，使用RDMA等技术实现GPU到GPU的直接内存访问。
    - **零拷贝优化**：数据在GPU内存中保持非序列化状态，仅在需要时才进行传输，避免了不必要的序列化/反序列化开销。
    - **灵活的传输后端**：支持用户选择特定的数据传输技术，包括NCCL（用于集体通信）、NIXL（用于单边传输）等，能够充分利用高速网络硬件。
    - **控制平面管理**：虽然数据传输路径发生了变化，但Ray仍然负责管理控制平面的所有方面，包括协调数据传输、垃圾回收等系统级功能。
3. [00:05:45](https://youtu.be/NrTmPUGQtVU&t=345s) ​**RDT的技术特性与API设计**：

    - **支持范围**：目前主要支持Ray actor之间的数据传输，特别适用于有状态的GPU内存管理。支持Torch张量，包括嵌套在其他Python数据结构中的张量。
    - **可变对象语义**：与传统的Ray对象存储不同，RDT对象是可变的，因为数据是通过引用而非值拷贝的方式传递的。这要求用户在使用时需要注意数据一致性问题。
    - **API使用方式**：

      - 对于集体传输：需要在actor方法上添加@ray.method装饰器，并预先声明集体组。
      - 对于单边传输：只需要在actor类级别进行配置，支持更灵活的数据传输模式。
    - **传输选项**：支持Glue（用于CPU通信调试）、NCCL和NIXL（用于GPU间通信）三种传输后端。
4. [00:09:49](https://youtu.be/NrTmPUGQtVU&t=589s) ​**RDT的执行流程与内部机制**：

    - **初始化阶段**：每个启用RDT的actor都会在GPU上建立内存对象存储，驱动程序获取RDT对象引用并确定使用的传输技术。
    - **元数据交换**：actor返回张量的元数据（形状、NIXL描述符等）给驱动程序，然后传递给目标actor。
    - **数据传输**：通过RDMA直接进行GPU到GPU的数据传输，避免了CPU介入的开销。
    - **垃圾回收**：采用引用计数机制，当对象引用计数归零时，系统会清理本地副本和主副本，确保内存资源的有效管理。
5. [00:12:25](https://youtu.be/NrTmPUGQtVU&t=745s) ​**性能优势与基准测试**：

    - **显著性能提升**：对于大规模数据传输，RDT相比传统Ray对象存储可以实现超过1000倍的性能改进。
    - **技术原因**：避免了序列化/反序列化过程，直接使用GPU到GPU的RDMA传输，比基于TCP的传输更加高效。
    - **实际影响**：这种性能提升对于需要频繁进行模型权重同步和大量数据传输的AI训练任务具有重大意义。
6. [00:13:36](https://youtu.be/NrTmPUGQtVU&t=816s) ​**实际应用场景**：

    - **LLM强化学习**：在推理引擎和训练引擎之间实现高效的权重同步，支持复杂的RL算法流程，包括回放缓冲区管理等。
    - **异构多模态训练**：支持不同模型或子模型使用不同的并行策略，无论actor在GPU间的布局如何，都能实现高效的数据传输。
    - **实施示例**：展示了如何在RL训练循环中使用RDT进行权重同步，通过简单的装饰器配置即可启用高速数据传输。
7. [00:15:48](https://youtu.be/NrTmPUGQtVU&t=948s) ​**未来发展路线图**：

    - **功能扩展**：计划支持异步I/O、更多传输后端（如CUDA IPC）、完全可插拔的传输架构等。
    - **性能优化**：持续改进错误检测和处理机制，探索与Ray其他组件（如Ray Train、Ray Data）的深度集成。
    - **生态合作**：正在与多个RL项目（如Verl、Sky RL）进行合作，推动RDT在更广泛场景中的应用。

## **Hybrid RL + Imitation Learning for Robotics with Ray at RAI Institute**

[https://www.youtube.com/watch?v=JfdPxD2vLLMIY](https://www.youtube.com/watch?v=JfdPxD2vmIY)

视频介绍: 本视频由机器人与人工智能研究所（RA Institute）的Ahmed和Valerio共同呈现，详细介绍了他们基于Ray框架开发的混合强化学习与模仿学习平台。该平台旨在解决机器人机器学习中的基础设施挑战，通过模块化设计实现仿真、训练和服务组件的解耦，显著提升了训练效率和系统性能。

结论: 通过构建基于Ray的混合学习平台，研究团队成功解决了机器人机器学习中仿真与训练基础设施的集成难题。该平台通过异构资源调度、运行时依赖隔离和异步训练机制，实现了最高达2倍以上的训练加速。特别是通过教师-学生知识蒸馏框架，能够高效地将 specialized 的强化学习策略迁移到通用的模仿学习模型中，为构建大规模通用机器人策略奠定了坚实基础。未来研究方向包括更细粒度的异步训练优化和跨集群互联速度提升，展现了混合学习方法在机器人领域的巨大潜力。

关键点:

1. [00:01:45](https://youtu.be/JfdPxD2vmIY?t=105s) ​**机器人机器学习的基础架构挑战**：演讲者深入分析了构建机器人ML系统面临的核心基础设施难题。

    - **异构资源需求**：训练基础设施需要高性能计算GPU（如A100、H100），而仿真基础设施则需要RTX系列GPU进行实时光线追踪和渲染，这两种硬件在CUDA版本、PyTorch构建版本和额外依赖方面存在显著差异。
    - **运行时冲突风险**：将训练和仿真组件部署在同一集群中极易产生运行时依赖冲突，导致资源利用率低下和迭代速度减慢。
    - **规模化瓶颈**：随着模型复杂度增加和仿真环境扩展，传统的同址部署方式会严重限制训练效率，特别是在需要处理多模态静态和动态数据流的场景下。
2. [00:10:14](https://youtu.be/JfdPxD2vmIY?t=614s) ​**模块化平台设计原理**：团队提出了基于Ray的模块化框架设计，有效解决了基础设施集成难题。

    - **组件解耦架构**：将仿真、训练和服务三个核心组件分离到不同的计算资源上，每个组件可以独立优化其运行时环境。
    - **灵活的资源调度**：通过Ray集群的自定义计算类型功能，能够将特定任务调度到最适合的硬件资源上，例如将仿真任务分配到RTX GPU，训练任务分配到高性能计算GPU。
    - **通信机制**：各组件通过Ray集群进行高效通信，支持从仿真环境加载数据，无论是基于策略的在线学习还是离线学习场景都能良好支持。
3. [00:11:15](https://youtu.be/JfdPxD2vmIY?t=675s) ​**模仿学习的异步优化方案**：针对传统模仿学习的效率瓶颈提出了创新性解决方案。

    - **同步训练的低效性**：在标准的DDP数据并行机制中，训练需要等待仿真环境完成策略评估，导致GPU利用率低下。
    - **异步仿真提取**：将仿真环境完全提取到独立节点运行，训练过程无需等待仿真结果，可以持续进行。
    - **性能提升效果**：通过解耦训练和仿真组件，并使用更大规模的GPU进行模型推理，实现了至少1.4倍的训练加速，且随着模型复杂度的增加，性能提升效果更加显著。
4. [00:13:59](https://youtu.be/JfdPxD2vmIY?t=839s) ​**强化学习的分布式优化**：针对强化学习特有的数据收集瓶颈提出了分布式解决方案。

    - **仿真吞吐量瓶颈**：在传统的强化学习设置中，仿真渲染和物理计算成为数据收集的主要瓶颈，特别是在需要处理高分辨率视觉输入的场景下。
    - **专用仿真节点**：将仿真任务分配到专用的RTX GPU节点，训练任务分配到高性能计算GPU，实现了资源的专业化利用。
    - **性能基准测试**：相比传统的同址部署方式，分布式方案实现了超过2倍的训练加速，通过进一步增加仿真节点数量，可以持续提升训练效率。
5. [00:18:29](https://youtu.be/JfdPxD2vmIY?t=1109s) ​**教师-学生知识蒸馏框架**：详细介绍了混合学习平台的核心应用场景。

    - **框架原理**：首先训练专门的强化学习教师模型，收集其在仿真环境中的演示数据，然后通过模仿学习将这些知识蒸馏到通用的学生模型中。
    - **可扩展性优势**：每个RL教师模型专注于少量技能，可以并行运行多个教师模型，将它们的知识集中蒸馏到单个学生模型中，实现高效的策略泛化。
    - **硬件映射**：教师训练使用专门的仿真和RL策略GPU，收集的演示数据存储到离线服务器，学生训练在高性能计算集群上进行，验证过程在独立的仿真GPU完成。
6. [00:23:01](https://youtu.be/JfdPxD2vmIY?t=1381s) ​**端到端系统演示**：通过实际演示展示了整个混合学习平台的工作流程。

    - **集群配置**：使用8个A100 GPU作为训练集群，6个RTX ADA 6000 GPU作为仿真集群，通过Ray集群统一管理异构计算资源。
    - **训练监控**：集成Weights & Biases进行实时训练监控，可以同时观察损失函数变化和仿真视频反馈，全面掌握训练进展。
    - **任务完成度**：从训练初期的随机探索到训练后期的精确任务执行，完整展示了机器人从零开始学习拾取立方体任务的全过程，验证了平台的有效性。

## **Ray Train: Distributed Solutions for Removing Training Bottlenecks | Ray Summit 2025**

[https://www.youtube.com/watch?v=BuYkzhlCeEg](https://www.youtube.com/watch?v=BuYkzhlCeEg)

视频介绍：本视频由Anyscale公司的RAID团队软件工程师Justin和Timothy分享，重点介绍了如何利用Ray Train和Ray Data消除分布式训练中的瓶颈问题，通过异构集群架构和异步操作显著提升GPU利用效率。

结论：通过系统性地解决数据加载、容错恢复、检查点和验证等关键瓶颈，Ray生态系统能够将分布式训练运行时间减少超过68%，同时保持成本效益。核心优化策略包括：使用Ray Data实现数据预处理的独立扩展、支持中段 epoch 恢复的快速容错机制、异步检查点上传以及并行验证执行。这些技术共同确保了GPU能够持续高效地进行模型训练，为大规模机器学习工作负载提供了可扩展且经济高效的解决方案。

关键点：

1. [00:02:27](https://youtu.be/BuYkzhlCeEg&t=147s) ​**分布式训练瓶颈的系统性分析**：讲者首先构建了典型的分布式训练系统架构，并深入剖析了三个主要性能瓶颈。

    - 数据加载瓶颈：当PyTorch DataLoader无法以足够快的速度产生批次数据时，GPU会出现周期性空闲。这种瓶颈表现为GPU利用率图表中的尖峰形态，根本原因在于数据读取、转换和整理等CPU密集型操作无法跟上GPU的处理速度。
    - 检查点瓶颈：传统的同步检查点机制会阻塞训练过程，模型需要从GPU拷贝出来并上传到持久存储后，训练才能继续。这段时间内昂贵的GPU资源完全处于闲置状态。
    - 验证阶段瓶颈：验证期间模型参数不更新，导致整体训练吞吐量下降。更重要的是，验证仅需模型推理，计算和内存需求远低于训练，使用昂贵的训练GPU进行验证是资源浪费。
2. [00:05:00](https://youtu.be/BuYkzhlCeEg&t=300s) ​**CPU-GPU资源配比的根本限制**：深入探讨了为什么简单增加DataLoader工作进程数量无法解决数据加载瓶颈。

    - 硬件资源约束：在典型的计算节点配置中（如96个CPU配8个GPU），CPU与GPU的比例固定为12:1，这从根本上限制了能为每个GPU服务的数据处理资源。
    - 资源竞争问题：当数据加载工作进程数量超过某个阈值后，它们会开始竞争有限的CPU资源，导致性能不升反降。
    - 核心洞察：问题的根源在于所有数据加载操作都与训练工作进程同节点部署，这种紧耦合架构限制了系统的扩展性。
3. [00:08:23](https://youtu.be/BuYkzhlCeEg&t=503s) ​**Ray Data的异构集群解决方案**：通过将数据预处理与训练解耦，实现资源的独立扩展。

    - 架构创新：将数据读取、转换和批次整理等CPU密集型任务从训练节点卸载到专门的CPU工作节点，训练节点仅保留轻量级的迭代器来获取预处理好的批次数据。
    - 实现方式：使用Ray Data定义预处理流水线，通过read_parquet和map_batches等API构建数据处理流程，然后将其传递给Ray Train的TorchTrainer。
    - 额外优势：支持GPU预处理阶段、流式数据处理、自动数据集分片以及根据训练消耗吞吐量自动调整数据加载工作进程数量。
4. [00:11:42](https://youtu.be/BuYkzhlCeEg&t=702s) ​**基准测试的性能提升验证**：通过具体实验数据展示了优化效果。

    - 性能改进：在添加CPU节点卸载数据转换任务后，运行时间减少了约68%，这主要得益于消除了每个训练步骤中因数据加载导致的等待时间。
    - 成本效益：尽管增加了额外的CPU节点，但由于GPU节点的运行时间大幅缩短，而GPU节点远比CPU节点昂贵，总体成本反而下降。
    - 资源利用：优化后GPU能够持续保持高利用率，不再出现因数据供给不足导致的空闲时段。
5. [00:12:45](https://youtu.be/BuYkzhlCeEg&t=765s) ​**中段epoch快速恢复机制**：解决了训练过程中故障恢复时的数据状态管理问题。

    - 传统方案缺陷：常见的恢复方法包括跳过已处理的批次或在故障点重新开始训练，这两种方法都会导致GPU长时间空闲和数据处理效率低下。
    - Ray Data解决方案：能够保存和加载全局数据加载器状态，与模型检查点一起存储。恢复时只需从剩余未处理的数据文件开始迭代，无需重复处理或跳过已处理数据。
    - 性能收益：在75%数据处注入模拟故障的测试中，这种机制带来了约10%的运行时间和成本减少。
6. [00:16:14](https://youtu.be/BuYkzhlCeEg&t=974s) ​**异步检查点上传技术**：通过并行化检查点操作来消除训练阻塞。

    - 同步模式问题：传统的ray.train.report API会阻塞训练过程，直到检查点完全上传到持久存储。
    - 异步解决方案：引入checkpoint_upload_mode参数，允许在单独线程中上传检查点，同时GPU可以继续训练。
    - 高级功能：支持通过checkpoint_upload_function参数使用第三方函数（如Torch DCP的async_save）直接从内存上传检查点，Ray Train会自动管理并发线程和内存使用。
7. [00:17:44](https://youtu.be/BuYkzhlCeEg&t=1064s) ​**异步验证执行策略**：将验证过程与训练并行化以进一步提升效率。

    - 实现机制：在单独的Ray任务中运行用户定义的验证函数，使GPU能够继续训练，而验证在后台并行执行。
    - 分布式验证选项：可以使用Ray Data的map_batches API进行高性能批量推理，或者创建专门的Ray Train训练器来运行验证循环。
    - 硬件优化：验证任务可以在更便宜的GPU硬件上运行（如A10G而非A100），因为验证不需要存储和计算梯度，计算需求较低。
8. [00:20:14](https://youtu.be/BuYkzhlCeEg&t=1214s) ​**Ray Train仪表板的诊断能力**：提供了深入的训练运行可视化和分析工具。

    - 运行监控：在Train Runs页面可以查看所有训练运行的列表，点击进入单个运行可以查看训练日志和各种指标。
    - GPU利用率分析：通过Grafana仪表板展示SM占用率图表，这是比传统GPU利用率更准确的指标。优化后的图表显示出更平滑、更高的GPU利用率，消除了数据加载、检查点和验证导致的间隙。
    - 性能剖析：通过设置环境变量可以启用GPU剖析功能，生成Torch性能剖析文件，可通过Chrome Tracing等工具进一步分析。
9. [00:22:44](https://youtu.be/BuYkzhlCeEg&t=1364s) ​**异构硬件独立扩展的整体架构**：总结了通过资源解耦实现性能优化的核心理念。

    - 演进路径：从仅使用训练GPU开始，遇到数据加载瓶颈后通过Ray Data自动扩展CPU节点进行预处理，最后通过在不同硬件上运行验证来进一步解除训练瓶颈。
    - 资源隔离：训练、数据预处理和验证可以在不同类型的硬件资源上独立运行和扩展，避免了资源竞争和低效利用。
    - 成本效益平衡：虽然使用了更多硬件资源，但由于运行时间大幅缩短，总体成本得到优化，特别在训练周期较长或模型较大的场景中效益更加显著。

## **Introducing Terminal-Bench: Evaluating LLM Agents in Realistic Terminal Settings | Ray Summit 2025**

[https://www.youtube.com/watch?v=zt-U4lDdenY](https://www.youtube.com/watch?v=zt-U4lDdenY)

视频介绍：本视频由斯坦福大学博士后Mike Merrill分享，重点介绍了他们开发的Terminal Bench项目——一个针对语言模型在真实终端环境中作为智能体性能的硬基准测试。演讲探讨了评估基准的发展历程、终端环境对AI智能体的重要性，以及该基准的设计理念和未来规划。

结论：Terminal Bench作为一个前沿的智能体评估基准，通过80个基于真实工作场景的Docker化任务，有效衡量了语言模型在终端环境中的实际工作能力。目前最先进模型的成功率仅约50-65%，表明智能体在长时程任务、工具使用和错误恢复等方面仍有显著提升空间。项目团队基于开发经验进一步构建了Harbor平台，支持任意容器化环境的智能体评估和训练，为AI智能体的发展提供了重要的评估基础设施。这些工作凸显了在AI能力快速进步的当下，开发复杂、真实的评估基准对理解和推进前沿模型能力至关重要。

关键点：

1. [00:00:56](https://youtu.be/zt-U4lDdenY?t=56) ​**语言模型评估基准的演进趋势**：

    - 传统评估范式：四年前的语言模型评估主要关注模型的知识储备，采用类似MMLU的多项选择题形式，具有明确的评判标准和单轮交互特点。这类评估易于评分但环境静态，仅考察输入输出匹配度。
    - 现代评估转变：当前评估更注重AI的实际行动能力，考察其在经济中的价值创造。以SWEBench为例，评估智能体能否在给定GitHub问题描述和单元测试的情况下解决问题，涉及工具使用、网络查询等多轮交互。
    - 能力扩展需求：实证研究表明，语言模型的操作时间范围从几分钟扩展到数小时，能够执行缓冲区溢出攻击、绕过机器人保护等复杂任务。这要求评估基准必须更加复杂和长时程，以准确衡量前沿模型的能力边界。
2. [00:04:43](https://youtu.be/zt-U4lDdenY?t=283) ​**终端环境作为智能体测试平台的核心价值**：

    - 文本模态优势：终端基于文本的特性与语言模型的最佳工作模态完美匹配，无需依赖GUI或计算机视觉，可直接接入终端完成实际工作。
    - 强大功能覆盖：终端几乎能执行计算机上的任何任务，是传奇工程师们的主要工作界面，被系统管理员、软件工程师、科学家等各领域专家用于完成高价值工作。
    - 实际效率对比：以配置AWS实例为例，通过GUI需要点击数十个菜单，过程繁琐耗时；而通过CLI工具只需单个命令即可快速完成，体现了终端环境在绕过GUI限制、提升工作效率方面的巨大潜力。
3. [00:07:22](https://youtu.be/zt-U4lDdenY?t=442) ​**终端智能体的产业现状与发展态势**：

    - 主流实验室布局：几乎所有前沿实验室都推出了某种形式的终端智能体，包括Anthropic的Claude Code、OpenAI的Codex CLI、Gemini版本、Cursors的终端智能体等。
    - 工具生态扩展：除了专门的CLI智能体，像Cursor、Manis等知名智能体也使用CLI工具进行文件编辑、Web应用操作等，表明终端shell是智能体控制计算机的肥沃领域。
    - 行业认可度：行业领袖强调，2025年最具影响力的产品中许多都是CLI工具，编码智能体喜爱运行CLI，ChatGPT通过编写调用CLI的脚本解决问题，进一步印证了终端环境的重要性。
4. [00:09:23](https://youtu.be/zt-U4lDdenY?t=563) ​**Terminal Bench基准的设计架构与核心特性**：

    - 任务来源与质量：包含80个由开源社区成员（主要是博士级贡献者）基于真实职业经历编写的任务，每个任务都配有经验证的真实解决方案和测试用例，确保任务可行性且无玩具示例。
    - 环境架构：采用简单的Docker容器规范，每个任务在独立的容器化环境中执行。智能体被安装或激活在容器内，接收指令解决问题，最后通过测试验证完成情况。
    - 设计原则：基准被设计为困难（最先进模型仅50%成功率）、真实（基于实际工作场景）和动态（版本化但不断扩展），随着新模型能力的解锁持续更新。
5. [00:13:18](https://youtu.be/zt-U4lDdenY?t=798) ​**基准接受度与模型性能分析**：

    - 行业影响力：发布3天后即被纳入Claude 4模型卡，是Daario在模型发布时提到的两个基准之一，在GitHub获得1000星标和100多名贡献者，Discord社区超千名成员。
    - 性能现状：在报告的所有基准中得分最低，表明距离基准饱和还有很大改进空间。模型排名显示提供商智能体与自身模型配合最佳（如GPT5与Codex CLI），但也有意外组合（如Claude 4.1与Open Hands表现最佳）。
    - 智能体行为模式：GPT5偏爱使用nano编辑文件但不擅长；某些智能体倾向于指导而非执行任务；Claude 4.1 Opus容易产生幻觉，在工具使用受阻时猜测解决方案而非坚持尝试。
6. [00:17:38](https://youtu.be/zt-U4lDdenY?t=1058) ​**提升Terminal Bench性能的关键因素**：

    - 模型合金技术：通过在不同模型间随机切换API调用，将整个对话上下文在不同提供商间复制，这种集成技术能带来5-6%的性能提升。
    - 模型进步：Sonnet 45是目前在各种智能体中表现最佳的模型，最先进水平已提升至60-65%，推动了基准版本的快速迭代。
    - 脚手架优化：对智能体框架进行定制修改，如增加提前取消长时间运行命令的选项，防止因错误命令导致的超时，能带来5-10%的性能提升。
7. [00:19:42](https://youtu.be/zt-U4lDdenY?t=1182) ​**从Terminal Bench到Harbor平台的演进**：

    - 未预期用例：用户将基准用于提示优化、智能体回归测试、在环境上进行RL rollout等，其他基准制作者请求使用其框架开发自己的基准。
    - 训练需求增长：RL环境训练成为新兴产业，实验室和模型制造商购买定制环境训练智能体。
    - 可扩展性挑战：本地Docker仅支持约6个并发环境，大规模实验需要数百上千个并发环境。
    - Harbor解决方案：支持在任何基准上评估任何智能体，抽象化Terminal Bench代码便于移植其他基准，集成Sky RL，支持Kubernetes、Docker及各种云提供商，实现大规模并行生成和评估。
8. [00:26:57](https://youtu.be/zt-U4lDdenY?t=1617) ​**智能体失败模式分析与改进方向**：

    - 主要失败原因：智能体过度急切，在工具使用困难时产生幻觉；陷入固定行为模式难以跳出（如反复尝试相同策略）；上下文窗口填满后推理能力下降；超过一小时的长时间任务表现不佳。
    - 优势领域：熟练使用shell命令，能生成复杂find命令等用户可能想不到但有效的解决方案，在grep、cat等工具使用上表现优秀。
    - 现实差距：真实工作环境中的指令通常没有基准中那么明确规范，智能体在理解模糊需求方面仍有困难，这是当前能力与经济价值创造之间的主要差距所在。

## **CoServe: Max Performance, Minimal Compute | Ray Summit 2025**

[https://www.youtube.com/watch?v=x-iP98pqHZA](https://www.youtube.com/watch?v=x-iP98pqHZA)

视频介绍：本视频由Cohere公司的Chenia分享，详细介绍了该公司如何优化其推理技术栈，以高效服务Command系列大语言模型。内容涵盖从模型架构设计、推理优化技术到硬件支持的全方位技术方案，展示了企业级语言模型服务在效率与成本效益方面的创新实践。

结论：Cohere通过创新的模型架构设计、多维度推理优化技术和全面的硬件支持，成功构建了高效且成本可控的企业级语言模型服务体系。其核心技术包括交错滑动窗口注意力机制、推测解码、多种量化方案以及针对不同硬件平台的深度优化，在保持模型质量的同时显著提升了推理效率。这些优化使得Command系列模型能够在少量GPU上支持超长上下文，为企业客户提供了可部署于多种环境的灵活解决方案，推动了大规模语言模型在真实业务场景中的实际应用。

关键点：

1. [00:02:04](https://youtu.be/x-iP98pqHZA&t=124s) ​**模型效率团队的三维工作架构**：Cohere的小型模型效率团队支撑着整个公司的推理技术栈，其工作系统性地分为三个关键领域。

    - **计算优化**：团队负责处理多种硬件平台，开发不同精度的计算内核，设计调度策略，并为各种架构构建理论性能模型。这确保了模型在不同硬件环境下都能获得最佳性能表现。
    - **量化技术**：开发量化方案并在不同评估集上验证精度是核心工作。量化能极大提升服务效率，但前提是必须维持模型质量，团队在此平衡点上进行了深入探索。
    - **效率算法**：探索预填充和解码阶段的前沿技术，如推测解码，并针对结构化输出和工具使用等日益重要的企业应用场景开发高效算法。
2. [00:03:30](https://youtu.be/x-iP98pqHZA&t=210s) ​**Command A系列模型的设计哲学与卓越表现**：Command A模型体现了可扩展效率的核心设计理念，在性能与成本间取得了出色平衡。

    - **高效部署**：仅需2张A100（80GB）或H100 GPU即可服务，计算需求远低于市场同类模型，这不仅关乎成本，更使得许多无法负担大规模GPU集群的企业也能使用先进AI技术。
    - **性能领先**：在1K上下文长度下，Command A能达到每秒158个输出令牌，性能显著超越GPT-4和DeepSeek-V3。同时支持256K超长上下文，赋能强大的检索增强生成、可验证引用、智能体工具使用等功能。
    - **质量保证**：在MMLU、数学智能体任务和编码挑战等学术基准测试中，Command A表现与DeepSeek-V3和GPT-4O相当或更优，证明效率提升并未牺牲质量。
3. [00:07:27](https://youtu.be/x-iP98pqHZA&t=447s) ​**Command模型六大核心架构创新**：理解模型架构是优化策略的基础，Command模型包含六项关键设计。

    - **交错滑动窗口注意力**：以特定模式混合滑动窗口和全注意力层。
    - **RoPE与NoPE交替使用**：全注意力层使用NoPE，滑动窗口注意力层使用RoPE。
    - **并行Transformer块处理**：并行处理注意力和前馈网络，提升硬件利用率。
    - **无偏置设计**：简化架构，减少内存占用。
    - **分组查询注意力**：减少KV缓存大小。
    - **共享输入输出嵌入**：参数共享进一步优化内存。 这些设计，尤其是交错注意力机制，已被Llama 4和GPT OSS等后续模型采纳，验证了其正确性。
4. [00:08:51](https://youtu.be/x-iP98pqHZA&t=531s) ​**交错滑动窗口注意力的深度解析与生产化实践**：这是Cohere最重要的设计之一，通过精巧的注意力机制实现内存与性能的双重优化。

    - **核心参数**：滑动窗口大小为4096，以3:1的比例与全注意力层交错，即每1个全注意力层对应3个滑动窗口层。这种设计带来了75%的内存节省和吞吐量的大幅提升。
    - **设计洞察**：关键在于并非每一层都需要看到整个序列，大多数层处理局部信息即可。这在不牺牲长上下文性能的前提下，极大地释放了KV缓存空间。
    - **生产价值**：在生产环境中，内存往往是瓶颈而非算力。通过减少75%的KV缓存，可以在相同硬件上服务更多并发请求，这对于高效服务长上下文模型至关重要。
5. [00:10:34](https://youtu.be/x-iP98pqHZA&t=634s) ​**定制化内存分配器实现架构潜力**：为了在生产中充分发挥交错注意力架构的优势，Cohere开发了定制化的内存分配器。

    - **设计前提**：基于固定嵌入大小、每层相同数据类型以及严格的3:1滑动窗口与全注意力层比例。
    - **分组策略**：Command A的64层被分为48层滑动窗口和12层全注意力，并组织成5个组。这种分组策略实现了无需填充，避免了内存浪费。
    - **前缀缓存优化**：利用全注意力和滑动窗口注意力之间的交集来维持高缓存命中率。对于全注意力层，缓存令牌0-6；对于滑动窗口层，则计算在窗口长度下哪些令牌可以被缓存。
    - **社区协作**：团队与社区合作开发了Djangga，提供更灵活的嵌入大小和更好的缓存驱逐策略。这个混合分配器对于实现架构的实际生产价值至关重要。
6. [00:12:13](https://youtu.be/x-iP98pqHZA&t=733s) ​**推测解码技术显著降低推理延迟**：Cohere基于Eagle 1实现了推测解码（不含树注意力），使其更简单实用。

    - **工作原理**：使用轻量级草稿模型预测多个未来令牌，然后主Transformer并行验证这些预测。
    - **关键技术**：在草稿模型的注意力层应用滑动窗口注意力以保持一致性；对草稿模型进行FBA量化以最小化开销；CUDA图优化减少内核启动开销；与混合内存分配器集成确保无缝协作。
    - **效率优势**：当预测正确时，推测解码基本上是"免费"的加速；即使预测错误，验证过程也很快，损失很小。这在生成企业应用常见的连贯结构化文本时能带来显著的每秒令牌数提升。
7. [00:13:41](https://youtu.be/x-iP98pqHZA&t=821s) ​**W8A8量化策略在效率与质量间的完美平衡**：8位权重和8位激活的量化方案是Cohere效率战略的另一支柱。

    - **技术方案**：对权重进行静态逐通道量化，在模型准备阶段一次性完成；对激活进行动态逐令牌量化，在推理过程中实时处理。
    - **显著收益**：相比BF-16，实现了2倍以上的吞吐量提升和内存节省，且精度损失极小。团队特别肯定了M-compressor内核的贡献，特别是Neuromagic团队的工作使得量化能轻松集成到生产系统中。
    - **应用价值**：W8A8在性能、效率和质量间找到了最佳平衡点，精度损失小到大多数应用无法察觉，但效率增益却非常显著。
8. [00:14:53](https://youtu.be/x-iP98pqHZA&t=893s) ​**W4A16内核深度优化与硬件级性能挖掘**：4位权重和16位激活的量化方案从内核优化角度展现了更多可能性。

    - **性能瓶颈发现**：在对最先进的W4A16内核Machete进行性能剖析时，发现在低批次解码场景下的内存绑定性能不理想，未能接近GPU内存带宽极限。
    - **根本原因**：通过详细剖析发现，Hopper中的张量内存加速器加载了2倍于所需的字节数，明显低效。
    - **巧妙修复**：仅通过一行代码更改调整TMA框大小或布局，就实现了30%的性能提升。这体现了理解硬件细节和细致性能剖析的巨大价值。
    - **功能扩展**：团队还将内核扩展到支持更小的分组大小（低至64）和非对称量化，使其对不同模型和用例更加灵活，这些改进已合并到vLLM中惠及整个社区。
9. [00:16:28](https://youtu.be/x-iP98pqHZA&t=988s) ​**W4A8量化方案实现最佳性能与效率组合**：4位权重和8位激活的量化结合了两种方案的优势。

    - **技术原理**：4位权重节省内存空间和带宽需求，8位激活则能利用现代GPU（如Hopper）上的FP8计算能力，这对性能至关重要。
    - **内核挑战与创新**：由于开源生态中缺乏高性能的W4A8内核，团队使用Cutlass针对Hopper架构构建了一个。关键创新是采用基于查找表的方法，将int4直接映射到FP8，完全绕过了传统转换的性能瓶颈。
    - **工程优化**：初始的Python权重重打包实现需要超过10分钟，团队为此开发了自定义CUDA内核，将启动时间从10多分钟减少到不到1分钟，使W4A8量化在实际部署中变得可行。
10. [00:19:42](https://youtu.be/x-iP98pqHZA&t=1182s) ​**W4A8量化配方从底层设计的质量保证**：实现良好的W4A8性能不仅关乎内核，更取决于量化配方本身。

     - **初始挑战**：尝试利用现有的W4A16工作流，但简单地将FP16分组尺度转换为FP8会导致糟糕的结果，出现大量精度损失和不良粒度。
     - **解决方案**：开发了从底层设计的定制化量化流程，专门为FP8尺度而非从FP16适配而来。这涉及仔细的校准和特定技术，确保FP8尺度保留足够的精度以维持模型质量。
     - **开源计划**：团队正与Elm compressor维护者合作开源此配方，让整个社区都能有效使用W4A8。
11. [00:21:02](https://youtu.be/x-iP98pqHZA&t=1262s) ​**多GPU通信优化策略提升系统整体性能**：在多GPU服务模型时，通信优化变得至关重要。

     - **技术基础**：利用对称内存优化GPU到GPU的集体通信操作，通过在所有GPU上使用具有相同虚拟地址的缓冲区来实现。
     - **性能分析**：基准测试显示，NCCL 2.27在reduce操作上优于vLLM的默认实现，特别是在消息尺寸较大时。vLLM的自定义实现在小消息尺寸上有竞争力，但在约1MB及以上时开始落后。
     - **混合策略**：根据消息尺寸选择不同的通信策略：对非常小的消息使用vLLM的方法，对较大的消息使用NCCL。这显著改善了首令牌时间，特别是在通信量大的大批次场景中。
12. [00:22:28](https://youtu.be/x-iP98pqHZA&t=1348s) ​**全方位硬件平台支持与新一代硬件优化**：Cohere在所有硬件平台上组合应用这些优化技术，支持各种部署需求。

     - **部署灵活性**：支持仅用两张H100服务256K以上上下文长度，这对实现长上下文服务实用性至关重要。在单个B200或MI300X上支持256K以上上下文，为下一代硬件提供更高效率。
     - **B200深度优化**：针对Blackwell架构的优化包括利用FlashInfer和TensorRT-LLM内核、使用CUDA图最小化内核调度开销、调整代码对称内存以优化B200上的新NVLink配置。
     - **性能成果**：在TP8配置下，B200上的推测解码能达到近200 tokens/秒，相比H100的约135有40-50%的提升。这些硬件优化直接转化为客户更低的成本和更好的延迟体验。

## **High-Performance LLM Serving on Intel: vLLM for XPU, HPU &amp; CPU | Ray Summit 2025**

[https://www.youtube.com/watch?v=mv9oSYdDAy8](https://www.youtube.com/watch?v=mv9oSYdDAy8)

视频介绍：本视频由英特尔工程师分享，详细介绍了在英特尔全平台（包括CPU、GPU和Gaudi）上运行和优化vLLM（大型语言模型推理服务框架）的最新进展和性能表现。内容涵盖平台支持现状、大规模推理解决方案以及量化技术应用。

结论：英特尔通过深度参与vLLM开源社区，为其三大计算平台（CPU、GPU、Gaudi）提供了全面的推理加速支持。在性能方面，Granite Rapids CPU相比竞品展现出色吞吐量，Arc Pro B60 GPU在性价比上优势明显，而Gaudi 3在吞吐量和成本效益上均超越H200。通过PD分拆架构和量化技术的创新，英特尔为大规模语言模型推理提供了高性能、低成本的解决方案。未来还将继续推进异构架构支持和智能路由策略，进一步提升系统效率。

关键点：

1. [00:00:31](https://youtu.be/mv9oSYdDAy8&t=31s) ​**英特尔对vLLM的深度贡献**：演讲者详细阐述了英特尔作为vLLM项目的顶级贡献者，在过去两年中持续投入开发。

    - **社区贡献排名**：英特尔在vLLM项目的代码提交量位列所有组织中的前十名，仅次于Hugging Face，体现了公司在开源AI基础设施领域的深度参与。
    - **功能贡献分类**：主要贡献集中在三大方向：英特尔CPU支持、英特尔GPU支持和英特尔Gaudi支持，同时还贡献了PD分拆、量化技术和模型支持等通用功能。
    - **专项项目开发**：除了主代码库贡献外，英特尔还维护两个重要子项目：vLLM Gaudi插件和vLLM XPU内核，专门针对特定硬件平台进行优化。
2. [00:02:15](https://youtu.be/mv9oSYdDAy8&t=135s) ​**vLLM平台支持架构**：详细解析了vLLM在不同硬件平台上的软件架构设计。

    - **硬件无关组件**：vLLM的核心组件如调度器、引擎核心、建模方法和量化技术采用硬件无关设计，可在所有平台上共享使用。
    - **平台特定优化**：模型运行器操作实现和注意力内核等底层组件需要针对不同平台进行专门优化，英特尔为此开发了与CUDA API兼容的Flash Attention内核。
    - **分层架构展示**：通过颜色编码清晰展示了vLLM软件栈的分层结构，灰色代表通用组件，深蓝色和浅蓝色分别代表平台特定组件和插件，英特尔的工作主要集中在后两者。
3. [00:03:33](https://youtu.be/mv9oSYdDAy8&t=213s) ​**三大平台功能支持现状**：系统梳理了vLLM在英特尔各平台上的功能完备性。

    - **Gaudi平台**：支持最全面的功能，包括eager模式、graph模式、torch编译模式，数据并行和专家并行，PD分拆通过Nexo良好支持，推测解码目前支持单draft token。
    - **GPU平台**：当前基于Battle Mage B60，支持eager模式、torch编译模式和PD分拆，Triton内核可原生运行，graph模式和多token推测解码仍在开发中。
    - **CPU平台**：不支持graph模式，推测解码和PD分拆功能正在开发中，主要面向1B-8B规模的小型模型推理。
4. [00:06:27](https://youtu.be/mv9oSYdDAy8&t=387s) ​**CPU性能基准测试**：展示了英特尔各代CPU在vLLM上的性能表现。

    - **平台演进**：从Sapphire Rapids、Emerald Rapids到最新的Granite Rapids（第六代），Granite Rapids提供128物理核心和多种内存配置选项。
    - **性能对比**：在公开基准测试中，Granite Rapids相比AMD EPYC 9755吞吐量提升约1.4倍，相比9965提升约2.7倍；在1k输入/1k输出token场景下仍保持2倍优势。
    - **实际应用数据**：MLPerf 5.1提交数据显示，单节点Granite Rapids运行Llama 3.1 8B模型，离线推理吞吐量表现优秀，但在线服务模式受SLA限制（TTFT 1秒，TPOT 100毫秒）。
5. [00:09:02](https://youtu.be/mv9oSYdDAy8&t=542s) ​**GPU产品布局与性价比**：介绍了英特尔GPU产品路线图和性能优势。

    - **产品系列**：当前提供Arc Pro B60（24GB显存），下一代Crest Island数据中心GPU将提供160GB显存和原生MX FP4支持。
    - **性能表现**：8卡B60集群在SLA限制下可服务蒸馏版Llama 8B、Qwen 40B和Llama 70B模型。
    - **成本效益**：MLPerf 5.1数据显示，Arc Pro B60的每美元性能相比Nvidia RTX Pro 6000提升1.25倍，相比L40S提升4倍。
6. [00:10:23](https://youtu.be/mv9oSYdDAy8&t=623s) ​**Gaudi竞争优势分析**：深入比较了Gaudi与Nvidia竞品的性能表现。

    - **产品规格**：Gaudi 3每卡提供128GB设备内存，支持大规模模型推理。
    - **性能基准**：公开测试显示，在128输入/128输出token场景下，Gaudi 3吞吐量比双H200配置提升约1.27倍。
    - **成本优势**：每美元性能方面，Gaudi 3相比B200提升1.65倍，相比H200提升3.27倍；在vLLM基准测试中，对H100的TCO优势超过3倍。
7. [00:12:31](https://youtu.be/mv9oSYdDAy8&t=751s) ​**大规模推理与PD分拆架构**：探讨了异构架构下的大规模推理解决方案。

    - **架构理念**：基于与斯坦福大学的联合研究，提出HEROGS架构，允许用户继续使用CUDA设备（H200/B200）处理预填充阶段，同时利用Gaudi 3处理解码阶段。
    - **技术实现**：采用LMD路由策略进行请求分发，通过vLLM的Nexo连接器以请求粒度传输KV缓存，支持前缀缓存优化。
    - **传输优化**：提供英特尔定制版Nexo实现，结合英特尔UCX支持CUDA到Gaudi的设备直连传输，通过RDMA提升效率。
8. [00:14:19](https://youtu.be/mv9oSYdDAy8&t=859s) ​**异构架构技术挑战**：详细分析了支持异构架构需要解决的核心技术难题。

    - **后端支持**：需要实现GPU到Gaudi的直接后端支持，目前缺乏现成解决方案。
    - **KV布局异构性**：不同平台使用不同的KV布局格式（KV数量外层维度或块大小外层维度），需要统一的处理机制。
    - **张量并行异构**：支持预填充阶段使用TP1、解码阶段使用TP2或TP4的混合并行策略。
    - **块大小转换**：CUDA使用16的块大小，Gaudi使用128的块大小，需要在传输过程中进行块重组和映射。
9. [00:19:46](https://youtu.be/mv9oSYdDAy8&t=1186s) ​**量化技术全面支持**：总结了各平台对量化模型的支持情况和创新贡献。

    - **平台支持矩阵**：Gaudi对原生FP8支持最全面，包括逐通道量化和逐块量化，同时支持压缩张量格式；GPU支持原生FP8和MX FP4；CPU主要支持W8A8及AWQ、GPTQ方法。
    - **性能数据**：在Arc Pro B60上运行GPTQ OSS MXFP4模型，单卡支持200亿参数模型，四卡支持1200亿参数模型，吞吐量表现良好。
    - **算法创新**：英特尔贡献了AutoRound算法到vLLM，在4bit量化下实现接近16bit的精度，显著优于其他4bit量化方法，目前正集成到vLLM压缩器中。
10. [00:22:30](https://youtu.be/mv9oSYdDAy8&t=1350s) ​**总结与未来展望**：概括了英特尔在vLLM生态中的战略定位和发展方向。

     - **平台定位**：CPU专注于小模型推理和嵌入任务，提供合理的在线服务SLA；GPU展现强劲的TCO优势；Gaudi在服务超大规模语言模型方面提供竞争优势。
     - **模型规模**：Gaudi能够以BF16精度服务超过4000亿参数模型，以FP8精度服务超过8000亿参数模型。
     - **资源获取**：提供完整的GitHub资源链接和Docker运行命令，方便用户快速开始使用英特尔平台上的vLLM服务。

## **Scaling LLMs at Apple: Ray Serve + vLLM Deep Dive | Ray Summit 2025**

[https://www.youtube.com/watch?v=QcmA5zsIvjU](https://www.youtube.com/watch?v=QcmA5zsIvjU)

视频介绍：本视频由Apple数据平台团队的Ankur、Rahan和Deepak共同分享，详细介绍了在Apple内部如何基于VLM和Ray构建企业级开源模型托管平台，以应对快速发展的生成式AI环境中的挑战。

结论：Apple通过构建基于VLM和Ray的开源模型托管平台，成功解决了企业级AI应用中的无缝访问、治理简化、性能可靠性和成本效益等核心挑战。该平台通过统一的模型发现、分层的生命周期管理、智能的部署策略和全面的可观测性体系，为大规模企业应用提供了稳定可靠的模型服务能力。未来的发展方向包括更智能的自动扩缩容、模型多路复用和集成的成本可观测性，展现了企业级AI基础设施的成熟演进路径。

关键点：

1. [00:01:52](https://youtu.be/QcmA5zsIvjU&t=112s) ​**企业级开源模型托管的四大核心挑战**：演讲者详细阐述了在企业环境中托管开源模型时面临的关键需求。

    - 无缝访问：在企业环境中，用户群体多样化，需要提供简单易用的模型发现、重用和接入流程。统一的模型发现界面对于用户快速找到适合其用例的模型至关重要。
    - 治理简化：需要建立一致、合规且强大的访问控制机制，这不仅能建立用户对平台的信任，还能减少用户确保模型正确使用的管理开销。
    - 性能与可靠性：对于任何生产用例而言，性能和可靠性都是关键因素。随着新模型以极快速度发布，需要强大的工具支持快速模型部署和升级。
    - 成本效益：系统需要保持成本效益才能持续可行，这要求平台在提供高质量服务的同时优化资源利用率。
2. [00:04:13](https://youtu.be/QcmA5zsIvjU&t=253s) ​**用户旅程视角的平台设计**：从用户使用流程的角度深入分析了构建生产级平台的关键考虑因素。

    - 统一发现：用户在构建AI应用或智能体时，需要在不同地方探索和搜索模型。提供单一视图界面让用户能够搜索、探索并识别适合其用例的模型。
    - 审批流程：在大型企业中，使用模型通常需要经过审批流程。简化的审批流程确保系统能够满足开源模型的使用条款要求。
    - 模型权重管理：建立明确的路径帮助用户下载模型并将其存储在受治理的存储中，这有助于实现合规性、启用使用情况的可追溯性和审计。
    - 端点抽象：从用户角度看，他们只关心能够访问和集成的端点，而基础设施层面的自动扩缩容、安全性、成本和可观测性等功能都是隐式的。
3. [00:05:47](https://youtu.be/QcmA5zsIvjU&t=347s) ​**模型生命周期管理策略**：详细介绍了从模型引入到退役的完整管理流程。

    - 模型引入：新模型通过明确定义的路径进入环境，使用Ray作业下载模型并进行必要的量化等转换处理。
    - 低代码部署：基于VLM和Ray构建的低代码框架能够将模型权重快速转化为一键部署的端点。
    - 持续维护：需要支持模型权重更新、就地升级等能力，同时为多租户环境提供访问管理、成本和可观测性支持。
    - 退役管理：模型具有生命周期，当使用量减少或有更好能力的模型出现时，需要有序地停用旧模型。
4. [00:07:21](https://youtu.be/QcmA5zsIvjU&t=441s) ​**分层部署策略**：介绍了基于使用模式的分层管理方法。

    - Tier 1层级：主要用于研究或实验性模型，这些模型使用量较低或处于早期部署阶段。支持研究、批量推理或数据生成等用例，当使用量减少时会退役，但当租户认为某个模型对其应用至关重要时，会升级到Tier 0。
    - Tier 0层级：包含大型模型或具有高持续使用量的模型，通常服务于延迟和SLA敏感的工作负载。这些模型经过基准测试，有发布的指标，为客户提供可靠性保证。
    - 层级转换：模型可以在Tier 1和Tier 0之间动态转换，根据使用情况和客户需求进行升级或降级。
5. [00:08:37](https://youtu.be/QcmA5zsIvjU&t=517s) ​**多租户与单租户部署策略**：深入探讨了不同层级的部署架构选择。

    - 多租户部署：适用于Tier 1模型，这些模型可以缩放到零，优化成本，权衡是冷启动时间，但对于早期阶段和非SLA绑定的模型来说是可接受的。
    - 单租户部署：当模型使用量增加时，会迁移到Tier 0并转为单租户部署。这种部署方式提供性能保证，确保良好的SLA，为单个用户或一组客户提供预置的吞吐量保证。
    - 水平扩展：单租户部署通常包含多个模型端点副本，保证为客户提供服务，并在流量到达时水平扩展。
6. [00:09:44](https://youtu.be/QcmA5zsIvjU&t=584s) ​**治理与可观测性架构**：详细阐述了平台治理和监控的关键组件。

    - 管理能力：需要统一的管理界面或API来部署、更新或停用模型，管理模型生命周期。
    - 安全控制：在多租户环境中，需要身份验证、授权和速率限制等安全措施，确保资源的公平共享。
    - 性能优化：引入缓存机制来保持性能、可扩展性和容错能力，减少因治理措施带来的延迟和可靠性问题。
    - 全面可观测性：从Kubernetes、Ray集群到VLM引擎等多个层面收集日志、追踪和指标，在一个地方聚合所有数据，为用户、基础设施提供商和操作员提供不同视角的监控视图。
7. [00:11:13](https://youtu.be/QcmA5zsIvjU&t=673s) ​**四维监控指标体系**：系统性地介绍了生产级模型服务平台需要监控的关键指标类型。

    - Token指标：跟踪每个模型的使用情况，对于配额管理、计费以及更好地理解模型扩缩容和流量需求至关重要。
    - API性能指标：包括首次Token时间、吞吐量和端到端延迟等指标，对于定义和保证客户SLA至关重要。
    - 速率限制指标：识别配额违规，确保一个客户的突发流量不会降低其他客户的服务质量。
    - 错误率指标：识别、诊断和调试系统中断，并预防未来的系统故障。
8. [00:12:14](https://youtu.be/QcmA5zsIvjU&t=734s) ​**智能自动扩缩容优化**：深入分析了模型服务中自动扩缩容的三个关键优化维度。

    - 模型加载优化：更快的模型加载减少了模型副本的冷启动时间，这对于应对突发流量时的模型扩展至关重要。并行模型下载技术可以将从冷存储加载模型权重的时间减少约2800%。
    - 模型传输优化：缓存权重在设备附近甚至设备上，使用网络文件系统等技术，可以通过避免昂贵的网络往返将模型权重传输时间减少高达53%。
    - 空闲GPU优化：随着模型硬件需求增长和延迟SLA缩短，空闲GPU时间成本越来越高。智能的缩减规则允许在几分钟而不是几小时内缩减模型副本，避免昂贵的硬件资源浪费。
9. [00:13:32](https://youtu.be/QcmA5zsIvjU&t=812s) ​**未来演进方向**：展望了生成式AI系统平台的未来发展路径。

    - 扩展弹性改进：包括自动扩缩容改进和模型流式处理，支持更响应的扩缩容以应对突发流量。
    - 资源优化：模型多路复用和更智能的GPU利用率编排，允许用更少的GPU服务更好、更大的LLM。
    - 性能提升：延迟调优和批处理调度等技术，从服务的LLM中获得更多性能。
    - 成本透明化：集成的成本可观测性仪表板和实时Token到成本的映射，为客户提供透明的计费模式，共同创造最佳最终用户体验。

## **State of vLLM 2025 | Ray Summit 2025**

[https://www.youtube.com/watch?v=tN_-nktp1Hk](https://www.youtube.com/watch?v=tN_-nktp1Hk)

视频介绍：本视频是vLLM项目负责人Simon在Ray Summit上的主题演讲，全面回顾了vLLM开源推理和服务引擎在过去一年的发展成果与未来规划。作为目前最受欢迎的LLM推理引擎之一，vLLM致力于构建最快、最易用的开源推理解决方案，支持在各种数据中心硬件上运行开源和专有的大语言模型。

结论：vLLM项目在过去一年取得了显著进展，在API功能、模型支持、引擎核心、硬件生态和分布式能力五个关键领域都实现了重大突破。项目通过v1引擎重构大幅提升了默认性能，扩展了多模态模型支持，建立了完善的硬件插件生态系统，并在分布式推理方面取得了重要进展。随着社区贡献者接近2000人、月运行GPU小时超过40万，vLLM正成为企业LLM部署的核心基础设施。未来，vLLM将继续推进全智能体API、更强大的多模态支持、异步调度优化等前沿功能，为大规模AI推理提供更强大的基础设施支持。

关键点：

1. [00:00:51](https://youtu.be/tN_-nktp1Hk&t=51s)​**vLLM项目的核心目标与社区生态**：Simon明确了vLLM项目的核心使命是构建最快、最易用的开源推理和服务引擎。

    - 项目定位：专门负责在数据中心硬件上运行各种大语言模型，包括在NVIDIA GPU上运行Llama、在AMD GPU上运行GPTOSS等场景。
    - 社区规模：作为开源项目，vLLM拥有接近2000名贡献者，其中核心日常贡献者约50人，包括大型组织和初创公司都在共同推动vLLM成为其核心战略组成部分。
    - 采用数据：项目已获得超过6万GitHub星标，通过有限遥测数据观察到月运行GPU小时超过40万，且保持7×24小时持续运行，显示了其在生产环境中的广泛采用。
    - 开发活跃度：每月处理约800个pull request，体现了社区的高度活跃性和持续创新动力。
2. [00:03:10](https://youtu.be/tN_-nktp1Hk&t=190s) ​**API功能的全面演进与智能化方向**：vLLM Serve的API功能从基础推理扩展到全智能体和强化学习原生支持。

    - 传统API基础：保留LM类接口用于大批量推理，支持页面注意力和连续批处理，可作为Hugging Face Transformers的替代方案；同时提供OpenAI兼容的API服务器，支持本地部署和直接对接现有客户端代码。
    - 多模态扩展：原生支持图像、音频、视频等多模态模型服务，提供重排序、池化和嵌入API，这些功能特别适用于RAG系统和奖励建模场景。
    - 智能体支持：新增Responses API支持通道交错和工具使用，可与GPTOSS等模型深度集成，并正在与Minimax等团队合作优化这类API的使用体验。
    - 云原生集成：提供Sagemaker和其他云原生API的直接支持，用户可轻松集成到现有云平台中。
    - 强化学习优化：新增强化学习专用API，支持token输入输出，避免重复的token化处理，为训练流程提供更精确的控制。
3. [00:06:50](https://youtu.be/tN_-nktp1Hk&t=410s) ​**模型支持策略与多模态趋势**：vLLM采用严格优先级策略支持前沿模型，并观察到多模态融合的重要趋势。

    - 模型覆盖广度：支持超过200种架构，并持续为零日发布的新架构提供支持，通过与Hugging Face合作集成Transformers后端，进一步扩展了模型支持范围。
    - 优先级策略：采用严格的优先级堆栈，在模型发布时优先确保准确性，即使需要牺牲部分性能也要保证模型输出与原始版本完全一致，这是通过提前访问模型和严格测试实现的。
    - 确定性保证：新推出的确定性批处理不变模式确保无论提示顺序如何，都能获得完全一致的token概率匹配，这对强化学习场景特别重要。
    - 性能优化：建立昂贵的性能测试回归系统，每月投入超过10万美元确保每个提交都经过充分测试，防止性能回退。
    - 多模态突破：支持Qwen-VL、DeepSeek OCR等先进多模态模型，观察到2025年多模态趋势是模型在保持文本能力的同时原生集成多模态理解能力。
4. [00:11:56](https://youtu.be/tN_-nktp1Hk&t=716s) ​**v1引擎架构重构与性能突破**：完全重写的v1引擎解决了多种推理优化的组合挑战，带来显著性能提升。

    - 架构革新：v1引擎是vLLM代码库中的全新实现，已完成所有用户的迁移，旧代码已从代码库中删除，用户现在使用的都是全新架构。
    - 优化组合挑战：解决了前缀缓存、推测解码、分块预填充、结构化输出、级联注意力、CPU卸载等多种优化技术之间的相互影响问题，这些技术都涉及核心调度器的修改。
    - 功能矩阵完善：通过系统性的功能兼容性测试，将原本半数不兼容的功能组合转变为几乎全部默认开启且协同工作的状态。
    - 性能提升：用户报告开启v1引擎后获得约2倍的性能提升，得益于优化的引擎循环、API服务器改进、更简单灵活的调度器以及默认开启的前缀缓存。
    - 并行架构：提供清晰的并行架构，支持所有类型的并行化策略，包括高效的输入准备、Torch编译原生集成和多模态原生支持。
5. [00:15:20](https://youtu.be/tN_-nktp1Hk&t=920s) ​**异步调度与内存管理创新**：正在开发的全重叠调度和混合内存分配器进一步优化资源利用率。

    - 异步调度进展：全重叠调度（异步调度）已进入主分支，接近默认开启状态，该功能可减少CPU空闲时间，在小型模型运行于快速GPU如NVIDIA Blackwell时，最多可消除20%的执行开销。
    - 实现复杂性：异步调度需要重新同步所有其他功能，涉及整个架构的重新思考，体现了vLLM团队在工程复杂性管理方面的专业能力。
    - 混合内存分配器：针对非上下文模型（特别是推理模型）的混合注意力机制，包括滑动窗口或状态空间模型，提供了高效的KV缓存内存管理解决方案。
    - KV连接器：定义了在vLLM实例、GPU到CPU区域以及磁盘区域之间移动KV缓存的接口，支持用户实现自己的KV连接器和稀疏注意力机制。
    - 生态系统集成：与LM Cache、Moon LMD等生态系统组件深度集成，支持解码上下文并行等先进技术。
6. [00:18:16](https://youtu.be/tN_-nktp1Hk&t=1096s) ​**硬件生态系统扩展与性能领先**：vLLM建立了完善的硬件插件生态系统，在各类硬件上都展现出领先性能。

    - 基准测试领导地位：参与Semi Analysis的Inference Max基准测试，确保在所有前沿硬件世代上vLLM都能作为核心引擎提供最佳性能。
    - 硬件合作：持续与AMD和NVIDIA合作，不仅优化当前主流硬件（H100、MI300X）的支持，还提前适配最新世代硬件（B200、MI350）和下一代集群级设备。
    - TPU支持突破：宣布支持vLLM TPU，这是与Google合作的重要成果，通过PyTorch到JAX的转换层，在TPU上提供最先进的性能，无需用户额外投入。
    - 插件架构：建立完整的硬件插件生态系统，用户可通过pip安装vLLM和特定硬件插件，自动获取最新的模型支持、API服务器和硬件特定驱动。
    - 性能优势：在多项基准测试中，vLLM在某些场景下甚至优于硬件厂商的原生解决方案，如在某些GPT模型上表现优于NVIDIA的TRT-LM。
7. [00:21:34](https://youtu.be/tN_-nktp1Hk&t=1294s) ​**分布式推理架构与算法创新**：分布式成为vLLM今年的重点投入领域，提供多种先进的分布式算法。

    - 集群内视角：关注并行化的容错性和弹性，研究模型如何在分布式环境中提供服务。
    - 集群间视角：解决路由、缓存和操作问题，与参考架构深度合作。
    - 生态系统协作：与LMD和NVIDIA Dynamo等生态系统组件协作，确保vLLM在不同分布式场景中开箱即用。
    - 智能调度算法：LMD提供智能推理调度，使用LLM感知的负载均衡来更智能地路由请求。
    - 高级并行技术：支持PD解聚合，可在预填充实例和处理响应的解码实例之间传输KV缓存；实现宽专家并行，可在数百个GPU之间交换token以实现注意力机制和专家并行的最大效率。
    - 专业化组件：原生捆绑EPLB（专家并行负载均衡器）等授权组件，确保工作负载始终获得最佳的专家分布。
    - 通信优化：深度技术工作专注于双重批处理重叠，确保在数百个GPU场景中能够重叠通信和计算，实现最大效率。

## **Ray Summit 2025 Keynote: Physical AI Turing Test with Jim Fan from NVIDIA**

[https://www.youtube.com/watch?v=7fDiui8cAVQ](https://www.youtube.com/watch?v=7fDiui8cAVQ)

结论：这段演讲从克劳德·香农在麻省理工学院博物馆的“残局机器”讲起，探讨了人工智能在解决棋类游戏等抽象问题上的巨大成功，进而引出了当前AI面临的下一个，也可能是最后一个重大挑战：让机器人像人类一样在混乱、不可预测的物理世界中自如地完成日常任务，即通过“物理图灵测试”。演讲的核心在于揭示机器人技术面临的“数据困境”以及英伟达团队如何通过创新的数据策略和模型策略来攻克这一难题。演讲系统地阐述了机器人技术面临的核心挑战——数据稀缺，并详细介绍了英伟达通过构建多层次合成数据“核燃料”（从精确的物理仿真到基于生成式AI的神经模拟器）来应对这一挑战的创新方法。其“数据最大化，模型最小化”的理念，以及由此开发的Groot VLA模型，为实现通用的物理智能奠定了坚实基础。展望未来，物理API的出现将彻底改变我们与物理世界互动的方式，开启一个由机器人无缝协助日常生活、加速科学和工业发展的新时代，最终让高级机器人技术变得无处不在且毫不起眼。

**关键要点**：

1. [00:00:05](https://youtu.be/7fDiui8cAVQ?t=5) 从抽象智能到物理智能的挑战：

    - 演讲者以香农的“残局机器”为例，指出早期AI研究者曾认为解决国际象棋就是AI的终极目标。
    - 过去70年，AI在棋类、扑克、电子游戏乃至数学奥林匹克和蛋白质折叠等抽象领域取得了远超预期的成就。
    - 然而，一个看似简单却极其困难的挑战依然存在：让机器人能够清理派对后的混乱房间、准备烛光晚餐，并让人无法分辨是人类还是机器人所为。演讲者将此称为“物理图灵测试”，并认为这是AI的下一个重大挑战。
2. [00:01:52](https://youtu.be/7fDiui8cAVQ?t=112) 当前机器人技术的笨拙现状与核心困境：

    - 视频展示了最先进的人形机器人在尝试简单任务（如抓取、烹饪）时的频繁失败，显得笨拙且不可靠。
    - 其根本原因在于数据稀缺。与大型语言模型（LLM）可以利用海量互联网文本数据（被称为“化石燃料”）不同，机器人技术需要的是高维、连续的关节控制数据，这些数据无法从互联网上直接获取，必须通过物理方式收集，过程缓慢且昂贵。
3. [00:03:35](https://youtu.be/7fDiui8cAVQ?t=215) 机器人数据的“金字塔”与“核燃料”愿景：

    - 数据策略是解决机器人问题的两大支柱之一。
    - 当前主要的数据收集方式是“遥操作”：操作员佩戴VR设备，实时控制机器人完成任务。这种方式数据质量高（“人力燃料”），但极其耗时，每天每台机器人最多只能收集约4小时数据，无法规模化。
    - 演讲者提出了一个数据金字塔：顶端是稀缺的“真实数据”，底部是LLM使用的“化石燃料”数据。而最令人兴奋的是中间的“合成数据”，它被比喻为“核燃料”，具有近乎无限的潜力，是推动机器人革命的关键。
4. [00:05:28](https://youtu.be/7fDiui8cAVQ?t=328) 仿真1.0：构建“数字孪生”与仿真原理：

    - 这是生成“核燃料”的第一种方法。通过在GPU上进行大规模并行物理仿真（如Isaac Lab），可以极大地加速训练过程（比实时快10,000倍）。
    - 关键技术是“领域随机化”：在成千上万个仿真环境中随机改变重力、摩擦力等物理参数。遵循“仿真原则”，一个能在百万种不同物理参数的仿真世界中掌握技能的AI模型，有很大概率能直接（零样本）应用于现实世界。
    - 应用案例包括训练机械手转笔、机器狗平衡和行走，甚至处理瑜伽球这种难以精确仿真的软体物体，并成功转移到现实世界。
5. [00:08:45](https://youtu.be/7fDiui8cAVQ?t=525) 迈向仿真2.0：生成式AI与“神经物理引擎”：

    - 仿真1.0需要人工创建精确的“数字孪生”，成本高且速度慢。
    - 仿真2.0利用生成式AI（如文本生成3D模型、扩散模型改变纹理）自动、程序化地生成无限多样的仿真环境和资产。英伟达开源的Robocassa引擎就是此类“混合仿真引擎”的代表。
    - 更进一步，通过“Groot Dreams”技术，利用视频基础模型作为“神经模拟器”。它能够根据语言指令，从相同的初始画面模拟出不同的未来（例如，拿起苹果还是拿起罐头）。这种方法不关心场景复杂度，计算效率恒定，并能自动学习光影、反射和物体力学，是“由互联网视频和数据编程”的“神经物理引擎”。
    - 演讲者甚至展示了在视频世界模型中实时遥操作机器人的前瞻性技术。
6. [00:14:10](https://youtu.be/7fDiui8cAVQ?t=850) 数据最大化与模型最小化的核心哲学：

    - 演讲过半才提及模型，强调了其核心理念：​**数据最大化，模型最小化**。
    - 机器人专家数据匮乏，因此不能挑剔，必须整合所有类型的数据（真实数据、仿真1.x数据、仿真2.x数据），通过一个简单的加权采样配方进行协同训练。事实证明，加入合成数据能显著提升模型性能。
7. [00:15:02](https://youtu.be/7fDiui8cAVQ?t=902) Groot VLA模型：从光子到动作的具身AI大脑：

    - 模型架构受“快与慢思考”启发，包含两个系统：

      - 系统二（慢思考）：一个VLM，负责理解语言指令并进行深思熟虑的推理。
      - 系统一（快思考）：一个扩散模型，用于生成连续、平滑的机器人动作，反应快速，运行频率超过100赫兹。
    - 这就是英伟达开发的Groot N1视觉-语言-动作基础模型。它仅含20亿参数，结构紧凑，但能通过不同的输出头适配多种机器人硬件。
    - 演示展示了Groot能够理解“拿最健康的零食”这样的复杂指令，并通过推理选择苹果。另一个演示是机器人能够连续数小时无故障地组装GPU，展现了极高的鲁棒性。
8. [00:17:54](https://youtu.be/7fDiui8cAVQ?t=1074) 未来展望：物理API与技能经济：

    - 随着物理AI的成熟，世界将出现“物理API”——一个用于编程和操纵原子世界的接口。
    - 这将催生一系列新事物：物理提示方法、具身MCP、智能体舰队协调、可编程工厂（快速定制生产）、自动驾驶湿实验室（自动化科学发现）。
    - 最终，将形成一个“物理技能经济”，一个终极应用商店，让任何人都能访问和享受人类所有灵巧性的总和。届时，机器人将融入背景，解决“物理图灵测试”将变得像星期二一样平常。
---
## Prompt Learning: A Reinforcement Learning-Inspired Approach to AI Optimization | Ray Summit 2025
[https://www.youtube.com/watch?v=znTG4LJEwqA](https://www.youtube.com/watch?v=znTG4LJEwqA)

视频介绍：本视频由Arise AI（专注于AI可观测性与评估领域的公司）的Jason Leteki分享，主题为“系统提示学习”。演讲者基于其公司过去六个月的研究与实践，探讨了当前构建AI智能体和助手时面临的核心挑战，特别是如何通过自动化的反馈循环来持续优化和提升系统提示的质量，从而让智能体更可靠、更高效地工作。

结论：构建高效、可靠的AI智能体的关键在于建立自动化、基于文本解释的反馈循环来持续优化系统提示。与传统的基于标量奖励的强化学习相比，利用大语言模型评估生成的、富含信息的文本解释作为反馈，能更精确地诊断问题并生成通用的解决方案指令，从而显著提升智能体性能。这种方法不仅速度更快，还能针对任务的不同部分进行细粒度优化，代表了未来AI系统自我改进的一个重要方向。演讲通过实际案例（如优化开源编码助手Klein）展示了该方法的有效性，并指出构建强大的智能体本质上是一项需要严谨测试、评估和反馈循环的硬核工程。

关键点：
1.  [00:02:32](https://youtu.be/znTG4LJEwqA?t=152s) **当前AI智能体构建的核心挑战与“系统提示学习”的提出**：演讲者指出，许多在实际世界中部署的AI智能体和助手效果不佳，存在大量“无效工作”。他从应用层开发者的视角，列举了几个常见问题：缺乏规划、上下文修剪不当、工具调用缺失或不够模块化。但他本次重点探讨的是最右侧的问题：**如何建立反馈循环来自动改进系统提示**。这是“系统提示学习”的核心思想。他引用Karpathy的观点，指出当前许多基于LLM的应用就像电影《记忆碎片》的主角，每天都要重新学习，系统提示是静态的，无法从昨天的错误中自动学习。开发者需要手动添加指令，就像在胸口写下备忘一样。而未来的AI系统不应依赖于手动编写提示，但为了构建极其健壮的智能体系统，我们又需要像Claude Code那样庞大而复杂的提示。因此，关键问题在于：**如何通过自动化达到这种复杂而高效的提示状态？**
    -   **问题根源**：静态的系统提示无法适应动态、复杂的真实世界场景，导致智能体行为僵化，错误频发。
    -   **核心洞察**：反馈循环是控制系统和强化学习中的强大工具，而**基于文本的反馈循环比基于标量数字的反馈（如传统RL）蕴含的信息量要大得多**，因而可能更强大。
2.  [00:05:56](https://youtu.be/znTG4LJEwqA?t=356s) **传统强化学习（RL）的局限性与文本反馈的优势**：演讲者并非反对RL，但他指出了传统RL在应用于提示优化时的一些根本性问题。
    -   **标量奖励的信息瓶颈**：传统RL依赖于一个标量奖励值（一个数字）来评估整个行动路径的优劣。Karpathy将其比喻为“通过吸管获取反馈”，信息通道极其狭窄。这个单一的数字无法解释具体哪里出了问题，只是笼统地上下调整所有路径的权重。
    -   **文本解释的“信息密度”优势**：相比之下，利用大语言模型进行评估（LM Eval）时，可以生成**解释（explanation）**，说明为什么某个答案是错的或对的。这个解释包含了丰富的、具体的问题描述信息。
    -   **细粒度与模块化更新**：基于文本解释的反馈允许开发者针对任务中特定的、出错的子部分（例如一个多步任务中的某一步）进行提示优化，而不需要像RL那样必须基于完整的轨迹路径进行更新。这使得优化更加灵活和高效。
3.  [00:07:55](https://youtu.be/znTG4LJEwqA?t=475s) **系统提示学习的工作原理：利用评估解释进行反馈循环**：演讲者阐述了其方法论的核心流程。
    -   **评估与解释的核心地位**：他强调，在运行评估时，生成的解释（而不仅仅是“对/错”的判断）是**最有价值的部分**。这些解释是手动调试和自动反馈的基石。
    -   **反馈循环流程**：
        1.  运行智能体，获取其输出（解决方案）。
        2.  使用LM作为评判者，对比智能体的输出与期望输出（或通过测试），生成一个包含问题分析的文本解释。
        3.  将这些解释反馈给一个“元提示”（meta-prompt），该元提示的任务是分析这些失败案例，并**生成通用的、可添加到原始系统提示中的新指令**，以预防未来类似的错误，而不仅仅是修复当前这个特定案例。
    -   **与GEA（Generative AI Prompting）的对比**：演讲者提到他们公开发布此方法略早于GEA论文。GEA采用了相似的“英语反馈循环”核心理念，但增加了更复杂的提示生成树结构。而他们的方法更简单，并且声称在实现上比GEA快数千倍。两者都认同“基于文本的反馈是核心魔力所在”。
4.  [00:11:00](https://youtu.be/znTG4LJEwqA?t=660s) **实践案例：优化开源编码助手Klein**：演讲者分享了他们将系统提示学习应用于实际场景的例子。
    -   **实验设置**：他们选择优化一个名为Klein的开源代码IDE助手，并在SWEBench等基准上进行测试。同时，他们也用自己公司的开源项目（Phoenix）的真实Pull Requests来测试，以验证其在真实代码库上的学习能力。
    -   **从“特定”到“通用”的优化**：通过分析发现，Klein在解决问题时，常常会生成针对当前问题非常特定的解决方案（例如，针对一个具体的错误值抛出特定的异常）。而通过反馈循环学习后，系统提示中会被添加更通用、更符合编程规范的指令（例如，使用Python标准的“NotImplemented”方式来处理一类问题）。这使得模型（如Claude 3.5 Sonnet级别的性能）的能力通过优化提示得到了显著提升。
    -   **工作流程**：智能体运行 -> 生成解决方案 -> 通过LM评判或实际测试进行评估 -> 生成解释 -> 元提示分析并生成通用指令 -> 更新系统提示。这个过程可以循环进行，不断积累和改进提示。
5.  [00:17:58](https://youtu.be/znTG4LJEwqA?t=1078s) **集成到观测与评估平台：构建自改进的智能体系统**：演讲者展望了如何将系统提示学习融入完整的AI开发运维流程。
    -   **在线评估的重要性**：对于生产环境中的智能体，进行在线评估（实时监控和评估生产数据）至关重要。真实世界的状态空间远大于离线测试集，能发现许多未曾预料的问题。
    -   **自动化反馈闭环**：一个理想的平台应能自动捕获每次智能体运行的数据，运行评估并生成解释，然后利用这些解释自动建议或应用对系统提示的改进。这形成了一个强大的自我改进闭环。
    -   **人机协同**：目前，他们实践中仍有人工审核环节，由人类决定是否采纳AI生成的指令建议。但随着系统越来越可靠，未来人工干预会减少。构建能工作的智能体，本质上是**硬核的工程问题**，需要结合测试、系统构建和持续的反馈优化。
6.  [00:25:25](https://youtu.be/znTG4LJEwqA?t=1525s) **问答环节精要：挑战、对比与工具**：在问答环节，演讲者进一步澄清和深化了主题。
    -   **方法的鲁棒性与失败案例**：当被问及该方法是否会出现类似RL的灾难性遗忘或模型崩溃时，演讲者承认早期阶段会出现问题，例如AI可能会生成过于具体而非通用的修复指令。他们通过元提示进行引导来缓解。他强调，尽管不完美，但如果没有这种反馈循环，根本无法构建出高质量的智能体。当前许多“垃圾”智能体正是因为缺乏这种迭代优化机制。
    -   **与GEA的具体区别**：再次被问及时，他总结主要区别在于：1) 他们的方法没有GEA中的树生成结构；2) 主要针对系统中的一个提示进行优化；3) 他们的评估系统具有高度并行化能力，使得优化循环非常快速。他认为核心价值可能就在反馈循环本身，复杂的树生成未必必要。
    -   **工具与架构决策**：有观众问及如何检测是否需要新增工具或分解复杂任务。演讲者认为，目前这类架构层面的决策很大程度上仍依赖人类经验。虽然可以考虑让AI来建议工具，但这仍是前沿探索方向。
    -   **开源实现**：最后，演讲者提到他们已在开源项目Phoenix中提供了该方法的实验性实现，鼓励开发者尝试。
---
## Distributed Embeddings at Scale: Processing 10M+ Rows/ Day with Ray, GPUs & Qdrant | Ray Summit 2025
[https://www.youtube.com/watch?v=pjJBkaKcfuU](https://www.youtube.com/watch?v=pjJBkaKcfuU)

视频介绍: 本视频由Zephyr AI公司的工程师分享，详细介绍了他们如何利用Ray框架和Quadrant向量数据库，大规模生成和存储文本与图像嵌入（Embeddings），以支持其社交媒体内容评估和定向业务。演讲涵盖了从嵌入基础、技术选型、系统架构优化到成本计算和开发者体验的全过程。

结论: Zephyr AI通过构建基于Ray和Quadrant的嵌入生成与检索流水线，成功应对了处理海量、多语言、多模态社交媒体数据的挑战。Ray的分布式任务执行和内存对象存储极大地简化了大规模嵌入生成的扩展，实现了GPU的高利用率。通过将计算密集型（GPU嵌入生成）与I/O密集型（数据上传）任务解耦，并利用Quadrant Cloud的托管服务，团队在保证高性能检索的同时，优化了资源利用率和运维成本。未来的探索方向包括采用TPU以应对GPU短缺、评估多云部署方案以及持续优化数据处理流水线，展示了Ray在工业级AI数据处理系统中的强大能力和灵活性。

关键点:
1. [00:00:43](https://youtu.be/pjJBkaKcfuU?t=43s) **嵌入（Embeddings）的核心概念与应用场景**：演讲者首先阐述了嵌入的基本定义及其在Zephyr AI业务中的关键作用。
    - **嵌入的本质**：嵌入是将文本、图像等非结构化数据转换为数值向量（高维空间中的点）的技术。通过计算这些向量之间的余弦相似度（在归一化后），可以实现高效的内容相似性搜索。
    - **业务需求驱动**：Zephyr AI与多个社交网络平台合作，进行内容评估和定向。这需要处理极其庞大且多样的数据集，例如寻找“恐怖主义”、“毒品”等罕见类别的内容，犹如“大海捞针”。此外，平台支持全球多种语言，需要捕捉文化和语境上的细微差别，以丰富训练数据。
    - **多模态数据价值**：虽然当前以文本处理为主，但庞大的图像数据集不仅能用于训练图像分类模型，还能通过与文本数据的交叉引用，辅助发现相关的文本内容，体现了多模态数据的协同价值。
2. [00:02:52](https://youtu.be/pjJBkaKcfuU?t=172s) **Ray如何简化大规模嵌入生成的扩展**：演讲者重点介绍了Ray框架在分布式计算方面的优势。
    - **自动化分布式执行**：Ray的核心价值在于自动处理分布式任务执行。当嵌入生成任务需要从单机扩展到数十甚至数百个节点时，手动管理变得异常复杂，而Ray抽象了这些复杂性。
    - **内存对象存储的关键作用**：Ray的内存对象存储（In-Memory Object Store）对性能至关重要。它允许数据在集群节点间高效流转和共享，确保GPU能够持续获得数据，从而保持高利用率（目标在95%-100%），避免了因数据I/O瓶颈导致的GPU闲置。
    - **灵活的底层计算支持**：通过Actor模型，可以轻松封装和管理计算资源（如GPU）。当前团队主要使用L4 GPU，但由于区域GPU资源紧张，他们正在探索使用TPU作为补充计算资源。
3. [00:04:00](https://youtu.be/pjJBkaKcfuU?t=240s) **系统监控、优化与架构演进**：通过仪表盘数据和架构调整，团队持续优化系统性能。
    - **性能监控**：展示的仪表盘清晰显示了GPU利用率达到或接近100%，而CPU利用率较低，表明计算瓶颈在GPU，流水线设计良好。
    - **架构优化：计算与I/O解耦**：在初始设计中，嵌入生成和向Quadrant上传数据是耦合的。他们发现，当切换处理不同平台的数据块时，GPU利用率会出现下降。通过将数据上传任务分离到独立的CPU集群上执行，他们实现了GPU资源的“排空”（spend down），让GPU专注于计算，CPU负责I/O。这一改动显著提升了整体资源利用率并降低了成本。
4. [00:05:05](https://youtu.be/pjJBkaKcfuU?t=305s) **选择Quadrant Cloud作为向量检索解决方案**：演讲者分享了从自托管到采用托管服务的决策过程和技术细节。
    - **从自托管到云托管**：团队最初尝试在Kubernetes上自托管Quadrant，单节点部署很简单。然而，维护一个高可用的分布式向量数据库需要专业的运维投入，这对于规模有限的SRE团队来说负担过重。因此，他们选择了Quadrant Cloud托管服务。
    - **托管服务的优势**：Quadrant Cloud提供了极佳的易用性，包括一键升级、轻松扩缩容集群。他们已成功存储了数亿个向量点而没有任何问题。
    - **集成与最佳实践**：Ray提供了丰富的数据源和接收器（sink），团队甚至为Quadrant实现了一个，使得只需几行代码就能将数据写入Quadrant、Snowflake或GCS。演讲者建议仔细规划需要索引的字段并进行调优，因为Quadrant可以在几分钟内完成重建索引，灵活性很高。
5. [00:06:46](https://youtu.be/pjJBkaKcfuU?t=406s) **使用Ray Data的实用技巧与陷阱规避**：基于实践经验，演讲者总结了一系列开发建议。
    - **优先使用内置功能**：不要重复造轮子。Ray Data和底层的PyArrow提供了高度可配置的数据处理功能，如果发现自己写了大量手动处理Pandas数据或直接写入存储的代码，很可能有更优雅的Ray原生方式。
    - **谨慎管理内存与序列化**：
        1. 尽量避免不必要地将数据具体化（Materialize）到对象存储中，以防上游数据量突然激增导致内存溢出。
        2. 确保在Actor间传递的对象是可序列化的，否则任务可能在半途失败。为此，为Actor实现完善的测试覆盖至关重要。
    - **优化数据处理流程**：在开始GPU计算前，对数据进行重分区（Repartition）和具体化，可以确保数据均匀分布，并利用高速网络I/O减少混洗（Shuffle）开销，从而最大化GPU利用率。
6. [00:08:18](https://youtu.be/pjJBkaKcfuU?t=498s) **成本计算、平台选型与未来方向**：演讲者分析了系统成本构成并展望了技术路线图。
    - **成本驱动因素**：系统成本主要由GPU驱动。在将CPU与GPU任务解耦后，两者都得到了充分利用。Quadrant的成本则与存储的向量数据量成正比。
    - **技术选型思考**：虽然Quadrant在延迟方面表现优异，但其计算与存储绑定的模型在非高峰时段可能造成资源浪费。演讲者提到，对于数据量极大但查询不频繁的场景，可以考虑基于对象存储的向量数据库如LanceDB或Turbopuffer。
    - **部署与未来规划**：团队目前使用Ray on Vertex AI（GCP）快速部署了解决方案。为了获得更便宜和更易获取的GPU资源，他们正在考虑向多云架构演进，并评估EKS和AnyScale（已宣布与Azure合作）等平台，AnyScale被认为能极大简化多云环境下的Ray管理。
7. [00:10:00](https://youtu.be/pjJBkaKcfuU?t=600s) **资源分享与社区贡献**：演讲者最后介绍了自己为社区准备的学习资源。
    - **书籍项目**：正在撰写《Ray Data Recipes》一书，旨在提供比官方文档更深入、更复杂的实战案例，例如如何从ClickHouse读写数据。
    - **开源示例项目**：提供了一个名为“ray-quadrant-embeddings”的完整示例项目。该项目引导用户从Hugging Face获取数据，使用Ray进行嵌入生成，将结果写入Quadrant，并最终部署一个Streamlit应用进行检索演示。该项目将使用之前提到的Qwen 3嵌入模型，所有代码均已公开。
---
## Boosting vLLM Inference on Huawei NPU with Ray Compiled Graphs — Huawei | Ray Summit 2025
[https://www.youtube.com/watch?v=gIuvVwt6cX8](https://www.youtube.com/watch?v=gIuvVwt6cX8)

视频介绍：本视频由华为加拿大的Arthur分享，主题为如何利用Ray Compile Graphs（编译图）技术来提升在华为昇腾NPU上进行视觉语言大模型（VLM）推理的性能。演讲深入探讨了大规模模型部署的挑战、Ray Compile Graphs的原理与优势，以及华为团队如何通过贡献代码，将这一技术与昇腾NPU硬件深度集成，最终实现了显著的性能提升和更灵活的部署方案。

结论：华为团队通过引入并优化Ray Compile Graphs，成功解决了大规模VLM模型在分布式NPU集群上推理时面临的高延迟、内存管理复杂和硬件支持有限等核心挑战。该技术通过静态图编译、零拷贝传输等优化，将推理延迟降低了高达40%，吞吐量提升了20-25%，并支持了更高的流水线并行度。更重要的是，团队通过泛化设备上下文和通信组管理，为昇腾等非GPU加速器提供了统一编程模型，增强了Ray生态的硬件包容性。未来，团队计划进一步集成Ray Direct Transport（RDT）和华为HCCL通信库，将优化扩展到更广泛的AI工作负载，为社区提供更多硬件选择。

关键点：
1.  [00:00:19](https://youtu.be/gIuvVwt6cX8?t=19s) **大规模VLM推理的挑战与需求**：演讲开篇即点明当前基础模型参数量已达千亿乃至万亿级别，部署视觉语言大模型（VLM）面临严峻挑战。
    -   **核心挑战**：
        -   **内存与规模**：模型无法单节点部署，即使使用多加速器，也需要跨节点扩展。
        -   **性能与稳定性**：使用Ray进行多节点推理时，观察到高延迟，导致性能不可预测，影响客户体验。复杂的KV缓存内存管理和内存碎片化问题加剧了难度。
        -   **硬件支持局限**：当时Ray对自定义加速器类型（如华为昇腾NPU）的支持有限，缺乏针对性的优化框架。
    -   **明确需求**：基于以上挑战，团队设定了三个关键需求来指导解决方案的开发：
        -   **低延迟执行开销**：执行引擎本身不能引入过高延迟。
        -   **零拷贝张量传输**：必须支持高效的、无需复制数据的张量传输机制。
        -   **统一编程模型**：需要提供一个能够同时支持GPU和华为昇腾NPU的编程接口，这对于希望引入自有硬件的公司至关重要。
2.  [00:02:06](https://youtu.be/gIuvVwt6cX8?t=126s) **Ray Compile Graphs：解决方案的核心**：为满足上述需求，团队引入了Ray Compile Graphs（早期称为Ray DAG）这一新特性。
    -   **技术原理**：它是一种静态有向图编程模型，代表一个任务工作流。其核心思想是预先获取这个计算图，将其编译成一个高效的数据平面，从而允许在引擎执行期间进行多种优化。
    -   **适用性**：对于模型推理这类工作负载（通常包含预填充和解码两个计算阶段）尤其适合。该技术不仅可以将这两个阶段解耦以优化性能，还能将模型层跨多台机器进行流水线并行，从而实现高可扩展性。
    -   **新旧方案对比**：
        -   **旧方案（无Compile Graphs）**：所有通信和数据移动都必须经过对象存储，并通过主机进行序列化，涉及大量gRPC调用，速度慢且可能成为瓶颈，甚至导致节点在TCP连接超时时退出集群。
        -   **新方案（Compile Graphs）**：支持零拷贝传输、可重用缓冲区、共享内存和缓存等优化。执行开销降低了两个数量级。关键启示在于，通过提前预知数据流，Ray Compile Graphs能够优化数据移动路径。
3.  [00:04:06](https://youtu.be/gIuvVwt6cX8?t=246s) **与VLM集成与性能收益**：Ray Compile Graphs的引入与VLM推理引擎的发布形成了强强联合。
    -   **成为默认后端**：当VLM发布其V1推理引擎时，Ray Compile Graphs已成为其默认的分布式后端。这为在NPU上进行多节点推理奠定了坚实基础，使部署更加稳定。
    -   **显著性能提升**：在实际部署和基准测试中，团队观察到延迟最高改善了40%。具体基准测试数据显示，与使用Ray加多进程的后端方案相比，采用Compile Graphs后端在推理工作负载上实现了20%到25%的更高吞吐量。
    -   **部署灵活性增强**：更关键的是，新方案能够支持翻倍的流水线并行因子，而基线方案则无法在此配置下有效服务模型。这为生产环境中的模型部署提供了更大的灵活性和扩展能力。
4.  [00:06:37](https://youtu.be/gIuvVwt6cX8?t=397s) **扩展至昇腾NPU的挑战与解决方案**：将Ray Compile Graphs支持扩展到昇腾NPU及其他加速器时，团队遇到了具体的技术难题。
    -   **主要挑战**：
        -   **代码耦合**：原有代码深度耦合于CUDA和NCCL库，并假设了GPU的执行模型。
        -   **硬件差异**：NPU等硬件拥有不同的内存模型，以及用于跨设备流和事件管理的不同内部机制（如同步屏障）。处理不当会导致Ray Actor永久挂起或产生竞态条件，输出错误的张量。
        -   **通信组生命周期**：PyTorch通信组的生命周期管理存在挑战。某些框架（如VLM）会提前初始化自己的PyTorch分布式运行时并为每个工作进程分配排名（rank）。对于NPU，需要使用PyTorch作为集体通信后端，这可能导致在Ray内部和外部出现重复初始化的问题。
    -   **解决方案**：
        -   **泛化设备抽象**：首先，将加速器设备上下文和流创建API进行泛化，使得任何硬件都可以实现自己的封装。
        -   **引入排名映射**：针对通信组问题，添加了一个排名映射（rank map）来维护上层框架与底层Ray初始化之间的一致性，确保通信上下文的正确对应。
5.  [00:08:42](https://youtu.be/gIuvVwt6cX8?t=522s) **未来展望：从Compile Graphs到RDT**：演讲最后展望了技术演进路线，即从Compile Graphs向更先进的Ray Direct Transport（RDT）发展。
    -   **Compile Graphs的局限**：虽然Compile Graphs速度快，但其静态图编程模型在某些需要更灵活表达逻辑和控制流的工作负载中存在限制。
    -   **RDT的演进**：
        -   **v0.5的GPU对象API**：部分解决了灵活性问题，但带来了需要手动初始化通信组的新问题。
        -   **v1的RDT**：有效解决了上述两个问题，同时保留了Ray Actor的语义，所有复杂操作都在底层自动完成。
    -   **未来计划**：
        -   **多设备支持**：希望在RDT中实现类似Compile Graphs的多设备支持。
        -   **通信层集成**：计划在传输层集成华为的HCCL库，用于张量通信，并与其他加速器库对接。
        -   **扩展应用范围**：目标是将RDT集成到VLM推理引擎中，并进一步扩展到推理之外的工作负载，例如在VRL等框架中进行分布式强化学习，从而为社区在GPU之外提供更多硬件选择。
---
## Fixing S3 Bottlenecks: Scalable I/O for Ray with Alluxio | Ray Summit 2025
[https://www.youtube.com/watch?v=QJEFQlbShf8](https://www.youtube.com/watch?v=QJEFQlbShf8)

视频介绍：本视频由Alluxio公司的Bin分享，重点介绍了Alluxio如何作为数据加速层，帮助消除S3等对象存储的I/O瓶颈，从而高效扩展AI和Ray应用的数据访问性能。演讲深入探讨了AI/ML研究人员面临的数据访问痛点，并展示了Alluxio的解决方案、性能基准及其与Ray等框架的集成。

结论：Alluxio通过构建一个分布式的统一数据缓存与加速层，有效解决了AI/ML工作负载中数据访问慢、不稳定、管理复杂等核心痛点。它为用户提供了透明、即插即用的数据访问体验，无需更改代码或配置，即可将远程对象存储（如S3、GCS）或本地存储（如HDFS）映射为高性能的本地文件系统视图。性能上，Alluxio能提供TB/s级别的聚合吞吐量、亚毫秒级的稳定延迟，并已在大规模生产环境中（管理PB级数据和数十亿文件）得到验证。通过与Ray、Hugging Face等生态的深度集成，Alluxio显著加速了模型训练和推理的数据流水线，是构建高性能、可扩展AI基础设施的关键组件。

关键点：
1. [00:00:15](https://youtu.be/QJEFQlbShf8?t=15) **AI/ML工作者的数据访问痛点与Alluxio的定位**：演讲者首先阐述了AI研究人员和模型开发者面临的核心挑战，他们希望专注于模型本身，而非底层数据基础设施的复杂性。
    - **痛点详述**：这些痛点包括：1) 数据集过大，无法放入本地磁盘；2) 从远程资源（如云存储）下载数据速度慢且不稳定；3) 在不同集群或云环境中部署任务时，需要处理不同的URL、网络问题、权限问题和数据同步回传问题。这些繁琐的数据工程任务严重分散了研究人员的核心精力。
    - **核心需求**：用户渴望一个简单、快速、可扩展的“即插即用”式数据解决方案，使他们能够立即开始使用数据。
    - **Alluxio的定位**：Alluxio正是为此而生，它是一个面向AI工作负载和Ray的数据加速层。它在持久化存储（如S3、Azure Blob、GCS、HDFS）与计算集群（GPU/CPU集群）之间插入一个透明缓存层，让用户访问数据就像访问本地挂载的磁盘或NAS一样简单直观，而实际上数据仍安全地存储在远端。
2. [00:01:59](https://youtu.be/QJEFQlbShf8?t=119) **Alluxio的核心特性：透明、分布式与高性能**：演讲者深入解释了Alluxio区别于传统文件系统客户端的关键特性。
    - **透明与易用性**：管理员可以轻松地将一个或多个存储桶“挂载”到Alluxio命名空间下。对用户而言，数据就像存放在本地磁盘上，可以进行读写操作，整个过程无需代码或配置变更，且兼容大多数现有计算框架。数据一旦推送到存储桶，即可立即可用，无需等待复制到本地。
    - **分布式系统本质**：Alluxio不是单节点的文件系统客户端（如s3fs），而是一个可横向扩展的分布式系统，可以从几个Worker扩展到数千个。这构成了一个统一、共享的缓存和数据加速服务。
    - **聚合性能与可扩展性**：作为分布式系统，Alluxio能提供集群级别的聚合性能。拥有数百个Worker时，可实现TB/s级别的聚合吞吐量，首次字节访问延迟可达亚毫秒级。其可扩展性已在生产环境中得到验证，有大型客户在缓存中管理着PB级数据和数十亿文件。
3. [00:03:37](https://youtu.be/QJEFQlbShf8?t=217) **性能基准测试：吞吐量、IOPS与延迟**：通过具体的基准测试数据，量化展示了Alluxio带来的性能提升。
    - **吞吐量对比**：以6个Alluxio Worker从云对象存储读取数据为例。直接访问对象存储的吞吐量约为7 GB/s。而单个Alluxio Worker即可提供约10 GB/s的吞吐量（已达100Gb网络80%以上的利用率）。当6个Worker协同工作时，聚合吞吐量可提升至接近60 GB/s。
    - **IOPS提升**：在IOPS（每秒输入/输出操作数）方面，6个Worker相比1个Worker能带来约5倍的提升，而相比直接访问云存储，则有近20倍的巨大提升。
    - **延迟优化**：延迟是Alluxio的设计重点。直接访问云存储的平均延迟约为20毫秒，P99延迟可能高达200-300毫秒。而Alluxio能提供稳定、一致的亚毫秒级延迟（包括平均延迟和P99延迟），这对于延迟敏感型工作负载至关重要。
4. [00:06:04](https://youtu.be/QJEFQlbShf8?t=364) **与Ray的集成及带来的收益**：演讲者专门介绍了Alluxio如何与Ray计算框架集成并提升其性能。
    - **集成方式**：Alluxio提供了与Ray的集成接口，用户可以通过Ray的`fs`接口或Ray Data来访问Alluxio，而非直接访问底层存储。
    - **性能收益**：基准测试显示，在相同区域下，通过Alluxio访问数据，能为Ray带来高达3.5倍的吞吐量提升。这种透明、易用的访问方式使得“Ray + Alluxio”的组合能显著加速数据处理流水线。
    - **访问模式**：用户可以通过两种主要方式让Ray应用访问数据：1) 将Alluxio挂载为本地目录（类似于Kubernetes的PVC），Ray应用像读写本地文件一样操作；2) 使用S3协议，只需将端点设置为Alluxio的S3网关，即可继续使用原有的S3 API与Alluxio交互。
5. [00:07:40](https://youtu.be/QJEFQlbShf8?t=460) **创新用例：集成Hugging Face与优化模型加载**：演讲者分享了Alluxio在模型加载和新兴AI用例中的前沿应用。
    - **挂载Hugging Face Hub**：Alluxio可以将Hugging Face的模型仓库直接挂载为文件系统。用户只需一条命令，就能像访问本地文件夹一样直接获取模型，极大简化了模型分发和管理。
    - **优化Safe Tensor格式加载**：Alluxio针对Hugging Face的Safe Tensor格式进行了智能优化。通过分析对该格式文件的访问模式（通常一个模型包含多个相关文件），Alluxio能够预测并预取接下来可能需要的文件，从而显著加速将模型权重加载到GPU的过程。
    - **实际应用效果**：例如，与Fireworks AI合作，Alluxio帮助实现了模型的快速加载，解决了冷启动问题，在单个部署中为模型推理用例实现了TB/s级别的聚合吞吐量。另一个案例是帮助Salesforce为其智能体记忆层构建亚毫秒级延迟的数据访问，即使底层数据存储在S3上。
---
## Benchmarking GPU Scheduling for Massive-Scale Ray Workloads at Minimal Cost - MSFT | Ray Summit 2025
[https://www.youtube.com/watch?v=1cnEPhzVv_c](https://www.youtube.com/watch?v=1cnEPhzVv_c)

视频介绍：本视频由微软Azure Kubernetes Service团队的工程师Anson分享，聚焦于如何构建一个围绕Ray框架的系统，以验证和优化在Kubernetes上大规模GPU调度的可扩展性与性能，同时控制成本。演讲者深入探讨了使用虚拟化工具（如Kubernetes的Kwalk项目）来模拟大规模集群负载，从而在无需巨额真实GPU投入的情况下，进行控制平面的性能测试与调优。

结论：通过引入Kwalk等虚拟化工具与自动化基准测试框架，工程师可以在成本可控的前提下，对基于Ray和Kubernetes的大规模AI训练平台的控制平面进行深入、可重复的性能验证与瓶颈分析。这种方法不仅帮助识别了如动态资源分配（DRA）带来的调度吞吐量下降等具体问题，还推动了Kubernetes社区相关配置参数的优化。演讲强调，在AI基础设施快速迭代的今天，结合自动化测试、持续监控以及明确的性能SLA（服务等级协议），是确保大规模生产作业稳定、高效运行的关键。未来的工作将扩展到故障注入和更大规模下的运行时韧性测试。

关键点：
1.  [00:00:54](https://youtu.be/1cnEPhzVv_c?t=54) **大规模AI训练作业面临的挑战与成本困境**：Anson首先阐述了在微软AI部门，研究人员日常进行中小规模的Ray训练实验，但定期会整合代码与数据进行大规模生产模型训练。这时，系统可靠性与作业启动成功率成为巨大挑战。
    -   **真实世界的痛点**：当大规模作业启动时，工程师常常需要紧急调试各种故障，例如Pod创建失败、长时间处于Pending状态、DNS查询错误或镜像拉取超时等。这些问题导致宝贵的GPU资源在故障排查期间处于闲置状态，造成巨大的资源浪费和项目延迟。演讲者生动地形容这个过程如同观看火箭发射，令人紧张，且墨菲定律时常应验。
    -   **理想的解决方案与现实的矛盾**：理论上，解决方案是使用能模拟真实使用模式的合成工作负载，进行自动化的大规模性能测试。自动化能提供足够的数据点以获得统计意义的结果，让工程师有信心部署变更，并能早期检测性能回归。
    -   **成本与资源的现实壁垒**：然而，申请上万块GPU节点仅用于测试的预算很难获批。即使获批，云提供商也往往无法提供足够的按需或竞价实例容量。若使用预留实例，运行持续的大规模基准测试成本又过于高昂，不具备实践可行性。这凸显了在真实硬件上进行大规模性能验证的奢侈与不切实际。

2.  [00:03:47](https://youtu.be/1cnEPhzVv_c?t=227) **创新的解决方案：使用Kwalk虚拟化GPU进行控制平面压测**：针对上述困境，Anson提出了一个巧妙且成本效益高的解决方案。
    -   **核心思路**：研究人员在真实的GPU上运行实验的同时，工程师可以利用**虚拟GPU**来“锻炼”和测试控制平面。这通过Kubernetes的上游项目 **Kwalk** 实现。
    -   **Kwalk的功能**：Kwalk能够模拟GPU和CPU节点的生命周期，以及Ray Pod的生命周期。它可以模仿真实的使用模式，在任何真实的控制平面（无论是云托管、自托管还是本地CI/CD环境）上产生逼真的负载。
    -   **整体架构**：整个测试系统包含几个部分：1) **集群加载工具**，用于向Kubernetes控制平面提交Ray作业和RayCluster；2) **KubeRay Operator**，负责协调自定义资源并创建Pod；3) **Kwalk控制器**，运行在真实的CPU节点上，模拟虚拟GPU节点和Pod的生命周期；4) **Prometheus和Grafana**，用于收集和可视化来自控制平面的指标，进行性能测量与分析。
    -   **极致的成本效益**：采用云托管的控制平面，每小时成本仅约0.1美元。再加上运行KubeRay Operator、Kwalk控制器和监控栈的三台真实CPU节点，现在可以以**每小时低于10美元的成本**运行大规模基准测试。这彻底改变了性能验证的经济模型，使其变得日常化和可持续。

3.  [00:05:32](https://youtu.be/1cnEPhzVv_c?t=332) **实战分析：动态资源分配（DRA）带来的性能瓶颈与调优**：Anson通过一个具体的测试案例，展示了如何利用上述框架发现并解决深层次的性能问题。
    -   **发现问题**：测试结果显示，启用由动态资源分配（DRA）支持的拓扑感知调度后，系统的最大作业吞吐量（以每秒创建的Pod数衡量）反而降低了。更令人担忧的是，随着集群规模增大，这种吞吐量差距变得更大。
    -   **根因分析**：通过分析监控指标，发现瓶颈在于Kubernetes控制平面中的 **ResourceClaim** 处理积压。每个使用DRA的Pod都需要一个ResourceClaim来指定所需GPU的确切类型和数量。在万级规模下，默认的ResourceClaim控制器工作线程数（最初为10，后社区增至50）仍不足以处理海量的并发请求，形成了处理队列的拥堵，导致Pod调度被延迟。
    -   **提出解决方案**：Anson没有止步于发现问题，而是深入Kubernetes源码，并为此向社区提交了**拉取请求（PR）**，建议使ResourceClaim控制器的工作线程数变为可配置参数，以适配不同规模的工作负载。这体现了通过基准测试驱动上游开源项目改进的实践。
    -   **更广泛的调优参数**：演讲进一步指出，为了支撑大规模编排，KubeRay Operator本身也有多个参数需要调优，例如协调并发度、Kubernetes客户端速率限制等，并且需要增加其CPU和内存限制以避免资源限制。Kubernetes控制平面本身也提供了大量可调节的“旋钮”。

4.  [00:08:01](https://youtu.be/1cnEPhzVv_c?t=481) **方法论总结：自动化、监控与SLA的重要性**：Anson强调了超越具体调优项的系统性工程方法。
    -   **自动化基准测试与持续监控的核心地位**：在技术快速迭代的环境中，性能问题可能随时因人为错误甚至AI辅助编码的“幻觉”而引入。因此，不能依赖一次性的手动测试。必须建立**自动化的基准测试和持续的监控**，以便在配置或代码变更时能及时验证性能表现，实现早期回归检测。
    -   **建立明确的性能SLA**：在研究人员（业务方）和工程师（平台方）之间，就性能与可扩展性达成明确的**服务等级协议（SLA）** 至关重要。例如，在多大规模下，作业启动延迟在什么水平内是可接受的（对研究员）且可实现的（对工程师）。这为性能优化工作设定了清晰的目标和共同期望。

5.  [00:08:54](https://youtu.be/1cnEPhzVv_c?t=534) **未来展望：扩展测试场景与增强系统韧性**：最后，演讲展望了该测试框架未来的应用方向。
    -   **持续暴露新瓶颈**：将系统规模再扩大一个数量级（例如10倍），必然会暴露出新的、未曾预料的瓶颈，需要新的优化手段。性能优化是一个持续的过程。
    -   **模拟运行时故障以增强韧性**：在实际的大规模GPU集群训练中，运行时故障（如GPU烧毁、NVLink或InfiniBand网络波动导致节点被封锁和排空）很常见。Kwalk非常适合用于**模拟这类故障**，并推动系统实现超大规模下的高可用性和容错能力。通过主动注入故障，可以使测试更贴近真实世界的复杂用例，从而验证和提升整个AI训练平台的韧性。
---
## Hugging Face + vLLM: One Model Definition to Rule Them All | Ray Summit 2025
[https://www.youtube.com/watch?v=LnXl5YR7dyA](https://www.youtube.com/watch?v=LnXl5YR7dyA)

视频介绍: 本视频由HuggingFace的VLM维护者Harry分享，主题是介绍VLM的“Transformers后端”功能。该功能旨在让开源大语言模型的创建者和微调者能够仅维护一套基于HuggingFace Transformers的模型代码，即可无缝地在VLM框架中使用，从而获得VLM的高性能推理和高级并行化特性，极大地简化了开发流程。

结论: VLM的Transformers后端为模型开发者提供了一个强大且高效的解决方案，它通过桥接Transformers和VLM，实现了“一次编写，多处运行”的理想状态。开发者无需再为两个框架分别维护模型实现，只需遵循简单的适配步骤，即可让Transformers模型自动获得VLM的KV缓存、张量并行、流水线并行、专家并行等高级功能，且性能损失极低（吞吐量下降<3%）。这显著降低了开发复杂性和维护成本，使得模型能够更便捷地接入各种训练引擎和量化算法，加速了从研究到生产部署的整个生命周期。

关键点:
1. [00:00:22](https://youtu.be/LnXl5YR7dyA?t=22s) **开发者的痛点与Transformers后端的引入**：演讲者首先指出了开源LLM开发者在模型实现上面临的核心挑战。
    - **双重实现的负担**：过去，如果希望模型既能在Transformers生态中使用，又能利用VLM的高性能推理和并行化特性（如专家并行负载均衡、流水线并行、LoRA等），开发者必须学习并维护两套模型代码（一套用于Transformers，一套用于VLM）。这对于新手来说可能相当复杂。
    - **同步维护的难题**：在整个开发生命周期中，从预发布阶段的架构调整，到发布后的bug修复或API更新，开发者都需要确保两套实现保持同步，这带来了巨大的维护开销和潜在的不一致风险。
    - **解决方案**：VLM的Transformers后端应运而生，它允许开发者只维护一个基于Transformers的模型代码库。这个后端将VLM添加到支持“通用模型实现”的框架列表中，使得Transformers模型能够直接在VLM中加载和运行，并自动获得VLM内置的高级特性。

2. [00:02:42](https://youtu.be/LnXl5YR7dyA?t=162s) **Transformers后端支持的核心特性**：演讲者详细列举了该后端所支持的模型特定功能，展现了其广泛的适用性。
    - **注意力机制**：支持编码器注意力、解码器注意力、滑动窗口注意力以及交错滑动窗口注意力，覆盖了当前主流和新兴的注意力变体。
    - **前馈网络**：支持标准的密集MLP以及混合专家模型。
    - **模态支持**：当前完全支持文本模态，而图像、视频和音频模态的支持正在开发中，即将推出。
    - **池化与适配器**：支持嵌入池化和分类池化，同时也支持VLM的池化适配器。
    - **并行化策略**：全面支持数据并行、张量并行、专家并行和流水线并行，并且这些并行方式可以以任何适用于特定架构的组合形式使用。一个关键优势是，如果模型在VLM不直接支持的模块之间（例如，在注意力前向传播或融合前馈网络之间）有新颖的设计（如独特的专家路由方法），只要这些逻辑包含在Transformers实现的`forward`方法中，它们就能在VLM后端中正常工作，因为后端本质上调用的是Transformers模型的前向传播。

3. [00:03:54](https://youtu.be/LnXl5YR7dyA?t=234s) **性能表现与使用情况**：演讲者提供了关于该后端性能的初步数据和实际使用规模，证明了其可用性。
    - **性能基准**：在对一个小型模型进行的有限基准测试中，使用Transformers后端相较于原生VLM实现，吞吐量下降不到3%，延迟几乎没有变化。这表明引入的抽象层带来的开销非常小。
    - **生产规模验证**：通过收集的匿名使用数据（通常在生产环境中禁用）显示，在2023年第三季度，该功能已被用于超过400万GPU小时。这有力地证明了该后端在真实的大规模生产环境中是“足够快”的，并且已经被广泛采纳。

4. [00:04:25](https://youtu.be/LnXl5YR7dyA?t=265s) **如何使用Transformers后端**：演讲者分步骤讲解了开发者和用户如何利用这一功能。
    - **对于使用者**：如果想尝试一个已经支持此功能的现有Transformers模型，只需在加载模型时添加一个特定的标志（`use_transformers_backend=True`），并在日志中确认出现了“using transformers backend”的提示即可。
    - **对于新模型开发者**：流程分为几个关键步骤：
        1. **编写Transformers模型**：像往常一样，遵循HuggingFace文档编写Transformers模型实现，并确保它能被`AutoModel`自动发现。建议将模型代码贡献到Transformers主库（上游），这样Transformers团队可以协助维护和优化。
        2. **启用VLM注意力**：在模型代码中，确保使用了`all_attention_functions`这个全局变量。在VLM后端中，这个变量会被注入VLM的注意力实现，从而使模型获得KV缓存和所有并行化特性的支持。
        3. **传递关键词参数**：必须确保从基础模型的`forward`方法开始，所有关键词参数都能一路向下传递，并最终进入被选中的注意力接口。
        4. **配置并行化**：
            - **张量并行**：在模型配置中通过`tensor_parallel_plan`注解指定哪些层应该被分片，以及分片的方向。
            - **流水线并行**：通过`pipeline_parallel_plan`注解指定哪些层并非存在于所有计算节点上，以及它们的执行顺序。
            - **专家并行**：这是支持融合前馈网络的先决条件。只需确保MLP模块包含一个`experts`属性，并且直接在其上调用`forward`方法，而不是在一个循环中逐个调用独立的线性层（如果专家是由多个独立线性层构成的话）。

5. [00:06:55](https://youtu.be/LnXl5YR7dyA?t=415s) **总结与生态兼容性**：演讲者总结了该功能的优势并强调了其带来的广泛兼容性。
    - **核心优势**：通过Transformers后端使模型与VLM兼容变得极其简单，性能损失可忽略不计，且团队已识别出可以进一步缩小性能差距的优化点。
    - **广泛的生态兼容性**：这套单一的、兼容VLM的模型实现，同时也能兼容众多流行的训练引擎，如Unsloth、TorchTitan、Axolotl等。它还能兼容大多数量化算法，并且可以方便地使用Transformers、Candle、GML等库在本地进行测试，为开发者提供了极大的灵活性和便利性。

6. [00:07:48](https://youtu.be/LnXl5YR7dyA?t=468s) **问答环节要点**：在最后的问答环节，演讲者补充了关于文档和向后兼容性的重要信息。
    - **获取方式**：该功能目前支持在VLM和Transformers两个项目的主分支上使用。需要注意的是，由于Transformers v5正在开发中，最新的提交可能暂时不兼容，需要关注一个特定的PR或等待修复。演讲者表示他正在积极处理此问题。
    - **向后兼容性**：对于HuggingFace Hub上已有的、使用早期Transformers版本实现的模型，只要按照前述步骤对模型代码进行适配（确保使用`all_attention_functions`并正确传递参数），就可以使它们变得与VLM兼容。这为现有模型的性能提升和现代化提供了清晰的路径。
---
## How the VAST AI Operating System Powers a Dynamic Data Plane for Ray | Ray Summit 2025
[https://www.youtube.com/watch?v=9FwufB6sHDQ](https://www.youtube.com/watch?v=9FwufB6sHDQ)

视频介绍：本视频由Vast Data公司的Glenn分享，探讨了Ray在AI工作流中面临的存储挑战，并介绍了Vast Data如何构建一个智能、响应式的数据平台（AI操作系统）来与Ray的计算平面“阻抗匹配”，从而创建一个更高效、更简化的AI基础设施。

结论：Ray作为强大的分布式计算框架，在处理大规模AI工作流时，其计算平面高度智能和弹性，但传统上其数据平面被视为被动、笨拙的“儿童桌”。Vast Data通过其“解耦共享一切”架构构建的AI操作系统，将数据平面提升到与Ray计算平面同等的智能水平。该平台不仅提供高性能、可扩展的S3/NFS存储，更通过统一的数据核心，原生支持事件驱动、向量搜索、事务数据库等多种语义丰富的接口。这使得数据能够主动“通知”Ray工作流，简化了如RAG等复杂流水线的架构，消除了多个组件拼接带来的复杂性、脆弱性和管理开销。最终，一个智能的数据平面与Ray智能的计算平面相结合，能够构建出更高效、更可扩展、更易于管理的下一代AI基础设施。

关键点：
1.  [00:01:17](https://youtu.be/9FwufB6sHDQ?t=77s) **Ray的计算智能与数据平面的“儿童桌”困境**：演讲者首先肯定了Ray在AI工作流中的核心地位，它能将本地开发轻松扩展到成千上万个GPU上，并智能地响应环境变化。然而，Ray的架构基于一个关键假设：计算是智能且可编排的，而存储和持久化数据则是“那边”一个独立、被动的基础设施。
    -   这个假设导致Ray工作流在与数据交互时，采用了“轮询”或“唠叨”模式。计算任务需要不断询问存储系统：“数据有变化吗？我需要做什么吗？” 演讲者用生动的比喻形容：Ray像是坐在“成人桌”进行高级对话，而数据则被放在“儿童桌”，只能接受“保管好、别弄丢、我要的时候还给我”这类基本指令。
    -   这种模式在GenAI爆发前尚可应付，但随着工作流和数据规模急剧增长，计算与数据之间的不匹配日益凸显。Ray的计算能力持续进化，而数据仍被当作原始、被动的实体对待，这成为了大规模AI工作流的瓶颈。
2.  [00:02:48](https://youtu.be/9FwufB6sHDQ?t=168s) **Vast Data的解决方案：构建与Ray“阻抗匹配”的智能数据平面**：为了突破上述瓶颈，Vast Data借鉴了Ray在计算平面的设计理念，构建了一个在能力上与之对等的智能数据平面。
    -   其基础设施也分为两层：顶层的**无状态计算层**（由可线性扩展的VM或容器组成）和底层的**Element Store**（由数千个SSD通过高性能网络连接构成）。这种“解耦共享一切”架构是关键。
    -   在无状态计算层上，Vast可以灵活部署各种数据服务。这些服务不仅提供类似传统存储系统的功能（如S3、NFS），更能像Ray一样具备响应性、弹性和丰富的表达能力。数据不再是静态文件，而是可以通过多种语义接口（如SQL、事件API、向量搜索API）进行交互的活跃实体。
3.  [00:06:33](https://youtu.be/9FwufB6sHDQ?t=393s) **智能数据平面与Ray结合带来的范式转变：数据获得“代理权”**：当这样一个灵活的数据平台与Ray的计算平面结合时，整个系统的交互模式发生了根本性变化。
    -   Ray的编排层不再是唯一的“决策者”。数据本身获得了“代理权”，能够主动发出事件来触发Ray工作流。例如，新数据到达存储系统的事件可以直接启动一个Ray任务进行处理，无需外部轮询机制（如Cron Job或Airflow）。
    -   这实现了计算与数据的双向、主动响应。Ray可以因数据变化而行动，数据也能响应Ray的操作。两者都能独立扩展到极致：Ray至数万GPU，数据至EB级命名空间、TB/秒级性能或每秒万亿次向量操作。
4.  [00:06:58](https://youtu.be/9FwufB6sHDQ?t=418s) **案例对比：传统RAG流水线与基于Vast AIOS的RAG流水线**：演讲者以常见的RAG（检索增强生成）流水线为例，清晰对比了两种架构的优劣。
    -   **传统架构（基于笨拙存储）**：
        1.  数据存入S3后，需要一个外部进程（如Cron Job）不断轮询S3是否有新文件。
        2.  检测到新文件后，该进程再手动触发Ray任务。
        3.  Ray任务进行数据分块、嵌入向量生成。
        4.  生成的向量需要被存入另一个独立的向量数据库（如Chroma）以供检索。
    -   此架构问题在于：组件多（存储、轮询器、Ray、向量数据库）、粘合逻辑复杂、脆弱且运维负担重。数据平面是分裂且被动的。
    -   **基于Vast AIOS的架构**：
        1.  数据通过S3 API存入Vast，这一操作会在Vast内部产生一个事件。
        2.  Vast通过其原生的、与Kafka兼容的事件API对外发布此事件。
        3.  一个订阅了该事件流的服务（可以是Ray服务本身）接收到事件，自动触发Ray处理任务。
        4.  Ray完成任务后，生成的向量可以直接作为原S3对象的**元数据**写回Vast。
        5.  关键点在于：Vast内部以统一的表格格式存储数据和元数据，因此这些向量元数据可以直接通过Vast提供的**向量搜索API**进行查询，无需额外的向量数据库。
    -   此架构的优势是极致的简化：事件驱动取代轮询，统一数据平面取代多组件拼接，权限和安全策略在数据和向量之间自动同步，无需额外数据同步作业。
5.  [00:11:07](https://youtu.be/9FwufB6sHDQ?t=667s) **统一数据核心带来的深层优势与未来展望**：演讲者简要提及了另一个利用其事务数据库能力的复杂用例，并总结了智能数据平面的核心价值。
    -   **统一性的威力**：由于所有数据（对象、向量、事件、表格数据）都存储在单一的核心表示中，不同的API（S3、向量搜索、事件流、SQL）只是这个核心的不同视图。这带来了巨大的运维优势，例如修改一个对象的权限，其关联的所有向量元数据的权限会自动、即时地更新，因为它们在底层本就是一体。
    -   **总结与愿景**：Ray在计算侧表现出色，但一直受限于缺乏智能的存储系统。Vast Data的使命就是提供一个在可扩展性、表达能力和响应性上与Ray对等的数据平面。让数据也能坐在“成人桌”，与Ray平等对话。二者的结合将使得整个AI工作流的运行更高效、更简单、更具可扩展性，从而推动AI基础设施向前迈进。
---
## LiquidAI’s Approach to Large-Scale Synthetic Data Generation Using Ray | Ray Summit 2025
[https://www.youtube.com/watch?v=uTexVOhLYhk](https://www.youtube.com/watch?v=uTexVOhLYhk)

视频介绍：本视频由Liquid AI公司的工程师在Ray峰会上分享，重点介绍了如何结合Ray Data、Ray Serve和VLM（vLLM）构建一个可扩展、易于维护的合成数据生成系统。演讲者以“授人以渔”为理念，着重讲解设计原则和架构模式，而非具体的数据内容，旨在展示如何应对大规模、复杂且需要快速迭代的AI数据生成挑战。

结论：通过将Ray Data、Ray Serve与VLM深度集成，Liquid AI构建了一个高度可扩展、模块化且面向未来的合成数据生成“工厂”。该模式成功地将CPU密集型的数据读取/预处理、GPU密集型的模型推理以及复杂的多模型工作流编排解耦，并利用Ray生态的自动扩缩容和流式执行能力，实现了从PB级数据到复杂生成逻辑的高效处理。核心优势在于代码的简洁性、可维护性以及对未来技术迭代的强适应性，为工业级的大规模AI数据生成提供了一个通用且强大的解决方案。

关键点：
1.  [00:01:45](https://youtu.be/uTexVOhLYhk?t=105s) **Liquid AI的业务背景与数据挑战**：演讲者首先介绍了Liquid AI作为一家专注于定制化基础模型的公司，其核心目标是在边缘设备上实现高性能部署。
    *   **公司定位**：Liquid AI是一家从MIT CSAIL分离出来的基础模型公司，致力于训练端到端的定制架构模型，并已成功在文本、图像和音频领域部署了多个模型，其标志性演示是在树莓派上实现高吞吐量的模型运行。
    *   **数据栈的复杂性**：训练前沿的边缘模型需要管理完整的数据栈，这带来了多方面的挑战：
        *   **规模巨大**：预训练阶段涉及PB级数据和数万亿的token。
        *   **质量要求演变**：从预训练到指令微调，再到对齐后的训练，数据需求从数量转向质量，对特定任务的对齐要求极高。
        *   **计算模式混合**：数据流水线中的任务性质多样，早期是I/O和CPU密集型，后期则变为高度GPU密集型的合成数据生成。
        *   **快速迭代需求**：AI领域发展迅猛，系统必须能快速集成新模型和新生成模式，同时保证代码的未来兼容性。
    *   **核心设计目标**：基于以上挑战，设计数据流水线时必须满足三大要求：**可扩展性**（处理PB级数据）、**复杂性处理能力**（支持多模型任意编排和质量保证）以及**未来兼容性**（快速适应行业变化）。

2.  [00:05:09](https://youtu.be/uTexVOhLYhk?t=309s) **示例工作流：可扩展数据工厂模式**：演讲者通过一个具体的“翻译-评判”循环示例，引出了构建可扩展数据工厂的核心问题。
    *   **工作流描述**：这是一个简化的合成数据生成流程：输入文本 -> 使用语言模型翻译 -> 使用另一个“评判”模型对翻译质量打分 -> 如果合格则收集，不合格则带反馈重试（最多三次）-> 最终失败则丢弃。
    *   **规模化挑战**：即使这样一个简单循环，在规模化时也面临严峻挑战：
        *   **数据规模**：输入语料库可能远超单机内存甚至单机存储容量。
        *   **模型编排**：需要独立管理和协调两个（或多个）模型，它们可能有不同的资源需求，并且需要支持独立更新和替换。
        *   **代码维护**：如何在保证处理如此复杂性的同时，保持代码简洁、可维护且易于调试？
    *   **模式目标**：本演讲的核心就是展示如何利用Ray生态解决这些挑战，将这个小工作流扩展成一个能处理任意数据量、任意复杂模式的“数据工厂”。

3.  [00:06:45](https://youtu.be/uTexVOhLYhk?t=405s) **利用Ray Serve与VLM抽象模型部署**：首先解决模型部署与服务的抽象问题，这是解耦业务逻辑与具体模型实现的关键。
    *   **VLM集成**：Ray Serve提供了与VLM（vLLM）的优秀集成插件，允许开发者用寥寥数行代码定义LLM配置（如“翻译器”和“评判员”模型）。这通过`model_source`抽象了具体模型，并通过封装成OpenAI兼容的API抽象了接口。
    *   **核心优势**：
        *   **部署简化**：开发者无需关心模型在VLM内部的具体实现，只需通过异步客户端调用API。
        *   **性能免费获取**：自动获得VLM提供的连续批处理优化，大幅提升GPU利用率。
        *   **独立扩缩容**：Ray Serve能够根据使用模式独立地为每个模型服务分配资源，实现自动扩缩容和负载均衡。
    *   **业务逻辑解耦**：在定义具体的`translate`和`judge`函数时，只需传入OpenAI客户端句柄。这意味着业务逻辑代码完全与底层模型类型、版本及部署细节解耦，模型的维护和更新仅在配置层进行，实现了高度的未来兼容性。

4.  [00:09:36](https://youtu.be/uTexVOhLYhk?t=576s) **实现可调试的单条数据控制流**：在解决了模型服务化之后，下一步是构建处理单条数据的、易于测试的业务逻辑。
    *   **同步函数设计**：`translate`和`judge`函数被设计为同步函数，这使得它们极易调试——只需传入单条文本即可观察输出。
    *   **并发潜力**：尽管是同步函数，但由于其本身计算量小（主要工作已卸载给Ray Serve），可以轻松并发处理多条数据，不会成为吞吐量瓶颈。
    *   **控制流类封装**：将整个“翻译-评判-重试”循环封装在一个类中，其`__call__`方法被实现为一个**异步生成器**。这是与Ray Data无缝集成的关键。
    *   **逻辑清晰**：生成器接收单行数据（包含文本），在循环中尝试最多N次（如3次）以生成合格翻译。成功则产出结果，耗尽尝试次数仍失败则记录日志但不产出，实现了对低质量数据的过滤。这种设计让复杂循环逻辑在单条数据层面保持清晰。

5.  [00:11:44](https://youtu.be/uTexVOhLYhk?t=704s) **通过Ray Data实现全流程流式并发执行**：最后，将封装好的控制流类嵌入Ray Data管道，实现从数据读取到写入的全程可扩展、流式处理。
    *   **无缝集成**：由于控制流类是异步生成器，它可以被直接用作Ray Data的一个`map`操作步骤。这是该模式最精妙之处。
    *   **资源卸载与并发**：
        *   **CPU/IO卸载给Ray Data**：数据从云存储（如Parquet文件）的读取、任何复杂的CPU预处理步骤，以及最终的结果写入，都被卸载到Ray Data。Ray Data以流式方式并发执行这些步骤。
        *   **GPU卸载给Ray Serve**：包含模型调用的`翻译-评判`步骤，其GPU计算被卸载到Ray Serve集群。
    *   **最大化资源利用率**：因为Ray Data的各个步骤（读、预处理、模型调用、后处理、写）在不同的进程中并行执行，系统能够同时压榨出CPU、GPU和I/O的吞吐量，形成高效的流水线。
    *   **模式通用性**：演讲者强调，这个“翻译”工作流只是一个示例。该模式适用于**任何任意的、多步骤、多模型的复杂工作流**。开发者可以在一个Ray Data步骤中封装任何循环或条件逻辑，从而构建出能扩展到任意复杂度的数据生成模式。
---
## Improved Scheduling Flexibility with Label Selectors in Ray | Ray Summit 2025
[https://www.youtube.com/watch?v=rNTnKrkfgvU](https://www.youtube.com/watch?v=rNTnKrkfgvU)

视频介绍: 本视频由Anyscale的工程师Mongjin Yan和Google GKE团队的工程师Ryan共同介绍，详细讲解了Ray核心团队新引入的“基于标签的调度”功能。该功能旨在解决传统资源调度机制在表达能力和灵活性上的不足，通过引入类似Kubernetes的标签选择器语法，使开发者能够更直观、更精细地控制其Ray工作负载在异构计算环境中的调度策略。

结论: 基于标签的调度是Ray调度系统的一项重大改进，它通过引入标签和标签选择器，解决了传统自定义资源机制在表达非数值属性、指定备选方案和实现复杂调度语义时的局限性。该功能使得开发者能够以更符合直觉的方式，根据节点属性（如CPU架构、GPU型号、实例市场类型等）来调度任务和Actor，并支持与自动扩缩容的深度集成。演示展示了其在多步骤AI流水线（如视频解码与推理）等复杂场景下的强大能力，能够实现成本优化和性能保障。未来，该功能将进一步集成到Ray Serve、Ray Train等高级库中，并支持更丰富的回退策略和拓扑感知调度，为大规模分布式计算提供更强大的基础设施。

关键点:
1. [00:01:11](https://youtu.be/rNTnKrkfgvU?t=71s) **传统调度机制的挑战与局限性**：演讲者以一个典型的两阶段AI应用流水线为例，揭示了传统Ray调度机制在处理复杂、异构需求时的不足。
    - **应用场景**：假设构建一个AI视频处理应用，包含两个步骤：第一步是视频切片解码（CPU密集型，且最好在AMD CPU和Spot抢占式实例上运行以节省成本），第二步是对象检测或分类（GPU密集型，且需要特定的NVIDIA L4 GPU）。
    - **资源环境**：集群中可能包含多种节点：AMD CPU节点、配备NVIDIA T4 GPU的节点、配备NVIDIA L4 GPU的节点，且这些节点可能是Spot或按需实例。
    - **传统方案的缺陷**：在引入标签调度之前，开发者需要使用“自定义资源”来模拟这些非数值属性（如CPU类型、市场类型）。这要求为每个节点分配一个总的数值资源量，并为任务指定所需的部分数值。这种方式存在几个核心问题：
        1. **表达力有限**：自定义资源仅支持“精确匹配”语义，无法表达“非”（not）、“或”（in）等逻辑，也无法指定优先级明确的备选方案。
        2. **不直观且易混淆**：将属性（如“CPU厂商=AMD”）硬编码为数值资源（如`{"amd_cpu": 1.0}`）非常不直观，增加了代码的复杂性和维护难度。
        3. **无法优雅处理回退**：当最理想的节点资源不足时，难以定义清晰、有序的回退调度策略（例如，先尝试AMD Spot实例，不行则尝试任何非Intel CPU的节点，最后再尝试任何可用节点）。
2. [00:04:40](https://youtu.be/rNTnKrkfgvU?t=280s) **基于标签的调度解决方案**：Ryan详细介绍了新引入的标签选择器API，它从Kubernetes的标签选择器中获得灵感，提供了强大且直观的调度表达能力。
    - **API概览**：标签选择器在Ray核心中通过Placement Groups、Tasks和Actors得到支持。开发者可以在启动节点时通过`ray start`命令的`--labels`参数或`--labels-file`参数（从YAML文件读取）为节点设置标签。
    - **默认标签**：Ray会自动检测节点的底层计算资源并设置一些默认标签，例如`accelerator_type: A100`、TPU拓扑信息等，这简化了常见硬件约束的指定。
    - **支持的约束**：标签选择器目前支持`equals`（等于）、`not equals`（不等于）和`in`（属于某个集合）三种约束操作符，语法与Kubernetes类似。
    - **与自动扩缩容集成**：这是关键特性。当任务请求的标签在当前集群中没有匹配的节点时，Ray自动扩缩容器（特别是在Kubernetes上通过KubeRay部署时）会考虑这些标签需求，并尝试启动一个拥有匹配标签的节点类型。这实现了“按需精确供给”资源的能力。
    - **回退策略**：标签选择器支持定义有序的回退选项列表。例如，解码任务可以首先尝试调度到`cpu_family != intel, market_type = spot`的节点；如果不行，则回退到`cpu_family != intel`的节点；最后，可以回退到一个空的标签选择器，即调度到任何节点。这提供了极大的调度灵活性。
3. [00:07:46](https://youtu.be/rNTnKrkfgvU?t=466s) **在Anyscale平台上的现场演示**：Mongjin通过一个在Anyscale平台上运行的演示，直观地展示了标签调度的实际效果。
    - **演示环境设置**：工作空间配置了四个工作节点组：两个AMD CPU组（一个Spot，一个按需），两个Intel CPU + NVIDIA GPU组（一个T4，一个L4）。初始已启动三个节点：AMD Spot、AMD按需、NVIDIA T4。
    - **第一步：调度解码任务**：运行一个要求`cpu_family != intel`且`market_type = spot`的解码任务。任务成功调度到了已存在的AMD Spot节点上，验证了首选调度逻辑。
    - **第二步：调度推理任务**：运行一个要求`accelerator_type = L4`的推理任务。由于当前集群没有L4节点，触发了自动扩缩容，系统启动了一个新的L4 GPU节点，并将任务成功调度其上。这展示了标签调度与自动扩缩容的无缝协作。
    - **第三步：调度多个解码任务（资源竞争）**：同时运行五个解码任务。两个任务被调度到理想的AMD Spot节点。当Spot节点资源用尽后，剩余的任务根据定义的回退策略，被调度到了AMD按需节点和NVIDIA T4节点（尽管后者是Intel CPU，但符合`cpu_family != intel`失败后的“任何节点”最终回退条件）。这完美演示了复杂回退策略在实际资源竞争场景下的作用。
    - **监控**：演示中还展示了如何在Ray集群仪表板中查看节点的标签信息，方便调试和验证。
4. [00:14:09](https://youtu.be/rNTnKrkfgvU?t=849s) **关键要点与未来工作**：Ryan总结了标签选择器的价值并展望了未来的发展方向。
    - **核心价值**：标签选择器通过更灵活、更具表达力的调度方式，解锁了更复杂的用例（如演示中的异构AI流水线）。它使得平台管理员（通过K8s）和应用开发者（通过Ray）能够使用统一的语义来管理工作负载的放置策略。
    - **现有应用**：该功能已经在支持多主机、多切片TPU的场景中发挥作用。通过默认设置的标签（如TPU拓扑、切片名称），可以确保一组任务原子性地运行在特定形状的TPU上，这对于SPMD（单程序多数据）范式至关重要。
    - **未来方向**：
        1. **增强回退策略**：计划增加更多回退策略选项，例如基于资源（而不仅仅是标签）的回退。
        2. **集成高级库**：正在将标签选择器功能集成到Ray Serve（服务部署）、Ray Train（分布式训练）等库中，为用户构建端到端应用提供更多调度控制选项。
        3. **拓扑感知调度**：计划利用标签选择器实现完整的拓扑感知调度。例如，对于一组需要紧密协作的任务或Actor，可以要求它们必须被调度到具有相同标签值（但具体值未知）的节点上，从而保证局部性，优化通信性能。
---
## How DigitalOcean Builds Next-Gen Inference with Ray, vLLM & More | Ray Summit 2025
[https://www.youtube.com/watch?v=DQGyRR6FHbE](https://www.youtube.com/watch?v=DQGyRR6FHbE)

视频介绍：本视频由Digital Ocean的AI团队分享，详细介绍了他们如何基于Ray和VLLM构建其AI推理产品栈的历程。演讲涵盖了从无服务器推理到专用推理的完整产品线，深入探讨了平台架构、核心优化技术以及未来的发展方向。

结论：Digital Ocean成功构建了一个基于Ray和VLLM的、分层的AI推理平台，旨在为开发者提供从简单易用到高性能、可定制的全方位解决方案。该平台通过无服务器推理降低了入门门槛，并通过前缀感知路由、SLA感知路由等优化技术显著提升了多轮对话等场景的性能。即将推出的专用推理服务则旨在满足规模化客户对成本、性能和控制力的更高需求。未来，平台将向多模态、模型组合、企业级治理和安全等前沿领域扩展，展现了云服务商在AI基础设施层持续创新的决心。

关键点：
1.  [00:02:34](https://youtu.be/DQGyRR6FHbE?t=154) **无服务器推理平台架构**：演讲者Bupati详细介绍了Digital Ocean无服务器推理平台的两层架构设计，该设计旨在为开发者提供统一、简单的接口，同时在后端实现强大的管理和编排能力。
    -   **服务层 (Serving Layer)**：作为平台的智能网关，所有请求首先到达此层。它承担了四项核心功能：
        1.  认证与授权：确保请求的安全性和合法性。
        2.  速率限制与配额执行：防止资源滥用，保障平台稳定性。
        3.  使用跟踪与指标收集：为计费和监控提供数据支持。
        4.  多后端路由：根据策略将请求智能地分发到后端的托管层。
    -   **托管层 (Hosting Layer)**：这是平台的“引擎室”，基于Ray Service构建。它围绕四个核心能力设计：
        1.  动态模型加载与配置：能够灵活地加载和配置不同的AI模型。
        2.  持续协调：确保系统状态与预期配置保持一致。
        3.  可扩展的模型服务：利用Ray的分布式特性，实现模型服务的水平扩展。
        4.  可观测性：提供完善的监控和日志，便于问题排查和性能分析。
    -   **统一体验**：通过结合Kubernetes提供的可靠基础与Ray提供的动态编排能力，该平台为开发者创造了一个完全托管的智能推理环境。开发者只需通过Gradient SDK或简单的REST API调用，即可访问所有模型，而Digital Ocean则在后台处理路由、编排、扩展和监控等所有复杂性，实现了开发者体验的极简化和后端管理的强大化。

2.  [00:05:03](https://youtu.be/DQGyRR6FHbE?t=303) **核心性能优化策略**：随着客户需求增长，团队意识到资源效率至关重要，并实施了一系列关键优化，显著提升了平台性能。
    -   **前缀感知路由**：针对多轮对话和智能体工作负载这一最常见场景进行优化。在分布式系统中，对话的每一轮可能被路由到不同节点，导致GPU需要重新处理整个对话历史，造成巨大的资源浪费。解决方案是双管齐下：
        1.  在节点级别启用KV缓存：保存之前轮次处理生成的KV缓存，避免重复计算。
        2.  在路由层添加内部路由器：实现会话粘性，确保同一对话的后续请求被路由到同一个节点，从而复用之前创建的KV缓存。这项优化为相关 workloads 的 P99 首令牌生成时间带来了高达 70% 的降低。
    -   **SLA感知路由**：为了满足客户多样化的需求，平台后端拥有多种硬件（如NVIDIA GPU、AMD GPU）。路由层被赋予智能，能够根据请求对延迟敏感度或吞吐量/成本效益的不同需求，将其路由到最能满足其SLA的硬件上，从而充分利用异构硬件的不同优势。
    -   **前瞻性优化技术**：团队正在积极引入更先进的优化技术以持续推动性能边界：
        1.  推测解码：使用一个较小的“草案模型”生成一个令牌序列，然后由主模型在单次前向传播中验证这些令牌，旨在显著提升整体吞吐量。
        2.  预填充与解码解耦：利用LLM推理中预填充阶段（计算密集型）和解码阶段（内存密集型）的特性，将这两个步骤服务在不同的硬件上，以优化吞吐量。这些优化旨在将性能提升（速度、延迟、成本）直接传递给客户。

3.  [00:08:50](https://youtu.be/DQGyRR6FHbE?t=530) **从无服务器到专用推理的产品演进**：演讲者Dari阐述了Digital Ocean推理产品线的逻辑，即根据客户不同发展阶段的需求提供相应服务。
    -   **无服务器推理的定位**：对于初创公司，无服务器推理是理想的起点。它无需预留GPU容量，按需付费，并能即时访问所有主流基础模型，极大降低了启动门槛和初始成本。
    -   **专用推理的驱动力**：当客户业务规模扩大，使用量超过某个阈值时，预留专用GPU容量并优化其上运行的模型，可能获得更佳的成本效益。客户因此提出了对“专用推理”服务的需求，即拥有独占的、非多租户共享的GPU资源来运行优化后的LLM。
    -   **专用推理的价值主张**：即将推出的专用推理服务，将填补无服务器推理与直接在虚拟机（Droplet）上托管模型之间的空白。它旨在解决客户自行在虚拟机上托管生产级模型所面临的巨大复杂性：
        1.  **基础设施层复杂性**：客户需要自行采购和管理GPU、GPU互连、网络、文件系统（如NFS以减少冷启动延迟）、自动扩缩容、负载均衡和智能路由等数据中心级基础组件。
        2.  **推理运行时调优复杂性**：客户需要根据业务目标（降低延迟或提高吞吐量），深入理解和调优批处理、注意力算法、内存管理、KV缓存/前缀缓存等一系列推理运行时参数。
    -   **专用推理的核心优势**：Digital Ocean的专用推理服务承诺为客户接管上述所有复杂性。客户只需预留容量并指定要运行的模型，即可专注于自身业务应用开发。该服务将提供单租户隔离、恒定资源与成本、更高程度的堆栈配置控制权（如选择NVIDIA或AMD GPU以获得最佳性价比），以及生产级的高可用性和容错能力。

4.  [00:15:21](https://youtu.be/DQGyRR6FHbE?t=921) **未来展望：多模态与企业级能力**：演讲者Jesh展望了平台未来的发展方向，即超越文本推理，构建支持复杂应用和企业级需求的能力。
    -   **支持多模态与复杂应用**：平台旨在支持前沿的多模态应用，例如视频游戏开发、跨地域培训视频翻译、电影制作、播客多语言化等。为实现这些，平台将提供模型组合性、模型链式调用、基于智能体的工作流编排等高级功能，并处理模型的响应、响应结果的存储等全流程。
    -   **增强企业级特性**：随着客户从开发者扩展到数字原生企业，对隐私和数据保护的需求日益增长。为此，平台计划在模型工作室中推出系列企业级功能，包括：
        1.  虚拟私有云支持与基于角色的访问控制。
        2.  团队级和企业级访问隔离。
        3.  模型微调。
        4.  自带模型支持。
        5.  企业级模型治理。
        6.  用于工作负载安全的企业级防护栏。这些规划展示了Digital Ocean致力于为各类规模的企业提供安全、可靠、功能全面的AI云服务的长期愿景。
---
## Scaling Production LLM Inference Using EKS Auto Mode & Ray Serve | Ray Summit 2025
[https://www.youtube.com/watch?v=_r_uCC9jEyE](https://www.youtube.com/watch?v=_r_uCC9jEyE)

视频介绍：本视频由AWS容器领域首席专家Apurva Kulkarni分享，重点探讨了在Kubernetes上运行大规模AI/ML工作负载（特别是Ray Serve应用）时面临的复杂性和挑战，并详细介绍了AWS EKS Auto Mode如何作为一种全托管解决方案，帮助用户简化基础设施管理，实现零停机升级和高可用性服务。

结论：AWS EKS Auto Mode通过将Kubernetes控制平面和数据平面的管理责任完全转移给AWS，极大地简化了运行Ray等分布式AI框架的复杂性。它提供了从一键部署、自动扩缩容、安全加固到无缝升级的全套托管体验。视频中的实时演示有力证明了，即使在模拟生产流量和进行Kubernetes版本升级的双重压力下，运行在EKS Auto Mode上的Ray Serve集群也能保持服务不中断。这为ML平台团队和开发者提供了一个稳定、高效且无需操心底层基础设施的平台，使其能更专注于模型开发与业务创新。结合其与上游Kubernetes的100%兼容性以及对多样化EC2实例（包括GPU实例）的支持，EKS Auto Mode成为在云上运行生产级AI工作负载的理想选择。

关键点：
1. [00:02:05](https://youtu.be/_r_uCC9jEyE?t=125) **在Kubernetes上运行AI/ML工作负载的核心挑战**：演讲者深入剖析了将大规模AI/ML工作负载（尤其是大语言模型）部署在Kubernetes上所面临的多重挑战，这些挑战是催生EKS Auto Mode等托管解决方案的根本原因。
    - **基础设施管理复杂性**：Kubernetes本身的管理就非常复杂，当与需要数百上千个GPU的大规模分布式训练和推理工作负载结合时，挑战呈指数级增长。这包括集群升级、AMI补丁管理、网络配置以及如何安全地将应用暴露给外部世界。
    - **高昂的计算成本与资源优化**：GPU资源昂贵且稀缺，企业必须想方设法最大化GPU在整个组织内的利用率，避免资源闲置，这对成本控制至关重要。
    - **大规模部署的治理与控制**：许多客户在全球运行成百上千个Kubernetes集群。统一管理这些集群的Kubernetes版本、确保合规性、避免因运行旧版本而进入昂贵的扩展支持模式，是一项艰巨的任务。
    - **技术生态的碎片化**：AI/ML和Kubernetes社区催生了大量优秀的工具（如KubeFlow, Ray, vLLM, LlamaEdge, AirBricks等），但这也带来了“技术蔓延”的问题。团队需要学习、维护和管理众多不同的工具和技术栈，增加了运营负担和认知成本。
2. [00:05:02](https://youtu.be/_r_uCC9jEyE?t=302) **EKS Auto Mode的核心理念与价值主张**：演讲者提出了一个关键问题：能否在享受Kubernetes生态优势的同时，将那些无差异化的繁重工作卸载给云提供商？EKS Auto Mode正是AWS给出的答案。
    - **责任共担模型的上移**：与传统EKS（用户管理数据平面）不同，EKS Auto Mode将责任分界线向上推移。AWS不仅管理Kubernetes控制平面，还全面接管数据平面（节点、网络、存储等）的管理。这类似于Fargate的体验，但是基于EKS并100%兼容上游Kubernetes。
    - **核心能力概述**：
        - **简化入门**：支持一键式部署EKS Auto Mode集群。
        - **广泛的实例支持**：提供对几乎所有EC2实例类型的访问，包括基于Graviton的ARM实例、各种GPU实例（如G6）以及Neuron实例，并支持按需实例、Spot实例和容量块预留等多种采购选项。
        - **全托管体验**：控制平面和数据平面均完全托管。升级过程同样是一键操作，AWS会自动处理控制平面和后续数据平面的滚动升级，用户只需确保其应用能够容忍节点更替。
        - **默认安全**：AWS负责为Auto Mode配置的节点进行全面的管理和打补丁，提升了集群的安全性基线。
3. [00:07:44](https://youtu.be/_r_uCC9jEyE?t=464) **EKS Auto Mode在计算、存储和网络方面的具体实现**：演讲者详细解释了Auto Mode如何通过内置的自动化机制处理基础设施的核心要素。
    - **计算（Compute）**：
        - **自动扩缩容**：底层采用名为“Carpenter”的技术（AWS版的集群自动扩缩器），根据Pod的资源需求自动按需创建或终止EC2实例，优化成本。
        - **容器优化系统**：使用名为“Bottle Rocket”的容器优化操作系统，专为运行容器工作负载而高度优化，并提供更安全的运行环境。
        - **健康监控与自动修复**：针对GPU硬件故障率高的问题，通过一个以DaemonSet形式运行的代理来监控GPU健康状态，并在检测到故障时提供自动重启或更换实例的选项。
    - **存储（Storage）**：EBS（弹性块存储）驱动程序已内置到Auto Mode集群中，作为一个托管的CSI驱动提供。用户可以像在标准Kubernetes中一样，使用持久卷（PV）和持久卷声明（PVC）来使用EBS。
    - **网络（Networking）**：
        - **解决IP耗尽难题**：为大规模工作负载提供托管网络体验，自动管理网络配置，帮助用户避免IP地址耗尽这一常见挑战。
        - **全托管负载均衡**：负载均衡器控制器由Auto Mode托管。当需要暴露在线推理应用时，用户只需为服务应用指定Ingress类别，后台的负载均衡器控制器便会自动创建和配置负载均衡器。
        - **托管DNS**：CoreDNS功能也由Auto Mode完全托管。
4. [00:09:59](https://youtu.be/_r_uCC9jEyE?t=599) **实战演示：在EKS Auto Mode上运行Ray Serve并进行零停机升级**：这是视频最精彩的部分，通过一个实时Demo直观展示了EKS Auto Mode的强大能力。
    - **演示环境设置**：在EKS Auto Mode集群上运行一个Ray Serve集群，该集群正在服务一个基于Qwen 0.5B参数模型的vLLM应用。配置中指定了L4加速器类型（对应AWS G6实例），并使用了NVIDIA的`run_as_streamer`插件来加速模型加载。模型直接从S3（通过CSI驱动以文件系统形式挂载）加载到GPU内存中。
    - **模拟生产流量与升级挑战**：在集群正常响应查询（如“什么是机器学习？”）时，启动负载测试以模拟真实世界流量。随后，在流量持续冲击的情况下，**故意触发**将Kubernetes集群从1.33版本升级到1.34版本的操作。目标是实现Ray集群的零停机升级。
    - **升级过程机制详解**：
        - **声明式升级**：升级触发后，EKS先升级控制平面，然后通过修改Auto Mode节点池配置指向新的1.34 AMI镜像。
        - **Carpenter Drift机制**：Carpenter内部的协调循环会检测到节点当前状态与声明配置之间的“漂移”，随即开始创建新的、符合1.34版本的节点。
        - **无缝流量迁移**：整个过程利用Kubernetes原生范式实现业务无感知：
            - **Pod Disruption Budgets (PDBs)**：确保应用本身能容忍一定数量的Pod中断。
            - **Node Disruption Budgets**：通过Carpenter指定节点中断预算，控制每次只替换一个节点，而非批量操作，实现滚动更新。
            - **健康检查与优雅终止**：确保Pod在被终止前完成现有请求。
            - **托管负载均衡器**：自动将流量从旧节点引流到新启动的健康节点上。
        - **最终效果**：观众可以清晰地看到，在负载测试曲线持续波动（代表流量持续进入）的同时，集群的Worker Pod被逐个从旧节点排空并调度到新节点上，整个过程中服务请求的成功率保持稳定，直观证明了零停机升级的成功。
5. [00:15:25](https://youtu.be/_r_uCC9jEyE?t=925) **架构总结与最佳实践**：演讲者总结了在EKS Auto Mode上运行Ray的架构要点和关键启示。
    - **让Auto Mode管理Kubernetes集群**：用户应将Kubernetes集群本身的管理完全交给EKS Auto Mode，包括所有核心插件（VPC CNI, CoreDNS, kube-proxy, 负载均衡器控制器，EBS CSI驱动等）的版本匹配和升级。这解放了ML平台团队，使其能专注于Ray工作负载本身。
    - **利用Kubernetes原生机制保障高可用**：通过正确配置Pod Disruption Budgets、健康检查以及利用Node Disruption Budgets进行顺序更新，可以确保Ray应用在底层基础设施变更时保持高可用性。
    - **无缝的流量切换**：借助AWS托管的网络负载均衡器（NLB）及其控制器，流量可以在旧节点和新节点之间实现无缝过渡，用户无需手动干预。
    - **资源与后续支持**：演讲者提供了EKS Auto Mode官方文档、一个包含生产级参考实现的GitHub示例仓库，以及可供团队参加的EKS自定进度或讲师指导工作坊，帮助观众快速上手。最后，他留下了联系方式，鼓励遇到问题的用户随时联系AWS团队寻求帮助。
---
## Running Ray in Production: Google’s Guide to Operators & Observability | Ray Summit 2025
[https://www.youtube.com/watch?v=SUEBH-0oi-A](https://www.youtube.com/watch?v=SUEBH-0oi-A)

视频介绍：本视频由谷歌云（Google Cloud）的两位产品经理Sunny和Raja共同呈现，详细介绍了谷歌云如何通过Google Kubernetes Engine（GKE）与开源分布式AI框架Ray进行深度集成，以解决企业在构建和部署AI/ML（人工智能/机器学习）工作负载时，从平台搭建（Day 0）、性能与扩展（Day 1）到运维与可观测性（Day 2）全生命周期中面临的挑战。演讲重点阐述了双方与Anyscale的技术合作，以及如何通过KubeRay等工具实现更紧密的集成，从而提供增强的可观测性、可扩展性和性能。

结论：谷歌云通过将GKE与Ray深度集成，为企业级AI/ML平台提供了一个强大、弹性且易于管理的解决方案。该方案解决了从平台初始构建的复杂性和规模挑战，到运行时的资源效率、启动速度、多租户管理，再到后期运维中的日志关联和历史追溯等一系列核心痛点。通过KubeRay、托管Ray插件、先进的调度策略（如原地Pod扩容、标签调度）、高性能基础设施（如RDMA、Titanium ML网络、Hyperdisk ML存储）以及统一的可观测性集成，谷歌云显著降低了AI/ML工作负载的运营开销，提升了资源利用率与计算性能，并简化了故障排查流程。这标志着Kubernetes与Ray的结合正从简单的“容器化运行”演进为深度融合的“分布式AI操作系统”，为大规模、高性能的AI应用提供了坚实可靠的基础设施。

关键点：
1. [00:01:19](https://youtu.be/SUEBH-0oi-A?t=79) **Ray on Kubernetes的挑战与全生命周期管理框架**：Raja开篇即指出，尽管许多客户选择在Kubernetes上运行Ray以获得灵活性和更好的资源编排，但在构建平台时，他们需要从三个关键阶段来确保无缝的开发者体验和平台能力。
    1. **Day 0（平台启用）**：核心目标是交付一个高性能、可扩展且稳定的平台。挑战包括：
        - **大规模集群管理**：客户集群规模从数千节点到超过五、六万节点，需要平台具备极强的可扩展性和韧性。
        - **拓扑感知与低延迟调度**：为了获得最佳性能，需要调度器能够感知节点间的物理拓扑（如网络距离），将相关任务调度到邻近节点。
        - **多样化工作负载的资源抽象**：AI工作流涵盖数据预处理、训练、推理等多个阶段，每种工作负载的资源需求特性不同，平台需要能够抽象并高效管理这些差异。
        - **与Kubernetes能力的深度整合**：需要让Ray能够充分利用Kubernetes在资源编排、调度等方面的原生高级能力。
    2. **Day 1（性能与扩展）**：当工作负载增长后，挑战转向运行效率。
        - **资源消耗与利用率**：如何避免资源浪费，提升昂贵的计算资源（尤其是GPU）的利用率。
        - **最小化中断**：在集群扩缩容或节点故障时，如何保证运行中任务的连续性。
        - **成本与性能优化**：在满足性能目标的同时，优化总体拥有成本（TCO）。
        - **节点启动时间**：大规模任务需要快速弹性伸缩，缓慢的节点启动会成为瓶颈。
    3. **Day 2（运维与可观测性）**：平台稳定运行后，运维挑战凸显。
        - **关联性挑战**：当Ray作业执行失败时，问题可能出在Ray层或Kubernetes层。缺乏有效的工具将两层的日志、事件和指标关联起来，使得根因分析（RCA）异常困难。
        - **历史数据丢失**：作业完成后，其执行上下文、详细日志和指标可能丢失，导致无法进行事后分析和长期优化。
2. [00:03:36](https://youtu.be/SUEBH-0oi-A?t=216) **KubeRay：实现深度集成的核心桥梁**：为了解决Day 0的挑战，实现Ray与Kubernetes的“相互感知”，谷歌云重点介绍了KubeRay项目。
    1. **核心作用**：KubeRay充当了连接Ray和Kubernetes世界的“粘合剂”。它将Ray的核心概念（如RayCluster、RayJob、RayService）映射为Kubernetes的自定义资源定义（CRD），使得用户可以使用熟悉的Kubernetes原生方式来声明和管理Ray集群与作业。
    2. **关键功能**：
        - **标签调度支持**：KubeRay使得Ray能够利用Kubernetes的节点标签进行调度，这对于实现**原地Pod扩容（In-Place Pod Resizing）** 和高效的自动扩缩容至关重要。调度器可以根据资源需求、硬件类型（如特定GPU型号）等标签，将任务精准调度到合适的节点。
        - **高可用与容错**：KubeRay支持Ray头节点（Head Node）的高可用部署，并能够自动处理失效的Ray节点，确保集群的韧性。
        - **可观测性钩子**：它提供了将Kubernetes层的监控数据与Ray层关联起来的接口，为后续的统一观测奠定了基础。
    3. **谷歌云托管Ray插件**：为了进一步降低运营负担，谷歌云提供了托管的Ray插件。
        - **自动化集成**：该插件开箱即用地自动化了KubeRay的部署以及与谷歌云监控栈的集成。
        - **统一控制台视图**：它自动关联Ray事件和Kubernetes事件，并将日志、指标统一汇聚，用户可以在谷歌云控制台的一个界面中查看所有相关信息，极大简化了运维。
3. [00:06:42](https://youtu.be/SUEBH-0oi-A?t=402) **基础设施性能优化：从网络、存储到启动速度**：为了满足Day 1对极致性能的要求，谷歌云在GKE底层基础设施上进行了大量投入。
    1. **拓扑感知与低延迟部署**：在GKE上部署Ray集群时，系统会自动将集群资源（节点）以“低延迟对齐”的方式部署，尽可能让节点在物理上靠近（如同一个机架），以最小化网络通信延迟。
    2. **高性能网络**：
        - **RDMA（远程直接内存访问）**：用于节点间通信，可以绕过操作系统内核，大幅提升网络吞吐量，降低延迟，对于分布式训练等密集通信场景至关重要。
        - **Titanium ML网络**：谷歌专门为AI工作负载设计的网络“高速公路”，能够卸载CPU的数据处理负担，释放更多CPU资源给计算任务，同时支持更高效的分布式训练和数据传输。
    3. **高速存储与数据加载**：
        - **Hyperdisk ML**：高性能块存储选项，每个实例可提供高达12.9 GB/s的吞吐量，确保在训练过程中能够持续、高速地向GPU“喂送”数据，避免GPU因等待数据而空闲（“饥饿”）。
        - **辅助启动盘与镜像流**：针对大型容器镜像和数据集加载慢的问题：
            - **辅助启动盘**：允许预先将数据集加载到磁盘上，Ray工作节点在启动时即可将其视为本地数据访问，加速数据准备。
            - **镜像流**：容器无需等待整个镜像下载完毕即可启动，系统会智能地流式传输所需的镜像部分，将节点启动时间从分钟级缩短到秒级。结合GCS Fuse等技术，实现了高达80%的启动时间降低和2倍的节点就绪速度提升。
4. [00:08:31](https://youtu.be/SUEBH-0oi-A?t=511) **多租户与高级调度：Kueue与标签调度策略**：当多个团队或多种工作负载共享大型Ray集群时，公平的资源分配和优先级管理成为关键。
    1. **Kueue的作用**：Kueue是一个运行在Kubernetes上的作业队列管理器，它可以坐在Ray集群的前端，管理多租户场景下的资源。
        - **租户管理与配额**：可以为不同团队或项目设置资源配额，确保资源分配的公平性。
        - **公平共享与借用**：支持基于权重的公平共享算法，并允许在资源空闲时进行“借用”，提高整体利用率。
        - **优先级与抢占**：支持基于优先级的作业调度，高优先级作业可以抢占低优先级作业的资源。
        - **组调度（Gang Scheduling）**：对于需要同时启动大量Pod的分布式训练作业，Kueue可以确保所有必需的资源同时可用，避免“资源碎片”导致的死锁。
        - **计算类别回退**：当首选的计算资源（如H100 GPU）不可用时，Kueue可以自动将作业回退到备选资源（如按需实例或L4 GPU），确保作业总能执行，同时实现成本感知的调度。
    2. **标签调度的用户友好性**：Sunny强调了标签调度对数据科学家和ML工程师的友好性。
        - **开发者无需接触YAML**：他们可以直接在Ray的Python代码中，以声明式的方式指定资源偏好和回退策略（例如，优先使用H100 Spot实例，失败则回退到H100按需实例，再回退到L4）。
        - **平台管理员保持控制**：与此同时，平台管理员仍然可以通过Kubernetes的标签体系，在底层保持对资源分配的细粒度控制和治理。这完美平衡了开发者的易用性和平台的可管理性。
5. [00:09:08](https://youtu.be/SUEBH-0oi-A?t=548) **Day 1性能增强关键技术：原地Pod扩容与Anyscale Runtime**：针对Day 1的具体性能瓶颈，演讲介绍了两个关键技术。
    1. **原地Pod扩容（IPR） vs. 水平扩缩容**：
        - **Ray Autoscaler V1的问题**：传统的水平扩缩容模式在资源需求小幅增加时，可能会触发启动全新的Kubernetes Pod甚至节点，这个过程需要数分钟，无法满足对延迟敏感的突发性工作负载需求。
        - **IPR的优势**：IPR优先采用垂直扩缩容。它允许直接向一个正在运行的Pod（即Ray工作节点）动态添加CPU和内存资源，而无需停止和重启节点。这大大减少了因扩缩容导致的作业中断时间（OMS），并且由于深度集成于Kubernetes，调度逻辑更加透明，便于调试和观察。
    2. **Anyscale Runtime的性能优势**：对于最苛刻的AI工作负载，谷歌云推荐使用Anyscale Runtime（与Ray深度优化的商业运行时）。
        - **性能基准**：图表显示，在GKE上运行批处理推理工作负载时，Anyscale Runtime比在开源Ray（OSS Ray）上运行快44%。这证明了经过深度集成和优化的平台能为高性能AI应用带来显著的性能提升。
6. [00:13:09](https://youtu.be/SUEBH-0oi-A?t=789) **Day 2运维：统一可观测性与Ray历史服务器**：为了解决Day 2的运维痛点，谷歌云提供了增强的可观测性解决方案并探索了历史数据持久化。
    1. **统一关联视图**：谷歌云托管Ray插件的核心价值之一就是解决了关联性挑战。它自动将Ray的日志、事件、指标与对应的Kubernetes层信息关联起来，在云控制台提供统一的视图。这避免了管理员需要在Ray Dashboard和Kubernetes日志之间来回切换、手动拼接线索的麻烦。
    2. **TPU指标集成**：对于使用谷歌TPU的用户，系统现在可以将TPU的详细使用指标也管道化传输到Ray的控制台，提供了更全面的硬件性能视角。
    3. **Ray历史服务器（探索中）**：针对作业完成后上下文丢失的问题，谷歌云正与Anyscale合作探索“Ray历史服务器”的概念。
        - **功能**：该服务器会持久化保存已结束作业的详细信息，包括执行位置、资源配置、日志摘要等元数据。
        - **价值**：这使得团队能够进行长期的问题排查、性能趋势分析和优化工作，即使作业在很久之前已经完成，也能追溯其执行情况，对于审计、成本分析和容量规划都极具价值。
---
## How DataRobot Parallelizes Agentic Pipeline Searches with Ray + syftr | Ray Summit 2025
[https://www.youtube.com/watch?v=3E0PoEbhYY0](https://www.youtube.com/watch?v=3E0PoEbhYY0)

视频介绍: 本视频由DataRobot的研究员Mark Steman分享，介绍了其团队在AutoML 2024会议上发表的科研项目"Sifter"。该项目旨在解决构建智能体（Agentic）AI和RAG（检索增强生成）工作流时，开发者面临的质量、延迟和成本等多目标权衡难题。Sifter通过结合贝叶斯优化（Optuna）和分布式计算框架（Ray），自动化地探索庞大的工作流组合空间，以寻找准确性与成本/延迟之间的最优帕累托前沿。

结论: Sifter项目成功地将自动化机器学习（AutoML）的思想引入到复杂AI工作流的构建中。它通过YAML定义搜索空间，利用Optuna进行高效的贝叶斯优化搜索，并借助Ray Tune实现大规模并行评估，从而系统化地解决了手动试错效率低下、成本不可预测的问题。该工具设计轻量，支持从个人笔记本到小型研究集群的灵活部署。实践表明，不同工作流之间的成本差异可达数个数量级，而Sifter能够有效发现那些用廉价模型处理简单子任务、仅对关键环节使用昂贵模型的"混合策略"，在保证质量的同时大幅降低成本。项目的成功也凸显了在探索过程中集中式日志、错误处理和应对云服务速率限制等工程挑战的重要性。Sifter为AI工作流的自动化优化提供了一个强有力的开源框架。

关键点:
1. [00:00:41](https://youtu.be/3E0PoEbhYY0?t=41) **构建智能体AI工作流的根本挑战与Sifter的诞生动机**：演讲者基于DataRobot与大量客户合作的经验，指出了阻碍智能体AI落地的三大核心痛点：质量、延迟和成本。开发者面临一个如同《爱丽丝梦游仙境》中红皇后般的困境——技术迭代飞速，每三个月就有新模型发布，导致优化工作永无止境，难以真正交付业务价值。
    - **决策空间爆炸**：构建一个工作流涉及海量选择，包括但不限于：嵌入模型、重排序器、是否使用智能体流程、选用何种智能体架构（如子问题分解、ReAct、批判-更新流程等）。这个组合空间极其庞大且仍在不断扩张。
    - **手动探索的低效与局限**：目前开发者只能手动尝试其中极小一部分配置，这个过程缓慢、随意且无法系统性地权衡多目标（如精度 vs. 成本）。Sifter的目标正是为了自动化、系统化地探索这个广阔的组合空间。
2. [00:02:33](https://youtu.be/3E0PoEbhYY0?t=153) **Sifter的核心工作原理：空间映射、搜索与评估**：Sifter将工作流构建抽象为一个自动化搜索优化问题。
    - **空间定义**：用户通过YAML文件灵活定义需要探索的配置空间（例如，不同的模型、参数、流程步骤）。
    - **搜索策略**：系统使用Optuna进行贝叶斯优化，智能地采样配置，旨在找到准确性和成本（或延迟）之间的帕累托最优前沿。它不会盲目尝试所有组合，而是根据历史评估结果动态调整搜索方向。
    - **可视化呈现**：结果以二维图呈现（例如Y轴为使用LLM作为评判员打分的准确度，X轴为对数尺度下的成本），直观展示不同工作流在“质量-成本”权衡中的分布。图中清晰揭示了不同流程成本可能相差数千甚至上万倍，凸显了自动化优化的巨大价值。
3. [00:04:11](https://youtu.be/3E0PoEbhYY0?t=251) **成本权衡的深度分析与“混合策略”的洞察**：视频深入探讨了工作流成本差异巨大的原因及优化机会。
    - **策略对比**：一种策略是使用多个LLM来精细分解问题、选择性检索，以减少最终合成步骤的令牌消耗；另一种策略则是简单地将大量上下文扔给一个强大的LLM去处理。孰优孰劣并非显而易见。
    - **“混合策略”的优势**：Sifter的优化过程能够发现，在某些定义明确、相对简单的子任务上，使用小型、廉价的模型足以胜任；而仅在需要复杂推理或合成的关键环节使用昂贵的大型模型。这种“因任务制宜”的混合分配策略，是实现极致成本效益的关键。
4. [00:05:32](https://youtu.be/3E0PoEbhYY0?t=332) **利用Ray实现高效并行化与轻量级部署**：为了在合理时间内完成成千上万的试验，并行化至关重要，这也是Sifter选择Ray的原因。
    - **并行执行**：通过Ray Tune，可以轻松地将每个由Optuna采样的配置（工作流实例）分发到独立的Ray工作节点上并行执行。
    - **轻量级架构**：Sifter的设计理念是轻量。通过将计算密集型的LLM推理和嵌入生成卸载到专门的VLLM服务器，工作节点可以非常轻量（仅需CPU），这使得用户可以在个人笔记本电脑上启动探索，也可以轻松扩展到小型研究集群，而无需复杂的Kubernetes编排。
    - **生命周期管理**：Ray负责管理这些工作节点的整个生命周期，即使不同任务的运行时间差异很大，Ray也能有效处理。
5. [00:07:07](https://youtu.be/3E0PoEbhYY0?t=427) **工程实践中的挑战：错误处理与可观测性**：在自动化探索智能体工作流时，会遇到许多意想不到的错误，处理这些错误是项目成功的重要环节。
    - **典型错误类型**：包括云服务商的内容审核拦截、工具调用（Tool Calling）解析失败、LLM返回畸形JSON等。这些错误可能由工作流中某个特定步骤或输入触发，原因复杂。
    - **集中式日志与可观测性的价值**：Sifter强调将所有工作节点的日志集中收集和分析。这使开发者能够在事后深入排查错误根源，例如理解为何某个看似无害的问题会触发内容审核策略。
    - **鲁棒性设计**：搜索过程需要具备容错能力。当某个试验失败时，系统不应崩溃，Optuna可以重新采样该区域或跳过，让搜索继续进行。这种设计保证了大规模探索的稳定性。
6. [00:09:33](https://youtu.be/3E0PoEbhYY0?t=573) **负载管理与对Ray Serve的探索与展望**：优化搜索过程会产生独特的负载模式，并引发了对于更高级集群资源利用的思考。
    - **动态负载与热点**：Optuna的搜索模式会导致负载集中——当它发现某个配置（如特定模型组合）表现优异时，会集中采样该区域，从而在对应的VLLM服务上产生请求热点。
    - **退避策略**：Sifter采用激进的指数退避重试策略来应对云服务的速率限制或服务端过载。工作节点在遇到超时或错误时会自动退避，从而以一种去中心化的方式平滑集群负载，避免了节点间复杂的协调通信。
    - **对Ray Serve的尝试与挑战**：团队曾探索使用Ray Serve来动态管理模型服务实例，以期更高效地利用GPU集群资源（例如，根据热点动态调整不同模型的实例数）。然而，由于深度依赖VLLM服务器提供的特定前端配置（如工具调用注入、聊天模板等），而这些配置无法直接传递给Ray Serve底层的模型执行引擎，因此该尝试未能成功。演讲者将此视为未来一个值得探索的、有潜力的优化方向。
---
## Contextual + Ray: Boosting SFT, RL & Inference at Scale | Ray Summit 2025
[https://www.youtube.com/watch?v=-bjWiQfonMs](https://www.youtube.com/watch?v=-bjWiQfonMs)

视频介绍: 本视频由Context AI的技术总监Falu分享，深入探讨了Context AI作为一家企业级AI中间层公司，如何在其平台上加速监督微调（SFT）和强化学习（RL）模型的推理过程。演讲涵盖了公司在数据处理、模型训练优化、RL训练挑战以及构建高可靠、低延迟推理服务栈方面的实践经验与核心技术。

结论: Context AI通过构建一个集成了高质量数据生成、高效模型训练与优化、以及生产级推理服务的完整平台，成功解决了企业构建AI应用时的复杂性和可扩展性问题。其核心优势在于：1）利用创新的合成数据生成方法，在多个专业领域超越了人工标注数据的质量；2）广泛采用LoRA等参数高效微调技术，大幅降低了训练成本和内存占用，并实现了在单台服务器上部署数千个适配器的能力；3）面对早期RL训练中工具使用和多轮对话的挑战，通过自研基础设施、自动化恢复机制以及结合过程与结果的奖励设计，构建了生产就绪的RL训练流程；4）打造了从模型层、引擎层到生产层的全栈自研推理系统，实现了低延迟、高可靠性和资源高效利用。最终，公司通过将训练与推理基础设施统一到Kubernetes平台，进一步提升了资源利用率和开发运维效率，展现了其在企业AI中间件领域的深厚技术实力与工程化能力。

关键点:
1. [00:00:22](https://youtu.be/-bjWiQfonMs?t=22) **Context AI的公司定位与价值主张**：Falu介绍了Context AI在AI应用栈中的核心位置及其为企业客户带来的关键价值。
    1. **中间层定位**：Context AI位于数据源（海量复杂文档）与应用层（聊天代理、自定义应用或工作流）之间，专注于处理所有与上下文相关的繁重工作，包括创建、执行、搜索和数据管理。
    2. **解决企业痛点**：在Context AI出现之前，企业若想自行构建此类应用，往往面临DIY方案复杂、难以扩展、非生产就绪且漏洞百出的困境。Context AI的平台解决方案使企业能够快速、可扩展地构建生产级的“企业AI优先”应用。
    3. **实际用例展示**：演讲列举了多个真实用例，证明了其平台的能力广度：
        1. **领域专业知识助手**：用于问答和聊天机器人。
        2. **设备日志分析**：能将根本原因错误的分析时间从数周缩短到几分钟。
        3. **测试代码生成**：对半导体测试和验证至关重要。
        4. **动态智能体编排**：处理复杂、动态的工作流。
        5. **结构化输出生成**：能够将任何文档转换为结构化输出。
2. [00:02:12](https://youtu.be/-bjWiQfonMs?t=132) **强大的研究实力与开源贡献**：Context AI的核心竞争力之一是其深厚的研究背景以及与工程团队的紧密协作。
    1. **研究成就**：公司的研究团队在多个公开基准测试中取得了领先排名。例如，其Llama模型在某个排行榜上位列第一；其奖励模型RM-Unit在Reward Bench的生成模型类别中排名第一，整体排名第二。
    2. **RM-Unit的技术优势**：与传统的单一标量分类器相比，RM-Unit提供了更丰富的特征和输出。它生成三个部分：解释性标记（说明回答好、坏或中立的原因）、新的分数（标量值）以及偏好检查（比较哪个答案更好）。在训练时，针对这三部分分别采用了交叉熵损失、均方误差损失和Bradley-Terry损失，这使得模型在真实用例中表现优异。
    3. **对开源的热爱与贡献**：公司不仅使用开源技术，也积极回馈社区。例如，其ReRanker V1是首个指令式重排序模型，而V2版本则在准确性和效率之间取得了更好的平衡，并完全开源。
3. [00:04:19](https://youtu.be/-bjWiQfonMs?t=259) **SFT与RL训练的核心挑战**：针对不同类型的模型微调，Context AI面临着不同的技术难题。
    1. **监督微调（SFT）的挑战**：
        1. **数据质量**：需要高质量的数据进行微调。
        2. **训练优化**：尽管训练框架已相对稳定，但仍存在大量优化空间，目标是实现更快、更低成本的训练。
    2. **强化学习（RL）训练的挑战**（更为复杂）：
        1. **复杂的数据流**：涉及策略模型、价值模型、评论模型、奖励模型等多个组件。
        2. **工具使用与多轮对话**：这增加了额外的挑战，尤其是在今年早些时候，相关基础设施（如工具调用）并不稳定，容易在训练中产生奖励黑客行为或崩溃。
        3. **长上下文处理**：当上下文长度达到10万甚至100万token时，会对基础设施提出新的要求。
        4. **研究挑战**：包括如何找到高质量的训练数据批次、如何进行可靠的评估、如何设计能够进行持续学习的智能体环境，以及如何避免奖励黑客和设计合理的奖励与信用分配机制。
4. [00:06:37](https://youtu.be/-bjWiQfonMs?t=397) **高质量训练数据的生成策略：合成数据 vs. 人工标注**：Context AI深入分析了两种数据来源的优劣，并发展出一套高效的合成数据生成方法论。
    1. **人工标注数据的优缺点**：
        1. **优点**：可以借助领域专家的知识，确保对真实用例、覆盖范围和分布有深刻理解；数据可信度高，可作为黄金标准用于评估。
        2. **缺点**：成本高昂（无论是内部专家还是外包）；耗时且流程繁琐；人类可能存在偏见；难以大规模找到特定领域的专家。
    2. **合成数据的优缺点**：
        1. **优点**：易于扩展，只要有计算资源即可；迭代速度快，完全由研究和工程团队控制；可以控制数据生成的“配方”和流水线。
        2. **缺点**：如果设计不当，数据分布可能与真实世界存在巨大差距；可能存在由模型或流水线bug导致的系统性偏差；初期需要研究和工程团队投入大量精力。
    3. **Context AI的合成数据生成方法**：模拟人类理解和标注数据的过程。
        1. **知识处理**：人类需要知识来回答问题，但无法记住所有知识，因此需要基于知识块进行检索和排序。公司的方法模拟了这一过程。
        2. **查询生成**：针对不同的用户角色（如支持工程师 vs. 研究科学家），问题会完全不同。公司会生成符合目标角色分布的查询，甚至可以基于现有查询的分布进行采样，加速合成查询的生成。
        3. **答案生成与过滤**：利用优秀的“教师模型”生成回答，但并非所有回答都是高质量的。因此，公司建立了一套过滤机制来剔除低质量答案，最终形成高质量数据集。
        4. **实际效果**：令人惊讶的是，在半导体、金融和法律等多个领域，使用这种合成数据训练的模型，其评分比使用人工标注数据训练的模型高出约60-70%。
5. [00:10:35](https://youtu.be/-bjWiQfonMs?t=635) **训练优化利器：LoRA（低秩适应）的应用与最佳实践**：Context AI广泛采用LoRA技术来高效地进行模型微调，并分享了其经验。
    1. **LoRA的核心原理与优势**：在微调时，保持原始模型权重冻结，只训练注入到每个Transformer层中的低秩适配器矩阵。由于适配器维度远小于原始权重，这带来了巨大好处：
        1. **大幅节省训练内存**：尤其是在使用Adam优化器时，需要存储原始权重、一阶/二阶动量以及梯度。LoRA可以节省约80%的训练内存。
        2. **高效服务**：在推理端，可以在单个服务器上同时服务成千上万个LoRA适配器。在实践中，一台配备8块H100 GPU的服务器可以服务数千个LoRA。每个LoRA的大小通常只有原始模型的1%左右，因此加载和切换速度极快。
    2. **LoRA应用的最佳实践**：
        1. **应用层选择**：仅将LoRA应用于注意力（Attention）和多层感知机（MLP）层。**不要**将其应用于语言模型头（LM Head）或嵌入层（Embedding Layer），因为这不仅节省成本有限，还会导致准确率大幅下降（特别是在排序任务中）。
        2. **秩（Rank）的选择**：推荐使用64或128作为“甜点”。使用更高的秩会导致适配器权重变大，失去轻量化的优势；使用更低的秩（如32）则可能因模型容量不足而导致任务性能下降。
        3. **Alpha参数**：根据具体用例，通常在2000左右进行调整。
6. [00:12:36](https://youtu.be/-bjWiQfonMs?t=756) **构建生产就绪的强化学习训练流程**：针对早期RL训练的不稳定性，Context AI从基础设施和研究两个层面进行了系统性的工程化建设。
    1. **基础设施层面的改进**：
        1. **增强框架支持**：在年初开源框架尚不支持多轮对话和工具调用时，公司通过自研代码实现了这些功能。
        2. **提升稳定性与效率**：
            1. **容忍单步间隙**：在训练流程中允许单步的失败或延迟，避免因小问题导致整个训练中断。
            2. **工具服务器自动扩缩容**：应对流量峰值和错误率上升。
            3. **上下文并行**：解决长上下文带来的内存和计算挑战。
            4. **全面的追踪、指标监控与自动恢复**：为工具调用和RL训练流程建立完善的监控和自动修复机制。
    2. **研究层面的策略**：
        1. **利用合成数据**：将成熟的合成数据流水线接入RL训练，以获得更大规模、更可靠的训练批次和评估集。
        2. **智能体与环境设计**：学习到的经验是，将智能体设计得更复杂、更动态，反而为其提供了学习空间。同时，让研究工程师专注于研究问题，而由基础设施团队或专门的研究工程师来处理工具错误和基础设施相关的恢复工作。
        3. **奖励与信用分配**：采用折中方案，结合**结果奖励**和**过程奖励**。对于奖励黑客问题，初期在句子级别使用GPU进行计算，后来借鉴了GSP（可能指某种策略梯度方法）的思想，在标记级别进行同步奖励计算，并加入了长度归一化。奖励信号则来源于公司自研的RM-Unit模型。
7. [00:16:14](https://youtu.be/-bjWiQfonMs?t=974) **自研的高性能、高可靠推理服务栈**：Context AI构建了一个分层的推理系统，以满足企业客户对延迟、可靠性和成本的高要求。
    1. **分层架构**：
        1. **模型层**：包含自研训练模型和开源模型，类型涵盖视觉、语言、嵌入、奖励模型等。根据硬件（如H100或B200）进行量化（INT8/INT4）。对于开源引擎无法满足的需求，会进行定制优化（如自定义逻辑处理器或自研内核）。
        2. **引擎层**：主要基于开源方案（如vLLM、SGL），但会根据需要进行定制。
        3. **生产层**：**完全自研**。该层接收模型ID和请求，并将其路由到不同的推理集群。核心功能包括：
            1. **模型管理与路由**：支持多种模型。
            2. **自动扩缩容**：应对流量峰值和低谷。
            3. **LoRA管理**：需要一个中央存储来管理成千上万个LoRA适配器，并智能地决定其加载位置和迁移策略。
            4. **优化策略分离**：对预填充（Prefill）阶段和解码（Decode）阶段采用不同的优化策略。
            5. **离散服务支持**。
            6. **生产就绪监控**：紧密关注延迟、关键业务指标、错误率和token使用情况等，以实现快速问题响应和调试。
8. [00:20:29](https://youtu.be/-bjWiQfonMs?t=1229) **基础设施统一：从Slurm到Kubernetes的迁移**：这一决策极大地提升了公司的资源利用率和运维效率。
    1. **原有问题**：训练使用Slurm，推理使用K8s，两套系统资源无法共享，维护开销高，且由不同团队负责。Slurm只能进行节点级资源调配，有时会造成资源浪费或出现可用性、调试问题。此外，研究人员需要额外学习推理部署知识。
    2. **迁移至Kubernetes的收益**：
        1. **资源共享与业务保障**：训练任务可以使用按需购买的云上GPU资源（如AWS），而推理任务则可以“借用”训练集群的资源。这确保了客户服务的资源优先级（客户体验高于内部研究），降低了业务风险。
        2. **降低维护开销**：实现了训练和推理基础设施的统一维护。
        3. **更精细的资源调配**：支持Pod级别的资源供给，提高了资源利用率。
        4. **更好的可观测性与调试能力**。
        5. **原生支持**：对RL训练和多租户场景支持更好。
        6. **提升开发速度**：简化了从训练到部署的流程。
---
## AWS + vLLM: Building the Future of Open, Fast LLM Serving | Ray Summit 2025
[https://www.youtube.com/watch?v=IwDnMAJwLyo](https://www.youtube.com/watch?v=IwDnMAJwLyo)

视频介绍: 本视频由AWS工程师分享，重点阐述了AWS对开源社区（特别是VLM项目）的贡献，分析了当前AI基础设施的演进趋势（如解耦架构、专家混合模型），并详细介绍了AWS如何提供模块化、灵活的底层堆栈（计算、存储、网络）来支持这些先进工作负载，最终引导开发者如何在AWS上构建和优化基于VLM的应用。

结论: AWS通过深度参与并贡献于VLM等开源项目，构建了一个面向未来AI工作负载的、模块化且灵活的云基础设施堆栈。演讲清晰地指出了AI模型从输入密集型向输出密集型、从单体架构向解耦架构演进的趋势，以及随之而来的内存、通信和专业化计算挑战。AWS通过提供弹性结构适配器（EFA）、支持多种加速器（如Inferentia、Trainium）、深度集成VLM到深度学习容器等一系列技术，为开发者应对这些挑战提供了强大的基础构件。无论是构建AI网关、部署解耦推理服务，还是进行专家混合模型训练，AWS的开放接口和模块化设计都旨在赋能开发者高效构建、优化和运行下一代AI框架与应用。

关键点:
1. [00:00:23](https://youtu.be/IwDnMAJwLyo?t=23s) **AWS对开源AI生态的持续贡献**：演讲者概述了AWS在AI基础设施开源领域的长期投入和关键合作。
    - **历史合作**：早在2021年，AWS就与Meta和PyTorch团队合作，共同推动了TorchServe、Torch Elastic以及S3插件等关键项目的开发，这些构成了PyTorch生态的重要部分。
    - **当前重点**：如今，AWS正积极与已成为PyTorch基金会一部分的VLM项目合作。贡献形式多样，包括为持续集成（CI）提供云积分、推动VLM在多种硬件（GPU和Trainium芯片）上的支持、进行生态系统集成等。
    - **降低使用门槛**：为了让开发者能轻松起步，AWS已将VLM集成到其深度学习容器中。这意味着开发者无需手动处理复杂的库依赖、驱动和运行时环境，可以直接使用一个预优化、开箱即用的环境在AWS上运行VLM。
2. [00:01:34](https://youtu.be/IwDnMAJwLyo?t=94s) **AI工作负载的范式转变与基础设施影响**：演讲深入分析了当前AI应用趋势的变化及其对底层基础设施提出的新要求。
    - **Transformer推理阶段分解**：现代大语言模型的推理过程可分解为三个阶段，每个阶段对资源的需求不同：
        - **分词（Tokenization）**：CPU密集型任务。
        - **预填充（Prefill）**：计算密集型（FLOPs bound），主要在GPU上完成。
        - **解码（Decoding）**：内存带宽密集型（Memory bound），需要高效地读取键值（KV）缓存。
    - **令牌比例从输入侧重转向输出侧重**：AI应用模式正在演变，从早期的RAG摘要、翻译（输入令牌多），到聊天机器人多轮对话（输入输出相对平衡），再发展到今天的推理模型和智能体工作流（输出令牌远多于输入）。这种转变使得解码阶段和内存子系统的重要性急剧上升。
    - **应对内存瓶颈的创新**：为了缓解内存压力，业界出现了多项创新，包括：
        - **框架优化**：如VLM的页注意力（PagedAttention）技术。
        - **注意力机制改进**：如多查询注意力（MQA）和分组查询注意力（GQA），减少KV缓存大小。
        - **低精度计算与量化**：硬件支持更低精度数据类型，进一步压缩模型。
        - **缓存策略**：不同框架支持灵活的KV缓存和卸载策略。
3. [00:03:38](https://youtu.be/IwDnMAJwLyo?t=218s) **解耦（Disaggregated）架构的兴起与优化**：为了更高效地利用资源，推理服务的架构正从单体式向解耦式演进。
    - **单体架构的局限**：在传统单体架构中，预填充和解码在同一台机器上进行。由于解码阶段GPU计算单元经常空闲，等待内存访问，导致整体计算资源利用率低下。
    - **解耦架构的优势**：将预填充和解码阶段分离到不同的硬件集群上。
        - **预填充集群**：可以采用计算能力更强（高FLOPs）的硬件。
        - **解码集群**：可以采用内存带宽更大或成本更优的硬件。
    - **关键技术挑战与解决方案**：
        - **KV缓存传输**：需要高效的数据传输技术（如NVIDIA的Nixl通信集合）在集群间移动KV缓存。
        - **KV缓存感知路由**：当多个工作节点处理过相同的请求上下文时，为了降低延迟、减少首令牌生成时间，系统需要具备智能路由能力，将新请求定向到已拥有相关KV缓存的节点，实现缓存命中。
4. [00:05:10](https://youtu.be/IwDnMAJwLyo?t=310s) **专家混合模型带来的效率与挑战**：MoE架构通过稀疏激活在成本效益上具有巨大潜力，但也引入了新的复杂性。
    - **工作原理与效率优势**：MoE模型（如DeepSeek-MoE）包含大量“专家”子网络，但每个输入仅激活其中一小部分（例如2/8个专家）。这种稀疏性意味着在训练和推理时，理论上可获得2到4倍的成本性能提升。
    - **内存与通信挑战**：
        - **内存需求大**：尽管激活稀疏，但需要加载全部参数，因此对内存容量要求很高。
        - **通信模式复杂**：MoE采用的专家并行（Expert Parallelism）打破了传统数据、张量、流水线并行的通信模式。
            - **通信量大**：可能比稠密模型多出3到6倍的通信量。
            - **难以预测**：专家之间的通信量随输入动态变化，难以预先规划。
            - **非对称性**：通信模式是非对称的，不同于传统并行中对称、固定量的数据交换。
5. [00:06:57](https://youtu.be/IwDnMAJwLyo?t=417s) **AWS底层基础设施如何支持先进AI工作负载**：AWS提供了一系列模块化构建块，以开放的接口支持上述复杂范式。
    - **弹性结构适配器**：EFA是AWS提供的网络接口，专为高性能计算和AI工作负载设计。
        - **绕过操作系统内核**：实现远程直接内存访问（RDMA），支持加速器与加速器之间的直接通信，极低延迟。
        - **基于开放标准**：EFA基于开源的libfabric库，可与Open MPI、NVIDIA NCCL、Nixl等多种通信集合库集成，方便开发者构建高级通信模式。
        - **应用广泛**：不仅用于节点间网络，还可用于GPU直接存储，让GPU直接从如FSx for Lustre等高性能存储中读取数据，绕过CPU，进一步提升I/O效率。
    - **计算加速器的多样性**：VLM等框架在AWS上可以跨多种加速器运行。开发者可以根据任务特性（如预填充需要高算力，解码需要高内存带宽）选择最合适的硬件，如NVIDIA GPU、AWS Inferentia、Trainium、Graviton或AMD加速器，以实现最佳性价比。
    - **开箱即用的体验**：通过将VLM集成到深度学习容器和提供“AI on EKS”开源仓库（包含推理图表、蓝图和指南），AWS极大地简化了在Kubernetes环境或AWS上使用VLM和Ray的入门难度。
6. [00:10:06](https://youtu.be/IwDnMAJwLyo?t=606s) **实践案例与资源指引**：演讲最后分享了成功案例和进一步学习的资源。
    - **成功案例**：亚马逊的购物助手Rufus在Prime Day期间，成功部署并扩展到了超过80,000个Inferentia和Trainium芯片，展示了基于AWS基础设施构建的大规模AI服务的可行性与弹性。详细技术细节可参考相关的AWS博客文章。
    - **资源指引**：开发者可以通过“AI on EKS”开源仓库快速获取在AWS上部署VLM或Ray的实践蓝图和指导，加速应用开发与部署进程。
7. [00:11:14](https://youtu.be/IwDnMAJwLyo?t=674s) **问答环节精要**：演讲后的问答澄清了实际应用中的两个关键问题。
    - **模型分片**：针对是否能用多个小内存GPU运行大模型的问题，回答是肯定的。VLM支持通过张量并行等方式，将模型分片跨多个节点运行，从而解决单节点内存不足的问题。
    - **与全托管服务的关系**：关于AWS Bedrock等全托管服务是否利用这些底层优化的问题，回答指出Bedrock团队同样通过开源渠道为VLM等项目做出贡献，并在服务底层使用VLM等技术。同时，开发者可以构建混合架构，通过路由层将请求灵活分发至Bedrock托管模型和自管理的VLM模型，兼具便利性与灵活性。
---
## High-Throughput Inference for Synthetic Data & Evals at Sutro | Ray Summit 2025
[https://www.youtube.com/watch?v=igsAMZWH1e4](https://www.youtube.com/watch?v=igsAMZWH1e4)

视频介绍：本视频是 Sutro 公司创始人 Seth 在 Ray Summit 上的演讲，主题是构建一个水平、可预测、高吞吐的批量推理服务，以支持合成数据生成、大规模评估等任务。演讲深入探讨了构建此类服务时遇到的技术与运营挑战，分享了实践经验与核心洞见。

结论：构建一个基础的批量推理服务相对容易，但要打造一个快速、高效且真正受工程师喜爱的服务则极具挑战。关键在于不仅要解决分布式计算框架选择、数据与计算管理、吞吐性能剖析等技术难题，更要从运营角度出发，思考如何构建一个赋能用户、提升实验速度、具备完善元数据追踪且用户体验愉悦的系统。批量推理作为AI领域的重要工作负载，其高效实现将极大推动生产力和研究进展。演讲者强调，最终的系统应服务于人，其设计需紧密围绕目标用户（如研究员、企业用户）的核心需求，在保证性能与可靠性的同时，提供卓越的协作与使用体验。

关键点：
1.  [00:01:21](https://youtu.be/igsAMZWH1e4?t=81s) **批量推理服务的“易”与“难”**：演讲开篇即澄清了一个常见误解。客户常问“批量推理不是很简单吗？”，答案是既简单又困难。
    -   **简单之处**：创建一个基础的批量推理服务确实不难。其核心可以理解为以大型语言模型推理为基本计算单元的“映射与规约”操作。
    -   **困难之处**：真正的挑战在于构建一个**快速、高效**，并且能让工程师团队**真心欣赏和乐于使用**的服务。这需要超越基础功能，在性能、成本、可靠性和用户体验等多个维度达到高标准。
    -   **重要性**：Seth认为批量推理是未来几年能带来巨大生产力和研究进展的关键工作负载，其应用场景包括合成数据生成、大规模模型评估（常与后训练基础设施紧密循环）、非结构化数据分析（如信息结构化提取、分类、开放集标注）等。

2.  [00:01:34](https://youtu.be/igsAMZWH1e4?t=94s) **核心技术与运营决策考量**：构建批量推理服务时，面临一系列关键决策，可分为技术和运营两个层面。
    -   **技术决策**：
        1.  **管理与计算**：核心挑战并非云服务商选择（当前GPU资源已相对充足，尤其是适用于批量推理的竞价实例），而在于**选择和治理一个分布式计算框架**。系统需要能管理多云、多种加速器类型、应对竞价市场波动和对象存储，并具备处理网络中断、系统拥塞等常见分布式故障的容错能力。
        2.  **推理框架选择**：虽然不深入讨论具体框架优劣，但选择后需考虑其内存与性能特性、冷启动时间、版本频繁变更（因很多项目仍处于早期）等问题。同时，需要自行管理模型仓库，避免每次都从Hugging Face拉取，并设法减少权重传输和存储带来的开销。
        3.  **性能与成本剖析**：根据服务对象，可能需要构建工具来剖析性能、质量和成本。精确测算实际吞吐量、模型大小、量化效果、平均工作负载形态是一大挑战。**减少GPU本身的开销**是提升计算资源利用率和最终产出（yield）的关键。
    -   **运营决策（更为关键）**：这关乎服务是**赋能还是限制**使用它的工程师和利益相关者。系统是为人而建，设计需围绕目标用户画像：
        1.  **面向研究员**：需重点设计**高实验速度**、有用的**元数据**和**协作功能**。
        2.  **面向企业任务**：需要为技术背景较弱的用户（如数据标注员）创建优秀的**人机交互界面**。
        3.  **面向关键服务**：需在保持系统节省成本的本质（这也是构建批量推理系统的初衷之一）的同时，找到维持严格**服务等级协议**的方法。

3.  [00:05:11](https://youtu.be/igsAMZWH1e4?t=311s) **具体挑战一：吞吐性能剖析**：深入分析模型吞吐性能对于构建可预测的服务至关重要。
    -   **重要性**：这直接关系到能否满足可预测的SLA，并且是正确根据预测的工作负载需求来调优推理框架的基础。需要根据通常的输入/输出令牌形态来验证推理框架是否调优得当。
    -   **非线性特性**：吞吐量高度非线性，取决于模型大小、硬件和并行类型，尤其是**工作负载形态**。虽然模型大小和硬件可能相对固定，但系统很可能会处理各种形态的工作负载。
    -   **工作负载形态的影响**：图表显示，对于不同的输入/输出序列长度组合，吞吐量差异巨大。
        1.  **短输入长输出**：常见于合成数据生成和推理任务，吞吐性能表现不同。
        2.  **长输入短输出**：常见于非结构化数据处理或分类任务，吞吐量特征迥异。
    -   **价值**：准确剖析并预测工作负载的计算需求，使你能**精确控制任务完成时间**。即，通过这项工作，可以计算出需要多少并行化资源才能在给定的SLA内完成作业。这对于按令牌进行成本核算也至关重要。

4.  [00:07:26](https://youtu.be/igsAMZWH1e4?t=446s) **具体挑战二：数据与计算管理**：在分布式系统中管理数据和计算涉及诸多复杂考量。
    -   **容错与检查点**：希望所选框架能相对优雅地处理机器和网络故障。强烈建议引入**外部存储检查点**机制，在任务运行时保存进度，以避免在机器或网络故障时重复工作。这对于可能价值数千美元的大型批量推理任务尤为重要，防止单个工作节点失败导致整个作业失败。
    -   **多云挑战**：并非所有云都完全相同。虽然像 Sky Pilot 这样的框架在使云资源更通用、提高计算资源可用性方面做得很好，但在多云环境下工作仍存在许多特殊问题。
    -   **数据与模型权重访问**：为了最大程度利用资源（如在全球云区域运行竞价实例），需要仔细考虑数据和模型权重的访问。虽然像 R2 这样的零出口费用存储方案很好，但它们有自己的延迟限制。在全球范围内向大量工作节点传输TB级别的模型权重会带来显著的瓶颈。

5.  [00:09:00](https://youtu.be/igsAMZWH1e4?t=540s) **具体挑战三：打造卓越的用户体验**：让批量推理服务像一款优秀产品一样令人愉悦，这本身是一大挑战。
    -   **元数据与实验追踪**：与过去几十年的数据处理工作负载不同，批量推理任务具有**非确定性**。因此，**元数据**和实验追踪能力比以往任何时候都更重要，对利益相关者至关重要，应视为一等公民。存储和元数据应根据用户需求紧密耦合。
    -   **任务互操作性**：批量任务通常只是流水线中的一个环节，需要与预处理和后处理步骤结合。常会遇到**复合型工作负载**，即一个作业的输出直接作为下一个作业的输入。
    -   **以用户为中心的设计**：核心是构建一个人类**乐于使用**的服务。这意味着要让它快速、支持协作，并能帮助用户高效完成任务。演讲者用一张DIY卡车的图片提醒大家：仅仅能完成基本工作，并不代表它会好用，也不代表“自建而非购买”是一个好决定。
    -   **产品化思维**：尽管现场听众多为“自建派”，但Seth指出，Sutro投入巨大精力打造了他们认为目前最好的托管批量推理产品，鼓励大家尝试，即使只是作为内部服务的基准参考。
---
## Efficient, High-Performance AI Inferencing with Intel Xeon 6 | Ray Summit 2025
[https://www.youtube.com/watch?v=3frBy9yE7v4](https://www.youtube.com/watch?v=3frBy9yE7v4)

视频介绍: 本视频由英特尔专家分享，重点介绍了如何利用最新的英特尔® 至强® 6 处理器实现高效、经济的人工智能推理，特别是针对当前快速演进的大语言模型。演讲深入探讨了硬件创新与软件生态的结合，并提出了三种关键方法以优化企业级AI推理部署。

结论: 英特尔® 至强® 6 处理器通过集成AMX指令集和支持高带宽MCR DIMM内存，为AI推理提供了强大的硬件基础。结合量化、专家混合模型和分布式推理这三种软件优化策略，企业能够在现有数据中心基础设施上，以极具成本效益的方式部署和运行从数十亿到数千亿参数的大语言模型。英特尔提供的完整软件栈和优化工具（如Intel Auto Round）进一步降低了使用门槛，使得用户无需专用GPU即可快速构建和扩展AI服务，实现从实验到生产的高效转化。

关键点:
1. [00:00:05](https://youtu.be/3frBy9yE7v4?t=5) **英特尔® 至强® 6 处理器的硬件创新与AI推理定位**：演讲开宗明义，指出至强® 6 处理器是专为AI工作负载设计的下一代产品。
    - **核心硬件优势**：处理器最高可配备128个性能核心，并内置了关键的AI加速引擎——高级矩阵扩展（AMX）。AMX指令直接内置于每个核心中，专门用于加速模型计算中占主导地位的矩阵乘法运算，其速度远超传统的AVX-512指令集。这为CPU执行高密度AI计算提供了原生算力支持。
    - **内存带宽突破**：除了计算单元，新一代处理器还支持多路合并阵列双列直插内存模块（MCR DIMMs），能提供超过30%的内存带宽提升。这对于需要处理超长上下文（高内存占用）的大模型至关重要，确保了数据能够高速馈送至计算核心，避免成为性能瓶颈。
    - **市场定位**：英特尔强调，其方案不仅提供硬件，更构建了完整的软件生态，覆盖了从使用PyTorch进行实验的开发者、需要大规模部署的进阶用户，到只想在现有硬件上尝试最新模型的AI爱好者等各类人群。所有优化均已集成至主流框架和库中，实现了开箱即用的体验。

2. [00:03:25](https://youtu.be/3frBy9yE7v4?t=205) **实现高效AI推理的三大软件优化方法**：演讲者借鉴计算机视觉模型的发展历程（模型尺寸缩小25倍仍能保持优异性能），提出了适用于大语言模型的三条优化路径。
    1. **量化**：
        - **硬件支持**：至强® 6 处理器原生支持BF16、FP16和INT8精度计算，并通过软件仿真支持FP8、INT4等更低精度格式。这使得GGML、GPTQ、AWQ等多种量化格式的模型都能流畅运行。
        - **精度保持挑战与解决方案**：量化通常伴随模型质量（如准确性）的下降。英特尔推出了 **Intel Auto Round** 工具来解决这一问题。该工具通过使用校准数据集，在量化过程中进行精细调整。演示图表显示，经Auto Round转换的4比特模型（紫色柱状图），在多项基准测试中的性能表现非常接近原始的BF16模型（绿色柱状图），显著优于其他量化方法（如GPTQ、AWQ），从而在极大压缩模型、提升推理速度的同时，最大程度保留了模型能力。
    2. **采用专家混合模型**：
        - **效率优势**：当前顶尖的开源模型（如DeepSeek-V2、Qwen 2.5）多采用MoE架构。这类模型总参数量巨大（如千亿级），但每次推理时激活的参数很少（仅数十亿）。这意味着实际所需的计算量远小于存储模型所需的内存容量。
        - **硬件需求匹配**：运行MoE模型的主要挑战在于需要巨大的内存来加载全部参数，而非极高的瞬时算力。这正是至强® 6 处理器搭配大容量、高带宽内存的优势所在。用户无需昂贵的多GPU集群，利用现有的至强服务器即可高效运行这些前沿模型，在延迟和成本之间取得最佳平衡。
    3. **分布式推理**：
        - **架构理念**：英特尔与LMSys的合作优化表明，可以将单个服务器中的多个NUMA节点视为独立的小型加速器。在一个双路系统中，通常会有4到6个NUMA节点。
        - **并行策略**：基于此硬件拓扑，可以实现多种并行策略来提升吞吐量和扩展性：
            - **张量并行**：将单个模型的运算拆分到多个节点上，加速单个请求的处理。
            - **流水线并行**：将模型的不同层部署到不同的NUMA节点上，处理超大规模模型。
            - **数据并行**：在多个节点上部署相同的模型副本，以服务更多的并发用户。这种分布式能力已集成在vLLM、Red Hat OpenShift等平台中，并支持原生Kubernetes，便于在现有数据中心环境中快速部署和扩展。

3. [00:08:27](https://youtu.be/3frBy9yE7v4?t=507) **性能基准与全栈企业解决方案**：演讲通过数据和实例展示了至强® 6 的实际效能和易用性。
    - **性能基准**：展示的基准测试图表表明，英特尔的64核至强® 6 处理器在Llama 3 8B等模型的推理性能上，可以超越竞争对手的128核最新一代产品。同时，对于从千亿参数（如DeepSeek R1）到较小规模（如Llama 3.2 3B）的各种模型，都能帮助用户满足在延迟或成本方面的服务等级协议（SLA）。
    - **企业AI解决方案**：英特尔提供了一套完整的企业级AI软件栈，涵盖了模型服务、特定优化、编排层等，支持多种操作系统和硬件配置。用户可以通过 `software-catalog.intel.com/ai` 免费获取所有组件，无需额外许可费用。
    - **实际演示**：现场演示了在仅配备46个物理核心（无任何加速器）的至强® 6 服务器上，通过Ray Serve部署和运行一个检索增强生成（RAG）聊天机器人。关键点在于，只需在部署配置中将GPU资源需求设置为0，即可让整个服务（包括向量数据库查询和LLM推理）完全运行在CPU上。这证明了利用现有数据中心服务器快速启动AI项目的可行性，极大降低了入门门槛和总体拥有成本（TCO）。
---
## End-to-End GenAI Orchestration with KubeRay | Ray Summit 2025
[https://www.youtube.com/watch?v=QFDGVa4qQ3I](https://www.youtube.com/watch?v=QFDGVa4qQ3I)

视频介绍：本视频由Simply Smart公司的Dwanch分享，聚焦于Ray框架在生成式AI（GenAI）全生命周期管理中的实际应用。演讲者通过具体的生产案例研究，展示了如何利用Ray显著加速模型训练与推理优化，并分享了其团队在构建端到端MLOps平台过程中，如何高效利用Ray解决大规模扩展和成本效益优化等核心挑战。

结论：Ray框架在GenAI生命周期的两个关键环节——训练和推理优化——中展现出巨大价值。通过将传统的PyTorch分布式训练转换为基于Ray的DDP，并结合自定义训练循环，团队成功将图像生成模型的实验时间从估算的600天缩短至4-5天，并额外获得42%的性能提升。在推理环节，团队通过Ray构建了一个系统化的优化流程，将复杂的配置组合搜索（如量化、GPU选型、张量并行）自动化，帮助用户在延迟、成本、质量和吞吐量四个关键KPI之间找到最佳平衡点，从而为不同应用场景（如实时语音机器人）提供最优的推理配置。最终，Ray帮助团队将组合爆炸式的优化问题转化为可管理的、自动化的AI工厂流程，实现了从批量成本敏感型工作负载到低延迟高吞吐量工作负载的平滑过渡。

关键点：
1. [00:02:56](https://youtu.be/QFDGVa4qQ3I?t=176s) **Ray在模型训练阶段的规模化提速**：演讲者通过一个客户案例，详细阐述了Ray如何解决大规模模型训练的可扩展性瓶颈。
    - **初始挑战**：客户试图训练一个图像生成模型，但在处理超过1亿样本时便无法有效扩展。他们经历了从单机单GPU到多机多GPU的传统扩展路径，但管理起来非常繁琐，尤其是在使用PyTorch分布式时。初步估算，完成整个数据集的训练需要大约600天，这在实践中是不可行的。
    - **Ray的解决方案**：团队利用Ray将客户的训练循环从PyTorch分布式转换为基于Ray的分布式数据并行（DDP），并轻松地将其扩展到数千个节点。这一转变直接将实验时间从600天大幅缩短至一个更现实的4到5天。
    - **进一步的性能增益**：在实现基础扩展后，团队进一步在Ray内部引入了自定义的训练循环。这一优化在客户原有训练循环的基础上，额外带来了42%的性能提升。这使得每次实验迭代的时间从完全不切实际（数年）缩短到可快速迭代的周期，从而帮助客户更快地获得更高质量的模型。
2. [00:04:33](https://youtu.be/QFDGVa4qQ3I?t=273s) **Ray在批量推理中的成本效益**：演讲者简要提及了Ray在批量推理任务中的应用，展示了其在处理大规模、成本敏感型工作负载时的效率。
    - **应用场景**：诸如转录或内容生成等批量作业，非常适合在Ray Jobs中运行。Ray的分布式能力使得这些任务能够高效地并行处理。
    - **显著的成果**：团队成功将成本效益优化到极致，能够以仅1美元的成本转录一个月的音频数据。这突显了Ray在实现大规模、低成本批量推理方面的强大能力，为成本敏感型应用提供了极具吸引力的解决方案。
3. [00:05:27](https://youtu.be/QFDGVa4qQ3I?t=327s) **从批量推理到实时推理的挑战与优化框架**：演讲者通过一个“债务催收语音机器人”的案例，深入探讨了如何将工作负载从成本敏感型批量任务转向对延迟和吞吐量敏感的实时应用，并提出了系统化的优化方法论。
    - **案例背景**：客户构建一个语音机器人，涉及语音转文本（ASR）和大语言模型（LLM）的流水线。关键性能指标是端到端延迟低于300毫秒（以保持实时感），同时降低每次对话的成本。
    - **优化成果**：团队通过优化推理栈，将LLM的首字节时间（TTFB）从450毫秒降至120毫秒，并将流式ASR的TTFB从500毫秒大幅降至70毫秒。值得注意的是，用于批量推理的Whisper模型（成本敏感）与用于实时场景的模型是同一个，但优化目标和手段完全不同。
    - **核心矛盾与选择**：演讲者指出，存在一种“过度优化”的配置，能将端到端延迟降至80毫秒，但成本会显著增加。这引出了工程实践中的核心权衡：在成本与用户体验（延迟）之间如何抉择。ML工程师每天都在为不同的用例（如自动驾驶需要极低延迟）组装和优化推理栈时面临此类选择。
    - **四维KPI框架**：为了简化决策，团队将用例需求归纳为四个关键KPI：**延迟、成本、质量和吞吐量**。明确这四方面的要求后，工程师可以从一个庞大的“网格搜索空间”中选择配置，这个空间包括：量化级别、GPU型号（如H100 vs A100）、张量并行度、推理引擎（如vLLM, TensorRT）等。
    - **配置组合的复杂性**：这个选择空间是组合爆炸式的。对于成本敏感型用例，可能选择H100配合TP1（张量并行度为1）；而对于延迟敏感型用例，则可能选择满载8张A100并设置TP8。不同的选择会得到在这些KPI上表现各异的“雷达图”。
4. [00:08:24](https://youtu.be/QFDGVa4qQ3I?t=504s) **利用Ray自动化推理栈优化与配置选择**：针对前述复杂的配置选择问题，演讲者详细介绍了如何利用Ray构建一个自动化流程，以高效找到特定用例的最优解。
    - **解决组合爆炸**：手动尝试所有配置组合是不现实的。团队的解决方案是利用Ray实现一个自动化机制。
    - **代理性能预测**：首先，他们为**主流架构、主流GPU和主流优化技术**建立了性能代理模型（近似估算）。基于这些近似值，可以筛选出一组有潜力的候选配置。
    - **自动化基准测试与部署**：然后，使用Kubernetes部署这些候选配置，并利用Ray对它们进行基准测试。最后，通过团队自有的API，结合用户提供的数据，自动选择出最佳配置。整个过程被封装并自动化为一个Ray Job运行。
    - **构建AI工厂**：这一流程的目标是将组合爆炸式的网格搜索问题，转化为系统化的、可构建“AI代工厂”的过程。最终目的是找到针对特定用例的、可能非直觉性的最优解决方案，而不是仅仅遵循最常见的路径。这确保了推理栈能够真正满足用户在延迟、成本等多维度上的独特需求。
---
## SQL or Python? Why It’s the Wrong Question in the AI Era | Ray Summit 2025
[https://www.youtube.com/watch?v=UcnP3_O1X5g](https://www.youtube.com/watch?v=UcnP3_O1X5g)

视频介绍：本视频是Firebolt工程副总裁Benjamin在Ray Summit上的演讲，他作为一名高性能C++/SQL系统程序员，深入探讨了在AI时代构建数据密集型应用时，不同数据处理框架（如SQL引擎与Python生态的Polars、Pandas、Ray等）的对比与融合趋势。他分析了当前AI应用开发的几大持久性模式，并揭示了这些看似不同的技术栈在底层构建模块上的惊人相似性。

结论：演讲者认为，我们正处在一个由两大技术社区（传统SQL/数据库工程师与新兴Python/ML工程师）共同构建AI应用的有趣时代。尽管表面使用的工具不同，但底层的数据管理挑战（如查询优化、开放格式支持、半结构化数据处理）高度相似。未来的发展方向并非“SQL vs Python”的二选一，而是各类系统的融合与取长补短：关系型SQL引擎需要更好地支持Python接口、多模态数据处理和CPU/GPU协同工作；而Python生态的数据处理框架则在查询规划器、半结构化数据处理效率等方面向成熟的SQL引擎看齐。最终目标是构建一个无缝的混合技术栈，让开发者能够根据任务需求，灵活、高效地运用不同工具的优势。

关键点：
1. [00:01:50](https://youtu.be/UcnP3_O1X5g?t=110s) **AI研究的爆炸性增长与不确定性**：演讲者通过数据可视化展示了自2021年以来，AI领域论文发表量的指数级增长，尤其是在ChatGPT发布后。这带来了巨大的机遇，也带来了技术快速迭代的不确定性。
    - **数据佐证**：他提到仅arXiv上关于人工智能的出版物就在飞速增长，2025年下半年（演讲时尚未结束）的数据预计将再次超越上半年。这直观地说明了该领域知识更新的速度。
    - **行业感受**：他指出在Ray Summit现场也能感受到这种“尘埃未定”的氛围，并现场询问观众是否对两年后的AI应用构建方式有信心，结果只有一人举手，这印证了整个行业仍处于快速探索和演变阶段。
    - **核心洞察**：这种不确定性是背景，但正是在这种快速变化中，一些基础性的、持久的应用模式开始浮现，为技术选型和架构设计提供了相对稳定的锚点。

2. [00:02:51](https://youtu.be/UcnP3_O1X5g?t=171s) **AI时代持久不变的四大应用模式**：尽管技术日新月异，但演讲者认为有四种构建AI应用的模式将长期存在。
    1. **大语言模型（LLMs）的普及**：LLMs改变了人机交互方式，能够构建更优质、更不同的软件，这一趋势不可逆转。
    2. **检索增强生成（RAG）作为核心模式**：无论具体技术如何演变（SQL for RAG, Graph RAG等），将相关业务数据输入LLM以获得贴合业务背景的答案，这一RAG范式将持续多年。
    3. **定制化机器学习模型的价值**：对于像Uber定价预测这类任务，使用LLM既不经济也难以解释。传统的、可定制的ML模型在特定领域仍能提供更优或更高质量的结果，不会被取代。
    4. **软件走向多模态化**：LLMs对音频、视频、图像的理解能力，正在使多模态数据处理对更广泛的公司变得触手可及。现场调查显示，工作中处理多模态数据的观众数量在短短几年内翻倍，预示着指数级增长趋势。

3. [00:05:02](https://youtu.be/UcnP3_O1X5g?t=302s) **构建AI应用的两大技术社区及其工作流差异**：当前高性能数据系统领域存在两个主要社区，它们构建AI应用的方式截然不同。
    1. **Python/ML社区（以观众为主）**：习惯使用Polars、Pandas进行数据框操作，使用Ray等框架编排ML训练和服务。他们偏好Python的灵活性和丰富的生态系统，进行复杂的数据框计算和模型开发。
    2. **SQL/数据库社区（Firebolt的许多客户）**：历史上可能未训练过自己的ML模型，但现在也开始构建AI应用。他们的工作流更接近传统的SQL批处理：通过ELT脚本将原始数据转换为适合服务的形态（类似于ML训练前的数据预处理），然后在服务层使用LangChain、MCP服务器等工具构建RAG管道。
    - **核心对比**：前者更注重实验、复杂计算和模型本身；后者更注重利用现有数据基础设施，以声明式、可管理的方式构建数据管道和应用逻辑。

4. [00:07:10](https://youtu.be/UcnP3_O1X5g?t=430s) **不同技术栈的底层构建模块对比分析**：演讲者通过一个带有表情符号评级的表格，深入比较了关系型SQL引擎、Python数据框引擎（如Polars）和分布式计算框架（如Ray）在多个维度的能力。
    1. **Python接口友好度**：
        - **SQL引擎**：承认在这方面做得不好，是明显的短板。作为数据库构建者，他认为应该让想写Python的人能轻松地使用SQL引擎。
        - **Python框架**：Pandas、Polars和Ray在这方面表现出色，是Python开发者的自然选择。
    2. **查询规划器**：
        - **SQL引擎**：是绝对强项。以Firebolt为例，其查询规划器拥有约170条优化规则，支持基于成本的连接重排序，并利用历史遥测数据优化新查询模式。
        - **Python框架**：Polars、Daft等系统也开始大力投入查询规划器，但SQL引擎目前仍保持领先。他强调，即使是用Python表达计算，底层有一个强大的查询规划器来优化执行图也极具价值。
    3. **开放格式支持**：
        - **所有系统**：在支持Parquet、JSON、Iceberg等开放格式方面都做得很好，这是整个生态的进步。
    4. **半结构化数据处理**：
        - **SQL引擎**：他认为这可能是争议点，但SQL引擎在高效处理半结构化数据（如海量结构各异的JSON文档）方面更具优势。它们拥有高效的索引结构，能在存储层对JSON对象进行“切分”，从而高效访问跨文档的字段。
        - **Python框架**：正在迎头赶上，但SQL引擎的基础设施目前更领先。
    5. **多模态支持**：
        - **SQL引擎**：是弱项。虽然可以将音视频数据存为BLOB，但缺乏表达性和高效处理能力。
        - **Ray等框架**：是绝对的冠军。能够大规模、在集群上运行，并轻松在CPU和GPU之间切换任务，非常适合多模态数据处理。Lance等新格式也正在被SQL数据库采纳。
    6. **缓存与索引**：
        - **SQL引擎**：由于面向的工作负载不同（如重复查询），在多层缓存（内存、SSD）和索引方面具有独特优势。
        - **ML训练框架**：在数据预处理管道中，通常不需要对同一图像进行百万次重复访问，因此对此需求较弱。
    7. **CPU/GPU协同**：
        - **SQL引擎**：需要大量改进。像Spark这样的传统系统并非为此设计。
        - **Ray等框架**：天生擅长在异构硬件（CPU/GPU）间分配和协调任务，是当前的最佳选择。

5. [00:13:46](https://youtu.be/UcnP3_O1X5g?t=826s) **未来展望：融合与取长补短**：演讲者总结认为，两大社区构建的系统底层相似，面临的数据管理问题也类似。
    - **SQL引擎的进化方向**：为了让对SQL引擎“不感冒”的Python/ML开发者也能受益，SQL引擎必须在以下方面努力：
        1. **改进接口**：让开发者能轻松地编写Python代码，并从中获得巨大价值。
        2. **增强多模态支持**。
        3. **简化CPU与GPU之间的工作交接**。
    - **理想的技术栈融合**：他展望未来，在一个完整的机器学习流水线中，开发者可以底层使用像Firebolt这样的系统进行大规模Python数据转换（利用其分布式能力和优化器），同时无缝衔接像Anyscale（Ray的商业化产品）这样的系统进行基于GPU的多模态处理和编排。这标志着从“工具之争”走向“工具协同”的新阶段。
---
## Ray Agent Engine: Deploying AI Agents with Ray Serve | Ray Summit 2025
[https://www.youtube.com/watch?v=2hGLGpR3PCY](https://www.youtube.com/watch?v=2hGLGpR3PCY)

视频介绍：本视频由来自Apple团队的Bumik和Deep共同分享，深入探讨了如何利用Ray框架构建并大规模部署生产级的AI智能体（Agent）系统。演讲首先剖析了智能体的基本构成，然后详细阐述了生产环境对智能体引擎的核心要求，并展示了Ray如何通过其原生组件（如Ray Cluster、Ray Serve和Ray Jobs）来满足这些需求，最后通过一个端到端的实例演示了使用Ray Serve将智能体从本地开发部署到Kubernetes生产环境的完整流程。

结论：Ray框架为生产级AI智能体的部署提供了强大、全面且开箱即用的解决方案。通过Ray Cluster实现异构资源的抽象与弹性伸缩，通过Ray Serve将智能体轻松封装为可自动扩缩容、具备容错能力的FastAPI服务，并通过Ray Dashboard提供深入的观测能力。整个流程实现了从本地快速迭代到云原生（Kubernetes）无缝部署的闭环，确保了开发友好性、框架无关性以及生产环境所必需的可靠性、可观测性与可管理性。这证明了Ray是构建和运维大规模、复杂智能体应用的理想技术栈。

关键点：
1. [00:00:38](https://youtu.be/2hGLGpR3PCY?t=38s) **智能体的基本构成与生产环境要求**：演讲者首先拆解了智能体的核心组件，并基于此提出了将其投入生产环境所需满足的关键要求。
    - **智能体解剖结构**：一个智能体通常由几个基本构建块定义：工具（Tools）、大语言模型（LLMs）以及一些智能体节点（Agentic Nodes）。这些组件赋予了智能体推理、工具调用和资源访问的能力。开发者将这些组件组合起来，形成智能体的“大脑”，使其具备编排调用、决策制定和状态管理等高级功能。最后，开发者会通过类似FastAPI的框架将智能体转化为一个可服务的应用程序。演讲通过一个简单的“支付状态查询”智能体示例，生动说明了这一流程：用户提问后，LLM决定调用“获取支付状态”工具，执行调用并整合结果返回给用户。
    - **生产级智能体引擎的核心需求**：当需要将此类智能体部署到生产环境时，对底层引擎提出了严苛要求：
        1. **框架无关性**：引擎必须能够支持使用各种流行开源框架开发的智能体。
        2. **开发友好性**：本地编写的代码应能几乎无修改地部署到生产环境，实现从开发到生产的平滑过渡。
        3. **运维基石**：系统必须具备**自动扩缩容**能力以应对流量波动，以及**容错性**以确保服务高可用。
        4. **管理与观测**：智能体作为服务，需要管理其访问权限。同时，在大规模运行时，必须提供完善的**可观测性**，包括日志、链路追踪和指标监控。

2. [00:03:19](https://youtu.be/2hGLGpR3PCY?t=199s) **Ray框架的天然适配性**：在明确生产需求后，演讲者指出Ray框架原生提供了满足上述大部分要求的强大能力。
    - **Ray Cluster**：它抽象了所有异构计算资源（如不同型号的CPU、GPU），并提供了集群级别的**自动扩缩容**能力，为上层应用提供了稳定、弹性的资源池。
    - **Ray Serve**：它是一个高性能的模型服务框架，能够轻松地将一个简单的Python API（如基于FastAPI的应用）转化为一个可以**根据流量需求自动扩缩容**且**具备容错能力**的生产级服务。这正是将智能体“服务化”的关键。
    - **Ray Jobs**：擅长运行异步任务，适合处理智能体可能涉及的背景作业或长时间运行的操作。
    - **开箱即用的可观测性**：Ray生态系统原生提供了丰富的指标、追踪信息，并通过Ray Dashboard提供了统一的监控视图，完美满足了生产环境对可观测性的要求。因此，Ray是构建生产级智能体引擎的绝佳选择。

3. [00:04:26](https://youtu.be/2hGLGpR3PCY?t=266s) **使用Ray Serve部署智能体的端到端旅程**：演讲者Bumik详细演示了将一个智能体从本地开发部署到Kubernetes生产环境的完整步骤。
    - **第一步：构建FastAPI应用**：首先将开发好的智能体包装成一个FastAPI应用。例如，创建一个`/agent`端点来接收用户查询并调用智能体，同时可以创建`/health`等端点用于健康检查。这是将智能体逻辑暴露为HTTP服务的基础。
    - **第二步：封装为Ray Serve应用**：将上一步的FastAPI应用指定为Ray Serve的“入口”（ingress）。仅需几行代码，即可将其转换为一个Ray Serve部署，从而能够利用Ray的所有服务特性（如WebSocket、流式响应等）。
    - **第三步：本地快速迭代与验证**：使用`serve run`命令即可在本地启动Ray Serve服务，并自动打开Ray Dashboard。开发者可以在此快速测试功能、观察服务行为、调整代码，实现高效开发闭环。
    - **第四步：配置生产参数**：Ray Serve允许通过代码注解或独立的YAML配置文件（`serve config v2`）来定义生产配置，如自动扩缩容策略（最小/最大副本数）、容错设置、资源限制等。这实现了配置与代码的分离，便于针对不同环境（开发、测试、生产）进行定制。
    - **第五步：利用Dashboard进行深度观测**：Ray Dashboard是观测Ray应用的核心工具，提供了资源利用率、请求指标、日志等多个维度的可视化信息，帮助开发者深入理解服务运行状态，为性能调优和故障排查提供依据。
    - **第六步：部署至Kubernetes**：通过定义`RayService`自定义资源（CRD），可以将整个应用打包部署到Kubernetes。该CRD包含两部分配置：`RayClusterConfig`用于定义底层K8s资源（如Head/Worker Pod的规格、镜像、环境变量）；`RayServeConfig`则引用之前准备好的`serve config` YAML文件。使用`kubectl apply`即可完成生产部署。

4. [00:10:02](https://youtu.be/2hGLGpR3PCY?t=602s) **技术栈整合与优势总结**：最后，演讲者总结了如何利用Ray的各个组件协同工作，构成一个完整、健壮的智能体部署方案。
    - **资源层（Ray Cluster）**：负责底层的资源隔离、供给与弹性伸缩，让开发者无需关心基础设施的细节。
    - **服务层（Ray Serve）**：在资源层之上，将智能体抽象为高性能、可管理的服务，自动处理请求路由、扩缩容和故障恢复等复杂的编排任务。
    - **观测层（Ray Dashboard）**：贯穿始终，为本地调试和生产监控提供了统一的视野，增强了运维信心和系统透明度。
    - **整体价值**：这套组合拳使得开发者能够专注于智能体本身的业务逻辑创新，而将部署、运维、扩缩容、观测等生产级挑战交给Ray框架处理，极大地提升了开发效率和系统可靠性，使得Ray成为部署生产级智能体的理想框架。
---
## Scaling AI the Snowflake Way: ML Workloads on Ray | Ray Summit 2025
[https://www.youtube.com/watch?v=VYWSAPLXtEs](https://www.youtube.com/watch?v=VYWSAPLXtEs)

视频介绍：本视频由Snowflake的工程师团队分享，重点介绍了他们如何利用Ray分布式计算框架构建“多模型框架”，以解决企业级预测分析中大规模、细粒度模型训练的复杂挑战。视频通过一个零售业销售预测的具体案例，阐述了该框架如何抽象化分布式训练的复杂性，并展示了其架构、核心优势及一个实时演示。

结论：Snowflake通过构建基于Ray的多模型框架，成功解决了在超细粒度（如按门店、按产品）上进行大规模、高频次Transformer模型训练的分布式系统难题。该框架将数据分区、模型训练、容错、可观测性以及动态扩缩容等复杂性对用户透明化，用户只需提供分区数据、定义训练函数即可。结合Snowflake的数据生态和Ray的分布式能力，该方案实现了训练与推理的高效流水线并行、计算资源的饱和利用以及近乎线性的性能扩展，为企业处理海量预测任务提供了强大、易用且可靠的一站式解决方案。

关键点：
1.  [00:00:44](https://youtu.be/VYWSAPLXtEs?t=44) **现代预测问题的复杂性与挑战**：演讲者以零售业销售和库存预测为例，阐述了为何当今的预测问题变得异常复杂，从而引出了对高效分布式训练框架的迫切需求。
    -   **数据模态复杂**：预测不再依赖简单的线性数据，而是涉及多模态数据，包括产品反馈图片、视觉信息、社交媒体提及等，这增加了数据处理的难度。
    -   **时间序列非线性**：预测通常具有强烈的非线性和复杂的季节性模式，例如长周末、体育赛事等特定事件都会对预测产生显著影响，需要模型能够捕捉这些复杂模式。
    -   **外部因素干扰**：市场动荡、地缘政治等外部不可控因素进一步增加了预测的不确定性和建模难度。
    -   **解决方案趋势**：鉴于上述复杂性，使用Transformer等复杂模型进行预测已成为Snowflake客户的主流选择。
    -   **规模化挑战**：真正的挑战在于规模化应用。例如，零售商需要为成千上万个“门店-产品”组合（SKU）进行超本地化的预测，这意味着需要同时训练和管理数万个模型。此外，由于市场动态变化，这类训练可能需要每日进行，从而带来了训练数万个Transformer模型的巨大计算和运维压力，包括容错、重试、可观测性、集群规模管理与动态扩缩容等，这本质上是一个极其困难的分布式系统问题。
2.  [00:02:44](https://youtu.be/VYWSAPLXtEs?t=164) **Ray框架的核心价值与多模型框架的抽象化目标**：此处明确了Ray在解决上述分布式训练难题中的关键作用，并介绍了Snowflake多模型框架的设计哲学——将复杂性抽象化。
    -   **Ray的卓越贡献**：Ray框架在处理这种大规模、细粒度的并发模型训练、容错、集群管理等方面表现出色，是解决该分布式系统问题的关键技术基础。
    -   **用户视角的简化**：多模型框架的核心目标是**为开发者和数据科学家抽象掉所有底层的编排复杂性**。用户只需关注两件事：
        1.  确保数据已按某个键（如SKU、产品ID、门店ID）分区。
        2.  编写自己的训练函数（即模型代码，如Transformer）。
    -   **平台承诺的责任**：用户只需通过Snowflake API指明训练数据集、分区键和训练函数，平台则承诺自动、容错地完成所有模型的训练。这极大地降低了使用门槛。
    -   **训练与部署一体化**：该框架不仅负责训练，还负责模型的部署与运营化。它会将所有训练好的分块模型**聚合成一个在模型注册表中可引用的统一资产**。在推理时，用户可以利用相同的分区模式，将这个“统一模型”自动应用到对应的每个产品ID或门店ID的数据上，实现无缝的批量预测。
3.  [00:04:13](https://youtu.be/VYWSAPLXtEs?t=253) **底层技术栈：基于Ray的容器运行时服务**：揭示了支撑多模型框架的底层基础设施，即Snowflake构建在Ray之上的托管服务。
    -   **容器运行时**：这是一个构建在Ray之上的**托管服务**，是框架能力的基石。Ray在其中扮演了核心组件角色。
    -   **Snowflake与Ray的深度集成**：Snowflake并非简单使用开源Ray，而是将其深度集成到自身的技术栈中，实现了高效协同。
    -   **关键优化**：集成的核心效率提升体现在**数据摄入**环节。框架能够将Snowflake中可能高达TB/PB级的数据，高效地输入到分布式计算集群中，并**优化数据直接馈送给Ray Actor的流程**。此外，对于分布式训练、超参数调优等其他处理任务，也带来了大量的简化和操作效率提升。
    -   **用户体验**：用户通过打开Notebook或运行任务，即可获得一个完整的、由Ray提供动力的操作环境，同时享受Snowflake平台的易用性优势。
4.  [00:05:34](https://youtu.be/VYWSAPLXtEs?t=334) **多模型框架的并行化架构与核心设计**：由工程师详细拆解了框架如何实现数据分区、并行训练和动态调度的技术架构。
    -   **数据分区与流水线并行**：
        1.  用户提供Snowflake表及分区列。
        2.  框架自动将表数据按分区列拆分为多个分区。
        3.  **关键设计**：**不等待所有分区数据都准备就绪**，而是采用流水线方式。一旦某个分区的数据准备完成，就立即将其放入队列，以便计算资源可以尽早开始工作，确保计算资源利用率最大化。
    -   **高效数据注入的两阶段过程**：
        1.  **数据提取**：利用Snowflake数据仓库将数据从Snowflake表加载到Snowflake Stage（底层由S3支持），存储为Apache Arrow IPC文件格式。
        2.  **并行下载**：框架实现了自定义的Ray数据源，通过多个Ray任务并行地从S3下载这些Arrow IPC文件到各个Ray Actor中。
    -   **动态任务调度与容错**：
        1.  后台线程监控队列，一旦发现有新分区数据就绪，便将该分区对应的IPC文件引用和分区ID提交给Actor池。
        2.  Actor池会指派一个特定的Actor，利用Ray数据源并行下载数据并开始执行用户定义的训练任务。
    -   **运行时动态扩缩容**：
        -   考虑到训练任务可能耗时很长，且用户难以预先精确估计所需资源，框架支持**在任务运行期间动态增加或减少节点**。
        -   例如，当发现GPU节点不足时，用户可以扩展集群，而无需中断现有工作流。框架的“Actor池缩放器”会检测到新节点加入，并创建新的Actor来从队列中获取任务，从而加速整体处理速度。
5.  [00:08:31](https://youtu.be/VYWSAPLXtEs?t=511) **实时演示：从训练、观测到推理的全流程**：通过一个销售预测的演示，直观展示了框架的易用性、强大功能和关键特性。
    -   **环境与数据**：演示在Snowflake Notebook中进行，使用一个包含门店、产品、日期、销售额及价格信息的合成数据集，目标是基于过去14天的特征预测下一天的销售额。
    -   **用户代码极简**：
        1.  定义一个简单的PyTorch模型。
        2.  编写一个训练函数，该函数接收一个与特定分区绑定的数据连接器和一个包含分区ID等信息的上下文对象。**演示特意在训练函数中为分区ID5抛出一个错误，以展示错误处理能力**。
    -   **动态扩缩容体验**：演示开始时集群只有一个节点，但可以在训练启动后，**异步地执行扩容代码**，将集群扩展到两个节点。新节点加入后会自动融入Ray集群并开始处理任务，实现了计算与资源准备的重叠。
    -   **强大的可观测性**：
        1.  用户界面提供实时的训练进度和资源使用情况（CPU/内存）。
        2.  高级用户可以直接访问**原生的Ray仪表板**，深入查看节点状态、任务、Actor等详细信息，便于进行深度调试和性能分析。
    -   **训练结果与错误处理**：
        1.  训练完成后，可以方便地获取进度报告。演示中，10个模型有9个成功，1个（分区ID5）失败。
        2.  成功模型可以查看训练过程中的损失输出。
        3.  **失败处理**：用户可以轻松筛选出失败的分区，查看具体的错误信息。之后，可以**仅针对失败的分区重新启动训练**，从而节省时间和成本。
    -   **模型注册与统一管理**：训练好的所有分块模型可以被**注册到Snowflake模型注册表**，形成一个统一的模型资产。在注册表中，可以查看模型的输入输出格式、元数据以及每个分区对应的PyTorch模型文件，便于管理和部署。
    -   **分区感知的推理**：在推理阶段，用户提供一个推理数据集。框架会根据数据行中的分区键（如门店ID），**自动将数据路由到对应的子模型进行预测**，并返回结果，例如预测的下一天销售额。
6.  [00:14:15](https://youtu.be/VYWSAPLXtEs?t=855) **性能表现与线性扩展能力**：最后简要总结了框架的性能优势，验证了其设计有效性。
    -   **相对于串行训练的增益**：即使在单节点情况下，随着分区数量的增加，多模型框架相比串行训练也能显示出性能优势，这主要归功于数据注入和训练过程的流水线并行设计。
    -   **近乎线性的扩展**：当从单节点扩展到多节点时，**性能提升接近线性**。演示图表显示，双节点下的任务完成时间几乎比单节点缩短了一半，证明了框架良好的水平扩展能力。
---
## How Runhouse Orchestrates Multi-Cluster Ray Workloads | Ray Summit 2025
[https://www.youtube.com/watch?v=Jv4qLDK7VMc](https://www.youtube.com/watch?v=Jv4qLDK7VMc)

视频介绍：本视频由Runhouse联合创始人兼CEO Donnie Greenberg分享，探讨了Ray分布式框架与Kubernetes结合的创新思路，并介绍了其团队开发的CubeTorch系统如何实现Ray程序的“无服务器化”运行，从而为机器学习工作流带来更灵活、高效的编排体验。

结论：Ray作为一个强大的分布式计算框架，其核心设计（如装饰器和任务/参与者模型）与Kubernetes的容器化、声明式资源管理理念存在有趣的对应关系。通过构建像CubeTorch这样的编排层，可以将Ray的计算能力无缝融入Kubernetes生态，实现远程原生执行、精细化的资源控制、卓越的依赖隔离和容错能力。这不仅极大地改善了开发者的体验，使得从本地笔记本到生产环境的代码迁移变得无缝，还降低了Ray的采用门槛，支持渐进式、按需使用的模式。CubeTorch通过解耦基础设施与业务逻辑，并利用HTTP通信和热同步技术，为在Kubernetes上运行复杂的、多阶段的ML工作流（如强化学习、多模型推理、超参数优化）提供了一种强大且优雅的解决方案。

关键点：
1.  **[00:00:05](https://youtu.be/Jv4qLDK7VMc?t=5s) Ray与无服务器框架的对比及核心差异**：演讲者开篇即提出一个引人深思的观察：Ray的装饰器语法（如`@ray.remote`）在形式上与Modal等无服务器框架非常相似，两者都通过装饰器来指定函数运行所需的计算资源。然而，它们的本质截然不同。
    -   **Ray的集群内调度模式**：Ray的装饰器作用是在一个**已经构建好的Ray集群内部**进行任务调度。它指定的是任务或参与者在集群节点和进程间的分配逻辑。用户需要预先配置和启动一个Ray集群（例如通过Kubernetes），然后代码在这个集群的上下文中运行。
    -   **无服务器框架的外部触发模式**：以Modal为例，其装饰器标志着该函数将由Modal平台**从外部**进行管理。代码本身在本地开发，但通过特定的CLI命令触发部署，平台负责按需分配资源、打包环境并执行。这是一种“函数即服务”（FaaS）的模式。
    -   **带来的困惑与启示**：这种表面相似性常常让新接触Ray的ML从业者感到困惑：为什么既要写装饰器指定资源，又要另外配置Kubernetes清单来分配集群资源？这引出了后续的核心思考——能否将Ray的编程模型与Kubernetes的资源管理层更深度地融合，甚至用Kubernetes的原生能力来“实现”一个类似Ray的系统。

2.  **[00:01:10](https://youtu.be/Jv4qLDK7VMc?t=70s) 思想实验：用Kubernetes组件“重建”Ray**：演讲者进行了一个大胆的思想实验：假设我们为Kubernetes重新设计Ray，用Kubernetes的原生组件替代Ray的核心部分。
    -   **组件映射**：
        -   **调度器**：将Ray的调度器替换为Kubernetes调度器，直接调度Pod等资源。
        -   **自动扩缩容**：用Kubernetes的HPA（Horizontal Pod Autoscaler）或Cluster Autoscaler替代Ray的自动扩缩容器。
        -   **可观测性**：利用Kubernetes成熟的监控（如Prometheus）、日志（如EFK栈）和事件体系。
    -   **计算原语的扩展**：这样做最大的好处是**极大地扩展了可用的计算原语**。在纯Ray集群中，节点本质上是进程。而在Kubernetes视角下，“节点”可以是：
        -   单个Pod（对应单机任务）。
        -   一组通过SPMD（单程序多数据）模式协同工作的Pod（如PyTorch分布式训练组）。
        -   一个控制类资源（如Ray集群的头节点或Spark的Driver），它本身又可以内部使用Ray来编排任务。
        -   自动扩缩容的推理服务（Kubernetes的Deployment + HPA）。
        -   有状态副本集（如游戏模拟器、沙盒环境）。
    -   **面临的挑战**：直接这样“替换”会丢失Ray提供的许多关键价值：**分布式任务派发与调度逻辑、日志/错误传播、容错机制、运行时环境管理**（如依赖的热更新）。在Kubernetes上构建一个可用的、类似Ray的动态Pythonic接口系统，需要自行解决这些问题。

3.  **[00:03:48](https://youtu.be/Jv4qLDK7VMc?t=228s) 新模式开启的复杂工作流编排可能性**：将Ray的编程模型与Kubernetes的丰富资源相结合，为编排复杂的多阶段机器学习工作流打开了新世界。
    -   **强化学习（RL）工作流示例**：一个典型的RL流水线可能包含：
        1.  **数据预处理**：由一组Ray分布式任务完成。
        2.  **轨迹生成**：由自动扩缩容的推理服务（作为参与者）完成。
        3.  **模型训练**：由SPMD模式的分布式训练工作节点完成。
        4.  **评估与沙盒**：由独立的、有状态的沙盒环境（如游戏模拟器）进行多轮次评估或智能体任务。
    -   **原生Python编排的优势**：如果能用统一的Python API（类似Ray的`remote`）直接请求和组合这些不同类型的Kubernetes资源（Pod、Deployment、Job等），就能在Kubernetes上**原生地、灵活地编程实现整个工作流**。这符合Ray使用模式的新趋势：在单个工作流中组合使用多个Ray集群或其他异构资源。

4.  **[00:05:12](https://youtu.be/Jv4qLDK7VMc?t=312s) 实现“远程原生”执行的关键：移除序列化，采用HTTP通信**：为了真正实现类似无服务器的体验，一个关键变革是改变Ray任务的分发机制。
    -   **Ray的序列化瓶颈**：Ray通过序列化（Pickle）任务函数和其闭包，将它们发送到工作节点执行。这要求集群内所有节点的Python环境（库版本等）必须高度一致，否则容易引发兼容性问题，这也是Ray Client被弃用的原因之一。
    -   **CubeTorch的HTTP/RPC方案**：CubeTorch移除了这种代码序列化。相反，它让“参与者”（远程计算单元）之间严格通过**HTTP/RPC进行通信**。用户在本地的Python代码（控制器）通过HTTP调用，将需要执行的**逻辑标识**（如函数名、模块路径）和参数发送给远程集群。真正的函数体代码并不在网络上传输，而是预先部署或热同步到远程环境中。
    -   **带来的根本性变化**：
        -   **依赖解耦**：不同阶段的工作负载可以使用完全不同的容器镜像和依赖环境，因为它们之间没有代码序列化的耦合。
        -   **真正的远程开发**：开发者可以在笔记本或本地IDE中编写代码，直接调用远在Kubernetes集群中的强大算力（如Ray集群），无需SSH隧道或置身于集群网络内部。体验如同使用一个远程的“超级计算进程池”。
        -   **无缝生产部署**：这段本地编写的、包含了远程调用逻辑的Python代码，可以**原封不动地**放入Argo、Airflow等生产工作流或CI/CD管道中运行，实现了开发与生产环境的完全一致。

5.  **[00:07:26](https://youtu.be/Jv4qLDK7VMc?t=446s) CubeTorch系统介绍：设计哲学、API与核心优势**：基于以上理念，Runhouse构建了名为CubeTorch的开源系统。
    -   **设计哲学与API**：CubeTorch明确将自己定位为**编排框架**而非分布式框架。它不鼓励使用装饰器将基础设施细节耦合进业务代码。其核心API是声明式的：用户首先在Python中**指定计算资源**（`compute = KubeCluster(...)`），然后**指定要运行的程序**（`remote_fn = compute.run(my_function)`），最后得到一个本地的可调用对象，对其的调用会通过RPC在远程集群执行。
    -   **极速打包与热同步**：这是实现良好体验的技术基石。直接使用Kubernetes部署代码通常很慢（需要拉镜像、启动Pod等）。CubeTorch通过后台保持计算资源“预热”，并将本地代码或环境变更在**几秒钟内**热同步到远程环境，实现了接近本地开发的迭代速度。
    -   **核心优势总结**：
        1.  **卓越的开发体验**：支持全远程开发，兼容任何IDE和笔记本，无需SSH隧道，提供了类似已弃用Ray Client的体验但更健壮。
        2.  **强大的容错能力**：由于通信基于HTTP，单个节点或任务的失败不会引起级联故障。控制器可以捕获远程异常，并据此做出决策（如换用更大内存的节点重试、执行检查点），实现高级别的容错逻辑。
        3.  **彻底的依赖隔离**：不同任务可使用完全独立的镜像，解决了Ray中因序列化要求而带来的环境一致性问题。
        4.  **优秀的可移植性**：代码在本地和生产环境（CI、Argo等）中运行方式完全一致，是云原生的且跨云兼容。
        5.  **精细的资源控制与效率**：提供与原生Kubernetes同等级别的资源控制粒度（如节点选择器、容忍度），甚至支持动态扩缩容正在运行的工作负载，从而最大化集群利用率。
        6.  **降低Ray采用门槛**：支持“增量式采用”。团队无需重写现有代码库或工作流（如Argo DAG），只需将计算密集的部分函数或类通过CubeTorch放到Ray集群上运行，其余部分保持不变，极大地降低了迁移成本和风险。
---
## How Coinbase Uses Ray, vLLM & LiteLLM to Power Secure LLM Services | Ray Summit 2025
[https://www.youtube.com/watch?v=RYaAwkgNTBc](https://www.youtube.com/watch?v=RYaAwkgNTBc)

视频介绍：本视频由Coinbase机器学习与平台团队的工程师Axat和Venue分享，详细介绍了他们如何利用Ray Serve、vLLM和LiteLLM等技术栈，为整个公司构建一个可信赖的、统一的LLM网关平台，以支持多样化的生成式AI应用场景，并特别强调了在数据安全和成本优化方面的考量。

结论：Coinbase通过构建一个基于开源标准的、混合云与自托管模型的LLM网关，成功解决了在金融科技领域部署生成式AI时面临的核心挑战：数据安全、成本控制和灵活扩展。该平台以LiteLLM作为统一路由层，整合了外部API和内部通过Ray Serve与vLLM部署的自托管模型，实现了对敏感数据（PII/MNPI）的严格保护，同时通过模型混部优化了昂贵的GPU资源利用率。这一架构不仅支持了数百个内部AI助手和外部应用，还为不同特性的用例（如延迟敏感型、高吞吐量型）提供了可靠的服务能力，展现了企业级AI平台在平衡创新、安全与效率方面的最佳实践。

关键点：
1.  [00:00:28](https://youtu.be/RYaAwkgNTBc?t=28s) **Coinbase对生成式AI用例的分类与设计原则**：演讲者首先阐述了Coinbase如何根据应用场景和数据敏感性对AI用例进行分类，这直接决定了后续的技术架构选择。
    1.  **内外用例之别**：开发任何生成式AI应用时，首先需明确是面向内部员工还是外部客户。这对下游设计选择有重大影响。
        1.  **外部用例**：如客户支持聊天机器人，需要优化成本和延迟，并有严格的输出格式要求和安全护栏（Guardrails）。这类用例为使用更小、更专精的语言模型（而非万亿参数大模型）提供了探索空间。
        2.  **内部用例**：目标是赋能每个团队和员工，挑战在于提供多样化的模型家族（如大语言模型、嵌入模型、排序器、知识工具等），并让团队能以完全自助的方式构建AI助手。
    2.  **基于知识敏感性的分类**：这是决定模型部署位置（云端或内部）的关键因素。
        1.  **敏感信息定义**：包括个人身份信息（PII，涉及员工或客户）和重要非公开信息（MNPI）。
        2.  **部署策略**：对于处理非敏感信息的用例，可以放心地将知识发送给第三方LLM提供商；而对于涉及难以脱敏的敏感信息（如MNPI）的用例，则必须将知识处理限制在Coinbase的虚拟私有云（VPC）内部，这就引出了自托管模型的需求。

2.  [00:03:05](https://youtu.be/RYaAwkgNTBc?t=185s) **GenAI平台采用开放标准后的规模化采纳**：演讲者简要回顾了平台发展的关键转折点，即采用开放标准后带来的爆发式增长。
    1.  **采纳开放标准**：早期2025年，Coinbase在LLM推理和工具调用层面采用了开放标准。
    2.  **具体技术栈**：采用OpenAI兼容的LLM路由器（即LiteLLM），并使用模型上下文协议（MCP）进行所有工具集成。
    3.  **增长效果**：这一举措直接促使公司内部在短时间内涌现出数百个AI助手，显示了标准化接口对于降低开发门槛、促进内部创新的巨大作用。

3.  [00:03:37](https://youtu.be/RYaAwkgNTBc?t=217s) **Coinbase GenAI平台的整体技术栈概览**：视频展示了公司层面统一的生成式AI平台架构，体现了分层解耦的设计思想。
    1.  **应用层**：顶层是多样化的应用，包括面向最终用户的应用、内部自动化工具、以开发者体验为中心的工具以及智能体构建能力。
    2.  **中间件层**：核心是一个中间件，用于连接所有标准化的路由器。
        1.  **LLM调用**：通过基于LiteLLM构建的LLM网关进行统一路由。
        2.  **工具调用**：在所有数据源之上部署MCP服务器。
    3.  **框架层**：由于采用了LLM调用和工具调用的标准协议，使得团队能够使用任何开源GenAI框架（如LangChain、LlamaIndex）来构建AI智能体，实现了框架的无关性。

4.  [00:04:27](https://youtu.be/RYaAwkgNTBc?t=267s) **内部LiteLLM网关的定制化与集成**：演讲者聚焦于LLM路由器，详细说明了其内部部署的LiteLLM如何与公司内部系统深度集成。
    1.  **内部部署**：使用内部部署的LiteLLM，而非SaaS服务。
    2.  **关键集成点**：
        1.  与内部密钥管理服务挂钩，实现类似OpenAI的API密钥管理。
        2.  集成内部速率限制器和配额管理服务，防止单一客户端耗尽资源（如Anthropic模型的配额）。
        3.  具备完善的审计能力，可追踪哪些服务或客户端使用了哪些模型、消耗了多少令牌。
        4.  将所有进入LLM网关的请求详细记录并流入Snowflake数据仓库，为下游的可观测性分析（如使用情况、性能监控）提供数据基础。

5.  [00:05:33](https://youtu.be/RYaAwkgNTBc?t=333s) **自托管模型技术栈：Ray Serve, vLLM与LiteLLM的协同**：Venue深入讲解了用于部署自托管模型的技术选型及其优势。
    1.  **技术栈构成**：
        1.  **Ray Serve**：用于可扩展的模型部署。其核心价值在于能够将多个模型（4-5个）混合部署在单个多GPU实例上，从而高效利用昂贵的GPU资源（如H100），避免资源浪费。这对于流量并非持续巨量的内部模型（如数百万次调用）尤其经济。
        2.  **vLLM**：一个开源的高吞吐量推理引擎，专门用于高效运行大型语言模型。
        3.  **LiteLLM**：作为统一的流量分发和控制层。
    2.  **数据安全优势**：自托管模型的核心驱动力是将敏感数据完全控制在公司VPC内部。例如，涉及诈骗电话记录等超级敏感信息时，绝不能发送给外部提供商。
    3.  **请求路由路径**：
        1.  服务可以直接调用Ray Serve端点（例如，针对BART、Whisper等特定模型）。
        2.  对于符合OpenAI SDK要求的LLM（如Llama 3、Gemma、Qwen），请求会先发送到LiteLLM网关，由网关进行路由、配额管理和日志追踪，再转发给内部托管的模型。

6.  [00:07:46](https://youtu.be/RYaAwkgNTBc?t=466s) **基于Ray Serve的模型部署实践**：演讲者以具体配置文件为例，说明了如何使用Ray Serve部署和管理模型。
    1.  **部署配置文件**：主要涉及两类文件。
        1.  **Serve YAML文件**：定义模型部署的核心配置，包括模型定义、计算资源配置（可从任何规模获取，包括内部预留资源）以及指向的Kubernetes集群。
        2.  **模型配置文件**：包含更细致的运行时配置。
            1.  **伸缩配置**：如最小/最大副本数、目标进行中请求数、最大请求数。
            2.  **运行时配置**：如使用的GPU数量、模型加载参数、是否启用自动工具调用功能等。
            3.  **Hugging Face配置**：用于指定模型来源。

7.  [00:08:48](https://youtu.be/RYaAwkgNTBc?t=528s) **自托管模型在Coinbase的具体应用场景**：列举了自托管模型（特别是较小模型）在保护敏感数据和执行限定任务中的实际用途。
    1.  **增强尽职调查（EDD）自动化**：处理用户上传的就业证明、政府签发身份证等包含PII和MNPI的文件。
    2.  **内部工具与知识检索**：
        1.  在Slack机器人中，利用内部知识库回答工程师关于如何设置资源调配或网络配置等问题。
        2.  使用小型嵌入模型对知识库（如Confluence页面）中的内容进行检索和排序，找出最合适的答案。这类任务不需要大模型，小模型即可高效完成。

8.  [00:10:23](https://youtu.be/RYaAwkgNTBc?t=623s) **问答环节精华：架构细节与决策考量**：观众提问揭示了平台设计的更多深层思考。
    1.  **路由逻辑澄清**：所有请求，包括对Ray Serve后端自托管模型的调用，都统一经过LiteLLM网关进行分发和管控，确保了策略的一致性。
    2.  **成本追踪方式**：
        1.  **自托管模型**：成本核算主要基于所使用的GPU资源。通过将多个模型混合部署在单张GPU上来分摊成本。
        2.  **外部模型**：通过追踪输入/输出/缓存令牌的消耗量来计算成本。
    3.  **自托管与使用外部API的决策边界**：这是最核心的决策之一。
        1.  **首要驱动力：数据安全**。当用例涉及PII、MNPI等敏感信息时，必须采用自托管。
        2.  **次要驱动力：成本优化与任务适配性**。
            1.  对于外部面向用户的用例，流量可能很高（如加密货币牛市时），在流水线中某些特定任务（如格式检查、简单护栏）上使用成本更低的小型自托管模型，比全部使用昂贵的第三方万亿参数模型更具成本效益。
            2.  决策没有绝对的流量阈值，关键在于平衡购买GPU的固定成本与使用第三方API按令牌付费的可变成本，这高度依赖于公司的具体运营规模。
---
